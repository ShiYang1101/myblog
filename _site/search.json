[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Aetius Lee",
    "section": "",
    "text": "Welcome to my blog posts! I am a theoretical physics graduate, aiming to transition to the bizarre world of data. Interested in conciousness and AI (Shadow of the Mind by Roger Penrose is godsend). Could AI arise conciousness one day? I would never know.\nProficient in Python, SQL, statistic, Tableau and machine learning algorithms."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My weird data corner",
    "section": "",
    "text": "Bike sales analysis\n\n\n\n\n\n\n\nEDA\n\n\nStatistical Analysis\n\n\nClustering\n\n\n\n\nAnalysis of bike sales data.\n\n\n\n\n\n\nJan 13, 2023\n\n\nAetius Lee\n\n\n\n\n\n\n  \n\n\n\n\nHealth data analysis\n\n\nAnalysis of my own health data. Data collected by using several fitness trackers including Garmin Vivostyle, Fitbit Charge 4, Fitbit Charge 5 and Garming Forerunner 45\n\n\n\n\nFitness\n\n\nEDA\n\n\nStatistical Analysis\n\n\nAutoGluon\n\n\nTimeseries Analysis\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMusic transcription\n\n\nConvering raw music files into MIDI format!\n\n\n\n\nComputer Vision\n\n\nStatistical Analysis\n\n\nLibrosa\n\n\nRNN\n\n\nLSTM\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/health/index.html",
    "href": "posts/health/index.html",
    "title": "Health data analysis",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/music/index.html",
    "href": "posts/music/index.html",
    "title": "Music transcription",
    "section": "",
    "text": "Music transcription\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nMusic transcription - MusicNet LSTM\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nOrchideaSOL CNN\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nOrchideaSOL RNN\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nPreprocessing\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/health/Open_Health_Data_Analysis/Fitbit/Fitbit_to_json.html",
    "href": "posts/health/Open_Health_Data_Analysis/Fitbit/Fitbit_to_json.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import pandas as pd\nimport glob\nimport time\n\nstart_time = time.time()\n\n\n\n\ndef get_json_to_df(file_list = []):\n    df_list = []\n    for json_file in file_list[1]:\n        df_list.append(pd.read_json(json_file))\n    df = pd.concat(df_list)\n    return df\n\ndef merge_dataframes(df1,df2):\n    merged = pd.merge(df1, df2,how='outer', on='dateTime')\n    return merged\n\ndef merged_to_datetime(merged):\n    merged['dateTime'] = pd.to_datetime(merged['dateTime'], format='%m/%d/%y %H:%M:%S')\n    merged = merged.sort_values(by='dateTime')\n    return merged\n\ndef make_new_df_value(x='',column_name=''):\n    try:\n        x = x[column_name]\n    except Exception as e:\n        print(e)\n        x = 0.0\n    return x\n    \n\n\n\n## Creating lists of all the respective files in the directory\nheart_rate_file_list = glob.glob('data/Fitbit_data_export/user-site-export/heart_rate-*')\nsteps_file_list = glob.glob('data/Fitbit_data_export/user-site-export/steps-*')\naltitude_file_list = glob.glob('data/Fitbit_data_export/user-site-export/altitude-*')\ncalories_file_list = glob.glob('data/Fitbit_data_export/user-site-export/calories-*')\n\n\nmaking_file_lists = time.time()\nprint(\"###Time taken to make files lists\",making_file_lists-start_time)\n\n###Time taken to make files lists 17.79209280014038\n\n\n\ndisplay(heart_rate_file_list)\n\n['data/Fitbit_data_export/user-site-export/heart_rate-2018-12-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-31.json']\n\n\n\n## reading json into dataframes\nheart_rate_df = get_json_to_df(file_list = heart_rate_file_list).reset_index()\n## Heart rate contains a sub json that are explicitly converted into column\nheart_rate_df['bpm'] = heart_rate_df['value'].transform(lambda x: make_new_df_value(x,'bpm'))\nheart_rate_df['confidence'] = heart_rate_df['value'].transform(lambda x: make_new_df_value(x,'confidence'))\nheart_rate_df = heart_rate_df.drop(['value','index'],axis=1)\n\n\nsteps_df = get_json_to_df(file_list = steps_file_list).rename(columns={'value': 'steps'})\n\naltitude_df = get_json_to_df(file_list = altitude_file_list).rename(columns={'value': 'altitude'})\ncalories_df = get_json_to_df(file_list = calories_file_list).rename(columns={'value': 'calories'})\n\nload_json_time = time.time()\nprint(\"###Time taken to load and transform json files\",load_json_time-making_file_lists)\n\n\n\nprint('## Time taken',function_time) \n\n\nmerged = merge_dataframes(heart_rate_df,steps_df)\nmerged = merge_dataframes(merged,altitude_df)\nmerged = merge_dataframes(merged,calories_df)\nmerged = merged_to_datetime(merged)\n\nmerged.to_csv('merged_export_data.csv')\n\nmerging_time = time.time()\nprint(\"###Time taken to merge dataframess\",merging_time-load_json_time)\n\n\n\nfunction_time = time.time() - start_time"
  },
  {
    "objectID": "posts/health/Open_Health_Data_Analysis/Fitbit/data_analysis.html",
    "href": "posts/health/Open_Health_Data_Analysis/Fitbit/data_analysis.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "!pip install requests_oauthlib\n!pip install cherrypy\n!pip inastall pandas_profiling\n\n\nRequirement already satisfied: requests_oauthlib in /Users/christopher/anaconda/lib/python3.6/site-packages\nRequirement already satisfied: oauthlib>=0.6.2 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests_oauthlib)\nRequirement already satisfied: requests>=2.0.0 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests_oauthlib)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib)\nRequirement already satisfied: idna<2.6,>=2.5 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib)\nRequirement already satisfied: urllib3<1.22,>=1.21.1 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib)\nYou are using pip version 9.0.1, however version 19.0.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nRequirement already satisfied: cherrypy in /Users/christopher/anaconda/lib/python3.6/site-packages\nYou are using pip version 9.0.1, however version 19.0.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nERROR: unknown command \"inastall\" - maybe you meant \"install\"\n\n\n\nimport pandas as pd\nimport numpy as np\nimport pandas_profiling\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n\n%%html\n<style>\n.output_wrapper, .output {\n    height:auto !important;\n    max-height:30000px; \n}\n.output_scroll {}\n    box-shadow:none !important;\n    webkit-box-shadow:none !important;\n}\n</style>\n\n\n\n\n\ntry:\n    df = pd.read_csv('merged_hr_sleep_steps.csv')\nexcept Exception as e:\n    print('Failed reading file, does it exists?',e)\n    print('Running data creation script..')\n    from get_fitbit_steps_and_hr_data import *\n    df = pd.read_csv('merged_hr_sleep_steps.csv')\n\n\ndisplay(df.describe())\ndisplay(df.tail(10))\n\n\n\n\n\n  \n    \n      \n      heart_rate\n      steps\n      sleep_stage\n    \n  \n  \n    \n      count\n      6815505.00000\n      6834023.00000\n      6834023.00000\n    \n    \n      mean\n      70.73566\n      0.82106\n      0.38562\n    \n    \n      std\n      17.09187\n      7.67283\n      0.53980\n    \n    \n      min\n      36.00000\n      0.00000\n      0.00000\n    \n    \n      25%\n      59.00000\n      0.00000\n      0.00000\n    \n    \n      50%\n      66.00000\n      0.00000\n      0.00000\n    \n    \n      75%\n      76.00000\n      0.00000\n      1.00000\n    \n    \n      max\n      203.00000\n      187.00000\n      3.00000\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      timestamp\n      heart_rate\n      steps\n      sleep_stage\n    \n  \n  \n    \n      6834013\n      2019-01-30 23:50:00\n      nan\n      0.00000\n      1.00000\n    \n    \n      6834014\n      2019-01-30 23:51:00\n      nan\n      0.00000\n      1.00000\n    \n    \n      6834015\n      2019-01-30 23:52:00\n      nan\n      0.00000\n      2.00000\n    \n    \n      6834016\n      2019-01-30 23:53:00\n      nan\n      0.00000\n      2.00000\n    \n    \n      6834017\n      2019-01-30 23:54:00\n      nan\n      0.00000\n      2.00000\n    \n    \n      6834018\n      2019-01-30 23:55:00\n      nan\n      0.00000\n      2.00000\n    \n    \n      6834019\n      2019-01-30 23:56:00\n      nan\n      0.00000\n      1.00000\n    \n    \n      6834020\n      2019-01-30 23:57:00\n      nan\n      0.00000\n      1.00000\n    \n    \n      6834021\n      2019-01-30 23:58:00\n      nan\n      0.00000\n      1.00000\n    \n    \n      6834022\n      2019-01-30 23:59:00\n      nan\n      0.00000\n      1.00000\n    \n  \n\n\n\n\n\npandas_profiling.ProfileReport(df)\n\n\n\n\n\n\n\n    \n        Overview\n    \n    \n    \n        Dataset info\n        \n            \n            \n                Number of variables\n                4 \n            \n            \n                Number of observations\n                6834023 \n            \n            \n                Total Missing (%)\n                0.1% \n            \n            \n                Total size in memory\n                208.6 MiB \n            \n            \n                Average record size in memory\n                32.0 B \n            \n            \n        \n    \n    \n        Variables types\n        \n            \n            \n                Numeric\n                3 \n            \n            \n                Categorical\n                0 \n            \n            \n                Boolean\n                0 \n            \n            \n                Date\n                0 \n            \n            \n                Text (Unique)\n                1 \n            \n            \n                Rejected\n                0 \n            \n            \n                Unsupported\n                0 \n            \n            \n        \n    \n    \n        \n        Warnings\n        sleep_stage has 4378621 / 64.1% zeros Zerossteps has 6691184 / 97.9% zeros Zeros \n    \n\n    \n        Variables\n    \n    \n    \n        heart_rate\n            Numeric\n        \n    \n    \n        \n            \n                \n                    Distinct count\n                    274\n                \n                \n                    Unique (%)\n                    0.0%\n                \n                \n                    Missing (%)\n                    0.3%\n                \n                \n                    Missing (n)\n                    18518\n                \n                \n                    Infinite (%)\n                    0.0%\n                \n                \n                    Infinite (n)\n                    0\n                \n            \n\n        \n        \n            \n\n                \n                    Mean\n                    70.736\n                \n                \n                    Minimum\n                    36\n                \n                \n                    Maximum\n                    203\n                \n                \n                    Zeros (%)\n                    0.0%\n                \n            \n        \n    \n\n\n    \n\n\n\n    \n        Toggle details\n    \n\n\n    \n        Statistics\n        Histogram\n        Common Values\n        Extreme Values\n\n    \n\n    \n        \n            \n                Quantile statistics\n                \n                    \n                        Minimum\n                        36\n                    \n                    \n                        5-th percentile\n                        54\n                    \n                    \n                        Q1\n                        59\n                    \n                    \n                        Median\n                        66\n                    \n                    \n                        Q3\n                        76\n                    \n                    \n                        95-th percentile\n                        109\n                    \n                    \n                        Maximum\n                        203\n                    \n                    \n                        Range\n                        167\n                    \n                    \n                        Interquartile range\n                        17\n                    \n                \n            \n            \n                Descriptive statistics\n                \n                    \n                        Standard deviation\n                        17.092\n                    \n                    \n                        Coef of variation\n                        0.24163\n                    \n                    \n                        Kurtosis\n                        2.7405\n                    \n                    \n                        Mean\n                        70.736\n                    \n                    \n                        MAD\n                        12.664\n                    \n                    \n                        Skewness\n                        1.6186\n                    \n                    \n                        Sum\n                        482100000\n                    \n                    \n                        Variance\n                        292.13\n                    \n                    \n                        Memory size\n                        52.1 MiB\n                    \n                \n            \n        \n        \n            \n        \n        \n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        58.0\n        281839\n        4.1%\n        \n            \n        \n\n        57.0\n        278298\n        4.1%\n        \n            \n        \n\n        59.0\n        277774\n        4.1%\n        \n            \n        \n\n        60.0\n        263826\n        3.9%\n        \n            \n        \n\n        56.0\n        259650\n        3.8%\n        \n            \n        \n\n        61.0\n        254255\n        3.7%\n        \n            \n        \n\n        62.0\n        249826\n        3.7%\n        \n            \n        \n\n        63.0\n        247628\n        3.6%\n        \n            \n        \n\n        64.0\n        238302\n        3.5%\n        \n            \n        \n\n        55.0\n        231245\n        3.4%\n        \n            \n        \n\n        Other values (263)\n        4232862\n        61.9%\n        \n            \n        \n\n\n        \n        \n            Minimum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        36.0\n        8\n        0.0%\n        \n            \n        \n\n        37.0\n        87\n        0.0%\n        \n            \n        \n\n        37.5\n        2\n        0.0%\n        \n            \n        \n\n        38.0\n        203\n        0.0%\n        \n            \n        \n\n        38.5\n        1\n        0.0%\n        \n            \n        \n\n\n            Maximum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        196.0\n        3\n        0.0%\n        \n            \n        \n\n        197.0\n        1\n        0.0%\n        \n            \n        \n\n        201.0\n        2\n        0.0%\n        \n            \n        \n\n        202.0\n        1\n        0.0%\n        \n            \n        \n\n        203.0\n        1\n        0.0%\n        \n            \n        \n\n\n        \n    \n\n\n    \n        sleep_stage\n            Numeric\n        \n    \n    \n        \n            \n                \n                    Distinct count\n                    6\n                \n                \n                    Unique (%)\n                    0.0%\n                \n                \n                    Missing (%)\n                    0.0%\n                \n                \n                    Missing (n)\n                    0\n                \n                \n                    Infinite (%)\n                    0.0%\n                \n                \n                    Infinite (n)\n                    0\n                \n            \n\n        \n        \n            \n\n                \n                    Mean\n                    0.38562\n                \n                \n                    Minimum\n                    0\n                \n                \n                    Maximum\n                    3\n                \n                \n                    Zeros (%)\n                    64.1%\n                \n            \n        \n    \n\n\n    \n\n\n\n    \n        Toggle details\n    \n\n\n    \n        Statistics\n        Histogram\n        Common Values\n        Extreme Values\n\n    \n\n    \n        \n            \n                Quantile statistics\n                \n                    \n                        Minimum\n                        0\n                    \n                    \n                        5-th percentile\n                        0\n                    \n                    \n                        Q1\n                        0\n                    \n                    \n                        Median\n                        0\n                    \n                    \n                        Q3\n                        1\n                    \n                    \n                        95-th percentile\n                        1\n                    \n                    \n                        Maximum\n                        3\n                    \n                    \n                        Range\n                        3\n                    \n                    \n                        Interquartile range\n                        1\n                    \n                \n            \n            \n                Descriptive statistics\n                \n                    \n                        Standard deviation\n                        0.5398\n                    \n                    \n                        Coef of variation\n                        1.3998\n                    \n                    \n                        Kurtosis\n                        0.33213\n                    \n                    \n                        Mean\n                        0.38562\n                    \n                    \n                        MAD\n                        0.49415\n                    \n                    \n                        Skewness\n                        1.0307\n                    \n                    \n                        Sum\n                        2635400\n                    \n                    \n                        Variance\n                        0.29139\n                    \n                    \n                        Memory size\n                        52.1 MiB\n                    \n                \n            \n        \n        \n            \n        \n        \n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        0.0\n        4378621\n        64.1%\n        \n            \n        \n\n        1.0\n        2266369\n        33.2%\n        \n            \n        \n\n        2.0\n        134154\n        2.0%\n        \n            \n        \n\n        1.5\n        41627\n        0.6%\n        \n            \n        \n\n        3.0\n        10231\n        0.1%\n        \n            \n        \n\n        2.5\n        3021\n        0.0%\n        \n            \n        \n\n\n        \n        \n            Minimum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        0.0\n        4378621\n        64.1%\n        \n            \n        \n\n        1.0\n        2266369\n        33.2%\n        \n            \n        \n\n        1.5\n        41627\n        0.6%\n        \n            \n        \n\n        2.0\n        134154\n        2.0%\n        \n            \n        \n\n        2.5\n        3021\n        0.0%\n        \n            \n        \n\n\n            Maximum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        1.0\n        2266369\n        33.2%\n        \n            \n        \n\n        1.5\n        41627\n        0.6%\n        \n            \n        \n\n        2.0\n        134154\n        2.0%\n        \n            \n        \n\n        2.5\n        3021\n        0.0%\n        \n            \n        \n\n        3.0\n        10231\n        0.1%\n        \n            \n        \n\n\n        \n    \n\n\n    \n        steps\n            Numeric\n        \n    \n    \n        \n            \n                \n                    Distinct count\n                    172\n                \n                \n                    Unique (%)\n                    0.0%\n                \n                \n                    Missing (%)\n                    0.0%\n                \n                \n                    Missing (n)\n                    0\n                \n                \n                    Infinite (%)\n                    0.0%\n                \n                \n                    Infinite (n)\n                    0\n                \n            \n\n        \n        \n            \n\n                \n                    Mean\n                    0.82106\n                \n                \n                    Minimum\n                    0\n                \n                \n                    Maximum\n                    187\n                \n                \n                    Zeros (%)\n                    97.9%\n                \n            \n        \n    \n\n\n    \n\n\n\n    \n        Toggle details\n    \n\n\n    \n        Statistics\n        Histogram\n        Common Values\n        Extreme Values\n\n    \n\n    \n        \n            \n                Quantile statistics\n                \n                    \n                        Minimum\n                        0\n                    \n                    \n                        5-th percentile\n                        0\n                    \n                    \n                        Q1\n                        0\n                    \n                    \n                        Median\n                        0\n                    \n                    \n                        Q3\n                        0\n                    \n                    \n                        95-th percentile\n                        0\n                    \n                    \n                        Maximum\n                        187\n                    \n                    \n                        Range\n                        187\n                    \n                    \n                        Interquartile range\n                        0\n                    \n                \n            \n            \n                Descriptive statistics\n                \n                    \n                        Standard deviation\n                        7.6728\n                    \n                    \n                        Coef of variation\n                        9.3451\n                    \n                    \n                        Kurtosis\n                        155.97\n                    \n                    \n                        Mean\n                        0.82106\n                    \n                    \n                        MAD\n                        1.6078\n                    \n                    \n                        Skewness\n                        11.93\n                    \n                    \n                        Sum\n                        5611100\n                    \n                    \n                        Variance\n                        58.872\n                    \n                    \n                        Memory size\n                        52.1 MiB\n                    \n                \n            \n        \n        \n            \n        \n        \n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        0.0\n        6691184\n        97.9%\n        \n            \n        \n\n        7.0\n        6279\n        0.1%\n        \n            \n        \n\n        8.0\n        5984\n        0.1%\n        \n            \n        \n\n        9.0\n        5058\n        0.1%\n        \n            \n        \n\n        6.0\n        5050\n        0.1%\n        \n            \n        \n\n        10.0\n        4297\n        0.1%\n        \n            \n        \n\n        11.0\n        3783\n        0.1%\n        \n            \n        \n\n        12.0\n        3430\n        0.1%\n        \n            \n        \n\n        4.0\n        3308\n        0.0%\n        \n            \n        \n\n        13.0\n        3144\n        0.0%\n        \n            \n        \n\n        Other values (162)\n        102506\n        1.5%\n        \n            \n        \n\n\n        \n        \n            Minimum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        0.0\n        6691184\n        97.9%\n        \n            \n        \n\n        1.0\n        651\n        0.0%\n        \n            \n        \n\n        2.0\n        483\n        0.0%\n        \n            \n        \n\n        3.0\n        373\n        0.0%\n        \n            \n        \n\n        4.0\n        3308\n        0.0%\n        \n            \n        \n\n\n            Maximum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        168.0\n        1\n        0.0%\n        \n            \n        \n\n        173.0\n        1\n        0.0%\n        \n            \n        \n\n        176.0\n        1\n        0.0%\n        \n            \n        \n\n        178.0\n        1\n        0.0%\n        \n            \n        \n\n        187.0\n        1\n        0.0%\n        \n            \n        \n\n\n        \n    \n\n\n    \n        timestamp\n            Categorical, Unique\n        \n    \n  \n    \n      First 3 values\n    \n  \n  \n    \n      2018-05-14 22:43:00\n    \n    \n      2018-02-03 12:59:49\n    \n    \n      2017-11-23 17:53:42\n    \n  \n\n\n  \n    \n      Last 3 values\n    \n  \n  \n    \n      2018-09-19 06:13:13\n    \n    \n      2018-08-29 20:26:45\n    \n    \n      2017-12-11 03:22:00\n    \n  \n\n\n    \n        Toggle details\n    \n\n\n    First 10 values\n    \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        2017-07-12 00:00:00\n        1\n        0.0%\n        \n            \n        \n\n        2017-07-12 00:01:00\n        1\n        0.0%\n        \n            \n        \n\n        2017-07-12 00:02:00\n        1\n        0.0%\n        \n            \n        \n\n        2017-07-12 00:03:00\n        1\n        0.0%\n        \n            \n        \n\n        2017-07-12 00:04:00\n        1\n        0.0%\n        \n            \n        \n\n\n    Last 10 values\n    \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        2019-01-30 23:55:00\n        1\n        0.0%\n        \n            \n        \n\n        2019-01-30 23:56:00\n        1\n        0.0%\n        \n            \n        \n\n        2019-01-30 23:57:00\n        1\n        0.0%\n        \n            \n        \n\n        2019-01-30 23:58:00\n        1\n        0.0%\n        \n            \n        \n\n        2019-01-30 23:59:00\n        1\n        0.0%\n        \n            \n        \n\n\n\n\n    \n        Correlations\n    \n    \n    \n    \n\n    \n        Sample\n    \n    \n    \n        \n  \n    \n      \n      timestamp\n      heart_rate\n      steps\n      sleep_stage\n    \n  \n  \n    \n      0\n      2017-07-12 00:00:00\n      nan\n      0.00000\n      0.00000\n    \n    \n      1\n      2017-07-12 00:01:00\n      nan\n      0.00000\n      0.00000\n    \n    \n      2\n      2017-07-12 00:02:00\n      nan\n      0.00000\n      0.00000\n    \n    \n      3\n      2017-07-12 00:03:00\n      nan\n      0.00000\n      0.00000\n    \n    \n      4\n      2017-07-12 00:04:00\n      nan\n      0.00000\n      0.00000"
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/EDA.html",
    "href": "posts/music/Music_transcription_fastai/notebooks/EDA.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\n\nfrom spectrogram_class import spectrogram"
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/EDA.html#meta-data-orchideasol",
    "href": "posts/music/Music_transcription_fastai/notebooks/EDA.html#meta-data-orchideasol",
    "title": "Most interesting blog on Earth!",
    "section": "Meta data (OrchideaSOL)",
    "text": "Meta data (OrchideaSOL)\nThe main dataset, other than the raw audio files, we are going to explore is the metadata dataframe. Lets first explore the Orchetral dataset.\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\n\nmeta_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13265 entries, 0 to 13264\nData columns (total 17 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   Path                         13265 non-null  object \n 1   Family (abbr.)               13265 non-null  object \n 2   Family (in full)             13265 non-null  object \n 3   Instrument (abbr.)           13265 non-null  object \n 4   Instrument (in full)         13265 non-null  object \n 5   Technique (abbr.)            13265 non-null  object \n 6   Technique (in full)          13265 non-null  object \n 7   Pitch                        13265 non-null  object \n 8   Pitch ID (if applicable)     13162 non-null  float64\n 9   Dynamics                     13265 non-null  object \n 10  Dynamics ID (if applicable)  12646 non-null  float64\n 11  Instance ID                  13262 non-null  float64\n 12  Mute (abbr.)                 13265 non-null  object \n 13  Mute (in full)               13265 non-null  object \n 14  String ID (if applicable)    7516 non-null   float64\n 15  Needed digital retuning      13265 non-null  bool   \n 16  Fold                         13265 non-null  int64  \ndtypes: bool(1), float64(4), int64(1), object(11)\nmemory usage: 1.6+ MB\n\n\n\nmeta_df.nunique().sort_values()\n\nNeeded digital retuning            2\nFold                               5\nFamily (abbr.)                     5\nFamily (in full)                   5\nDynamics ID (if applicable)        5\nString ID (if applicable)          6\nDynamics                           7\nMute (abbr.)                       7\nMute (in full)                     7\nInstance ID                       13\nInstrument (abbr.)                16\nInstrument (in full)              16\nTechnique (in full)               52\nTechnique (abbr.)                 56\nPitch ID (if applicable)          90\nPitch                            105\nPath                           13265\ndtype: int64\n\n\nThe Series above represents the number of unique values across all features. From the object types, unique values and intuition, we can conclude that all bu the path to the audio files are categorical features.\n\n# Check for null values\nmeta_df.isna().sum()\n\nPath                              0\nFamily (abbr.)                    0\nFamily (in full)                  0\nInstrument (abbr.)                0\nInstrument (in full)              0\nTechnique (abbr.)                 0\nTechnique (in full)               0\nPitch                             0\nPitch ID (if applicable)        103\nDynamics                          0\nDynamics ID (if applicable)     619\nInstance ID                       3\nMute (abbr.)                      0\nMute (in full)                    0\nString ID (if applicable)      5749\nNeeded digital retuning           0\nFold                              0\ndtype: int64\n\n\nThe only columns we are interested in are the Path, Instrument and Pitch. We can see that out of all three, only Pitch ID contains numm values. Now lets take a look at what the null values are, represented in pitch.\n\nmeta_df['Pitch'].unique()\n\narray(['A#0', 'A#1', 'A#2', 'A#3', 'A#4', 'A0', 'A1', 'A2', 'A3', 'A4',\n       'B0', 'B1', 'B2', 'B3', 'C#1', 'C#2', 'C#3', 'C#4', 'C1', 'C2',\n       'C3', 'C4', 'D#1', 'D#2', 'D#3', 'D#4', 'D1', 'D2', 'D3', 'D4',\n       'E1', 'E2', 'E3', 'E4', 'F#1', 'F#2', 'F#3', 'F#4', 'F1', 'F2',\n       'F3', 'F4', 'G#0', 'G#1', 'G#2', 'G#3', 'G#4', 'G1', 'G2', 'G3',\n       'G4', 'N', 'C#1_D#1', 'C1_C#1', 'C1_G1', 'D1_D#1', 'D1_F1', 'B4',\n       'C#5', 'C5', 'D#5', 'D5', 'E5', 'F5', 'A#5', 'A5', 'B5', 'F#5',\n       'G#5', 'G5', 'C#6', 'C6', 'D#6', 'D6', 'E6', 'F#6', 'F6', 'G6',\n       'A#6', 'A#7', 'A6', 'A7', 'B6', 'B7', 'C#7', 'C#8', 'C7', 'C8',\n       'D#7', 'D7', 'E7', 'F#7', 'F7', 'G#6', 'G#7', 'G7', 'C#3_B5',\n       'C#3_C#4', 'C#3_C#5', 'C#3_C#6', 'C#3_D#6', 'C#3_F5', 'C#3_F6',\n       'C#3_G#4', 'C#3_G#5'], dtype=object)\n\n\n\n# Getting the unique values of Pitch where the Pitch ID are missing.\nmeta_df[['Pitch', 'Pitch ID (if applicable)']] \\\n            [meta_df['Pitch ID (if applicable)'].isnull()]['Pitch'].unique()\n\narray(['N', 'C#1_D#1', 'C1_C#1', 'C1_G1', 'D1_D#1', 'D1_F1', 'C#3_B5',\n       'C#3_C#4', 'C#3_C#5', 'C#3_C#6', 'C#3_D#6', 'C#3_F5', 'C#3_F6',\n       'C#3_G#4', 'C#3_G#5'], dtype=object)\n\n\nIf you are not familiar with music, let me give you some insight (I am either grade 5 or 6 on piano ABRSM, cant remember which since I lost my certificate):\n\n\nNone of the terms make sense to me!\n\n\nThere is no note N in music, nor can you be C1 and C# at the same time! The documentation on OrchideaSOL doesnt include any useful information on the subject either. The only logical way to deal with the null values is to simple drop them.\nTo justify the decision, lets look at the precentage of the missing values.\n\n# Getting the fractiong of missing values in the Pitch ID feature.\nmeta_df['Pitch ID (if applicable)'].isnull().sum()/meta_df.shape[0]\n\n0.007764794572182435\n\n\nIts not even 1% of our data, we dont lose much training information from losing these unexplanable samples notes."
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/EDA.html#orchideasol-instrument-analysis",
    "href": "posts/music/Music_transcription_fastai/notebooks/EDA.html#orchideasol-instrument-analysis",
    "title": "Most interesting blog on Earth!",
    "section": "OrchideaSOL instrument analysis",
    "text": "OrchideaSOL instrument analysis\nNow that we have an understading of the note missing values, lets have a look at the instruments distribution.\n\norchidea_instrument = meta_df['Instrument (in full)'].value_counts(normalize=True)\norchidea_instrument\n\nViolin            0.149793\nViola             0.147154\nContrabass        0.123332\nCello             0.120090\nAccordion         0.065737\nTrombone          0.050509\nTrumpet in C      0.044478\nFrench Horn       0.044403\nFlute             0.039879\nHarp              0.038221\nBass Tuba         0.037693\nClarinet in Bb    0.030607\nAlto Saxophone    0.028421\nBassoon           0.026988\nGuitar            0.026611\nOboe              0.026084\nName: Instrument (in full), dtype: float64\n\n\n\nplt.figure(figsize = (18, 8))\nplt.bar(orchidea_instrument.index, orchidea_instrument.values)\nplt.title('Instrument distrubution of OrchideaSOL')\nplt.xlabel('Instruments')\nplt.ylabel('Fractions')\nplt.show()\n\n\n\n\nWe can see that violin, viola, contrabass and cello has more entries comparing to the rest of the instruments.\nSince we have some unbalanced data, we can either comtemplate the bias through adjusting the loss function (increasing the penalty for misclassifying minor classes), or creating more data for the minorities through audio file augmentation, more on that in the other notebooks!"
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/EDA.html#orchideasol-instrumentnotes-analysis",
    "href": "posts/music/Music_transcription_fastai/notebooks/EDA.html#orchideasol-instrumentnotes-analysis",
    "title": "Most interesting blog on Earth!",
    "section": "OrchideaSOL instrument/notes analysis",
    "text": "OrchideaSOL instrument/notes analysis\nNow that we have a sense of how the instruments distribution looks like, it is also useful to look at the distribution of the instruments corresponding to the notes.\n\n# Creating crosstab for instrument and Pitch ID, \n# which represents the value counts for all instrument/Pitch ID combinations\n\nplt.figure(figsize = (12, 10))\nsns.heatmap(pd.crosstab(meta_df['Instrument (in full)'], \n                                meta_df['Pitch ID (if applicable)']))\n\n<AxesSubplot:xlabel='Pitch ID (if applicable)', ylabel='Instrument (in full)'>\n\n\n\n\n\nWe can see that the majority of notes for viola and violin distributed in the middle of the note range. Wheareas Cello and Contrabass has distribution shift towards the lower ends of the range.\nIt is also useful to note that: * Accoridian, guitar and harp has lower number of note counts, * However, their notes are more evenly distributed across the note range."
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/classic_transcription.html",
    "href": "posts/music/Music_transcription_fastai/notebooks/classic_transcription.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import librosa\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom keras.layers import LSTM, ConvLSTM1D, Input, MaxPool1D\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\nfrom keras.layers.core import Dense, Dropout, Flatten\nfrom keras.models import Model, Sequential\nfrom librosa import display\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom classic_generator import classic_generator\nfrom spectrogram_class import spectrogram\n\nModuleNotFoundError: No module named 'classic_generator'\n\n\n\n!pwd\n\n/Users/ylee/Downloads\n\n\n\nimport pickle\n\ndef save_history(history, path):\n    with open(path, 'wb+') as f:\n        pickle.dump(history, f)\n        \ndef load_history(path):\n    with open(path, 'rb') as f:\n        return pickle.load(f)\n\n\nMusic transcription - MusicNet LSTM\nThis is a successive notebook from RNN models for OrchideaSOL dataset. In that notebook, we have demonstrate that the RNN model, in particularly LSTM, is able to capture the features of instrument and pitch in a raw audio file.\nThe purpose of this notebook is to apply the same principle on the more complicated MusicNet dataset. On our previous notebook, we have had mild success of predicting more than 50% in both insturment and notes, for short single instrumental audio files.\nAlthough it is the same principle, this time we are going to need to convert the whole sequence of audio file into the transcription in to the same music file length. To be more clear:\n\nFor all timesteps in the spectrogram, we are going to produce the classification of instruments and note.\n\nThe end goal of this project is to convert the corresponding output into a MIDI file.\nThe model architecture will be to be fine tuned since we are facing the challenges below:\n\nWe are making classifying prediction for every timestep,\n\nExcluding the expected dimension of time and notes as output, we also need the same classification for every instruments, which is 11 of them.\nThe audio files are not neccessarily made of single instrument, which means that our RNN model will need to find the relation of the sound signature for each instruments, in the sea of spectrograms magnitude.\nThe audio files has different length, range from 3 minutes to 20 minutes. Padding will be required, however, zeros padding will cause problem of exploding/vanishing gradient in RNN model.\n\n\n# Testing if GPU is enabled, if it's not, I am not running this notebook!\ntf.config.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\nimport tensorflow as tf\ntf.test.is_built_with_cuda()\ntf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n\nWARNING:tensorflow:From /var/folders/n9/gg0_718d4db65yyqhxlk09vh0000gp/T/ipykernel_38191/1822807733.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\nMetal device set to: Apple M1\n\nsystemMemory: 8.00 GB\nmaxCacheSize: 2.67 GB\n\n\n\n2022-09-16 16:43:02.967559: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-16 16:43:02.968025: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\nTrue\n\n\n\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n\n[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 3715140090454835554\nxla_global_id: -1\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nlocality {\n  bus_id: 1\n}\nincarnation: 11023430109333998903\nphysical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\nxla_global_id: -1\n]\n\n\n2022-09-16 16:43:03.107869: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-16 16:43:03.107888: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\nThroughout this notebook, we will be using integer code number for instrument and notes classification. The instrument and note lists below are generated by concatenating the training labels, which are seperated csv files.\n\ninstrument_list = [1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\nnote_list = [21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62,\n             63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]\n\n\n\nRNN-LSTM\nTo begin our construction of RNN model, we introduce the architect idea for our model.\nSince our model has to output the MIDI file equivalence for each instruments, we will be forking our output into 11 Dense layers, with each layer reprenting one instruments.\nEach output will have a dimension of : \\[\n\\text{\\# of instruments} \\times (\\text{\\# of timesteps} \\times \\text{\\# of notes})\n\\]\nThe architect idea for the model is as follow: 1. Passing input spectrogrom into convolutional layers, in order to extract features 1. Passing features in to multiple layers of Dense layers, to embed the features 1. Passing embedded features into LSTM model, to predict labels for each timestep depending on the sequnce of embedded features.\n\nUPDATE:\nAfter countless time wasted on differnet architect and changing parameter, we have decided to change the input shape, instead of feeding the batch of spectrogram which are padded with the longest time dimension across the batch, we will be taken random snipshot of fixed time length across all batch. This workaround is to prevent:\n* Long padding of zeros for shorter audio files, to prevent vanishing gradient * Long training time and * Reduce GPU memory usage (VsCode tends to close unexpectedly and doesnt release allocated GPU space before closing, making a full system restart required.)\nWe have decided to take 200 timeslices for the model input, with our default hop length (number of timestep to skip when computing fast-fourier transform for spectrogram), and using the sample rate of audio, which is 44100HZ, we can calculate that:\n\\[\n\\text{Length in second of inputs} = \\frac{300 \\times 200}{44100} \\\\\n\\approx 16 \\, \\text{seconds}\n\\]\n\ndef instrument_layer_simple(input, out_name):\n    '''\n    Support function from building multiple output models. Returns an output layer\n    of TimeDistributed Dense layer, corresponds to the number of notes in labels\n    \n    Input:\n    input: Preceeding input layer to be fed into.\n    out_name: str, name of output layer names to be assigned\n    '''\n    out = layers.TimeDistributed(Dense(len(note_list), activation='sigmoid'),\n                                 name=out_name)(input)\n    return out\n\n\ninstrument_list\n\n[1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\nNow we can start to build our (hopefully will be working this time) model.\n\nBATCH_SIZE = 8\n\n# We have choose to work with 128 frequency bins,\n# and the None input shape indicates the dynamic length of time dimension\ninp = Input((None, 128, 1), batch_size=BATCH_SIZE)\nnormalizer = layers.BatchNormalization()(inp)\n\n# Extracting features from first convolutional layers.\n# Padding set to same to maintain same time dimension\nconv_1 = Conv2D(10, (50, 40), padding = 'same')(normalizer)\n\nnormalizer_2 = layers.BatchNormalization()(conv_1)\n# pool_1 = layers.TimeDistributed(MaxPool1D(2))(normalizer_2)\n# flatten_1 = flatten = layers.TimeDistributed(layers.Flatten())(pool_1)\n\n# conv_2 = layers.Conv2D(5, (1000, 30), padding = 'same')(pool_1)\n# pool_2 = layers.TimeDistributed(layers.MaxPool1D(2))(conv_2)\n\n# Flatten each filter layer for each timesteps\nflatten = layers.TimeDistributed(Flatten())(normalizer_2)\n\n# Feed in the flattened layers to multiple layers of Dense layer,\n# Performing embedding\nDense_1 = layers.TimeDistributed(Dense(300, activation = 'relu'))(flatten)\nnormalizer_3 = layers.BatchNormalization()(Dense_1)\ndrop_1 = layers.Dropout(0.2)(normalizer_3)\n\nDense_2 = layers.TimeDistributed(Dense(150, activation = 'relu'))(drop_1)\nnormalizer_4 = layers.BatchNormalization()(Dense_2)\ndrop_2 = layers.Dropout(0.2)(normalizer_4)\n\nDense_3 = layers.TimeDistributed(Dense(50, activation = 'relu'))(drop_2)\nnormalizer_5 = layers.BatchNormalization()(Dense_3)\ndrop_3 = layers.Dropout(0.2)(normalizer_5)\n\nDense_3 = layers.TimeDistributed(Dense(200, activation = 'relu'))(drop_2)\nnormalizer_5 = layers.BatchNormalization()(Dense_3)\ndrop_3 = layers.Dropout(0.2)(normalizer_5)\n\n# Lastly, put the decoded features in to LSTM layer\nlast_lstm = LSTM(500, return_sequences=True, dropout = 0.3)(drop_3)\n\n# Outputing to final Dense layer with sigmoid activation,\n# To predict the note labels for each timesteps\nsimple_lstm_model = Model(inp, [instrument_layer_simple(last_lstm,\n                                f\"instrument_{ins}\") for ins in instrument_list])\n\n\n2022-09-16 16:43:04.148904: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-16 16:43:04.148928: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\n\n\ntf.keras.utils.plot_model(simple_lstm_model, show_shapes=True, show_dtype=True)\n\n\n\n\n\nclassic_train_generator =  classic_generator(mode='train', batch_size=BATCH_SIZE)\n                        # (tf.dtypes.float32, tf.dtypes.bool))\n\nclassic_eval_generator = classic_generator(mode='test', batch_size=BATCH_SIZE)\n\n\n# Getting the path to the latest trained model\nnewest_ckpt = !ls -dt $PWD/../models/classic_truc_conv_to_lstm/* | head -1\nnewest_ckpt = newest_ckpt[0]\nnewest_ckpt\n\n'/Users/ylee/Documents/Music_transcription_fastai/notebooks/../models/classic_truc_conv_to_lstm/20220916_135022_02_classic_truc_conv_to_lstm'\n\n\n\n# Run the following code to load the newest model\n\nsimple_lstm_model = tf.keras.models.load_model(newest_ckpt, compile=False)\n\nSince our prediction will be a really sparse metrics, with a few 1s, we will need to define our custom loss function such that the false negatives are hugely reduced. We will be using the weighted_cross_entropy_with_logits in tensorflow, and setting positive weight which is larger than 1.\n\ndef weighted_cross_entropy_with_logits(labels, logits):\n    loss = tf.nn.weighted_cross_entropy_with_logits(\n        labels, logits, pos_weight = 150\n        )\n    return loss\n\ndef my_loss():\n    return weighted_cross_entropy_with_logits\n\n# As mentioned before, since our prediction is a sparse multilabel problem, \n# the accuracy might not makes muc of sense, in addition, we will be adding\n# AUC for each instrument to gauge how well the model is performing\nsimple_lstm_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = 0.0005),\n                          loss={f\"instrument_{ins}\": my_loss()\n                                for ins in instrument_list},\n                          metrics=['accuracy', tf.keras.metrics.AUC()])\n\nNameError: name 'simple_lstm_model' is not defined\n\n\n\nfrom datetime import datetime\n\n# Define checkpoint to save model for every epoch\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    f\"../models/classic_truc_conv_to_lstm/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_classic_truc_conv_to_lstm\",\n    monitor='val_accuracy',\n    save_freq='epoch')\n    \n# Define checkpoint to save model if validation loss is decreasing\nearly_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)\n\n\nsimple_lstm_history = simple_lstm_model.fit(classic_train_generator, \n                                                epochs=5,\n                                                validation_data=classic_eval_generator,\n                                                validation_freq=1,\n                                                # use_multiprocessing= True,\n                                                # workers= 3,\n                                                verbose=1,\n                                                callbacks=[ckpt_callback])\n\nNameError: name 'tf' is not defined\n\n\n\n# simple_lstm_model.save('../models/classic_full_convlstm/')\n\nSince we have multiple output with 2 dimensional output, it is easier to use visualization to gauge how well the model is performing.\n\ndef musicnet_eval(model, generator, ins):\n    '''\n    Supporting function to  plot the predicted label and true labels\n    of for specified instrument.\n    \n    Input:\n    model: model to be used\n    generator: testing Sequence generator to be used\n    ins: int, instrument number, ranged from 0 to 10\n    '''\n    _feature, _label = generator.__getitem__(0)\n    _prediction = model.predict(_feature)\n    fig = plt.figure(figsize= (18, 6))\n\n    ax_1 = fig.add_subplot(121)\n    ax_1.set_title('Prediction')\n    sns.heatmap(_prediction[ins][0], ax = ax_1)\n\n    ax_2 = fig.add_subplot(122)\n    sns.heatmap(_label[f\"instrument_{instrument_list[ins]}\"][0], ax = ax_2)\n    ax_2.set_title('True label')\n\n    plt.xlabel('Frequency bins')\n    plt.ylabel('Timesteps')\n\n    plt.tight_layout()\n    plt.show()\n\n\ntest_generator = classic_generator(mode='test', batch_size=1)\n\n\nmusicnet_eval(simple_lstm_model, test_generator, 4)\n\n1/1 [==============================] - 2s 2s/step\n\n\n\n\n\n\ntest_set = test_generator.__getitem__(2)\n\n\ntest_set[0].shape\n\n(1, 200, 128, 1)\n\n\n\npredict_test = simple_lstm_model.predict(test_set[0])\n\n1/1 [==============================] - 0s 25ms/step\n\n\n\ninstrument_list\n\n[1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\n\nsns.heatmap(test_set[1]['instrument_41'][0])\n\n<AxesSubplot:>\n\n\n\n\n\n\ninstrument_list\n\n[1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\nThe problem with this model is that the model is predicting contant value along timesteps. The model focused and retained the sequential relation across timesteps too much. And eventually learned to predict zeros for all scenarios. Look at figure below for a better visualization.\n\n\n\n2 lstm\nOur first model produced unsatisfactory results, which implies we might have to start from a simpler model, we will be starting again at a model with 2 LSTM layers.\n\nBATCH_SIZE = 16\n\ninp = Input((None, 128), batch_size=BATCH_SIZE)\n\n# 2 LSTM layers\nx = LSTM(500, return_sequences=True, dropout = 0.3, stateful = True)(inp)\nx = LSTM(500, return_sequences=True, dropout = 0.3, stateful = True)(x)\n\nlstm_2 = Model(inp, [instrument_layer_simple(x,\n                                f\"instrument_{ins}\") for ins in instrument_list])\n\n\ntf.keras.utils.plot_model(lstm_2, show_shapes=True, show_dtype=True)\n\n\n\n\n\nlstm2_train_generator =  classic_generator(mode='train', batch_size=BATCH_SIZE, expand_dim = False)\n                        # (tf.dtypes.float32, tf.dtypes.bool))\n\nlstm2_eval_generator = classic_generator(mode='test', batch_size=BATCH_SIZE, expand_dim = False, \n                                            preprocess = False)\n\n\ndef weighted_cross_entropy_with_logits(labels, logits):\n    loss = tf.nn.weighted_cross_entropy_with_logits(\n        labels, logits, pos_weight = 2\n        )\n    return loss\n\ndef my_loss():\n    return weighted_cross_entropy_with_logits\n\n\nlstm_2.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = 0.001),\n                          loss={f\"instrument_{ins}\": my_loss()\n                                for ins in instrument_list},\n                          metrics=['accuracy', tf.keras.metrics.AUC()])\n\n\nfrom datetime import datetime\n\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    f\"../models/classic_lstm2/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_classic_lstm2\",\n    monitor='val_accuracy',\n    save_freq='epoch')\n    \nearly_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)\n\n\nlstm2_history = lstm_2.fit(lstm2_train_generator, \n                            epochs=10,\n                            validation_data=lstm2_eval_generator,\n                            validation_freq=1,\n                            use_multiprocessing= True,\n                            workers= 3,\n                            verbose=1,\n                            callbacks=[ckpt_callback])\n\nNameError: name 'lstm_2' is not defined\n\n\n\nmusicnet_eval(lstm_2, test_generator, 3)\n\n1/1 [==============================] - 0s 37ms/step\n\n\n\n\n\nWe still observe the same pattern, that constant values are passed along the timesteps.\nThe x-axis above represents the 81 notes, and y-axis represetns the timesteps.\nThe figure on the left represents the prediction of our model, whereas the right figure represents the true labels.\n\n\nConv to lstm\nAttempts last architecture of padding filters of convolutional layers to LSTM layers. This is the most basic model that extract features by convolutional layers, and passing the feature through the sequnce to make a prediction.\n\nBATCH_SIZE = 8\n\n# We have choose to work with 128 frequency bins,\n# and the None input shape indicates the dynamic length of time dimension\ninp = Input((None, 128, 1), batch_size=BATCH_SIZE)\nnormalizer = layers.BatchNormalization()(inp)\n\n# Extracting features from first convolutional layers.\n# Padding set to same to maintain same time dimension\nconv_1 = Conv2D(30, (50, 40), padding = 'same')(normalizer)\n\nnormalizer_2 = layers.BatchNormalization()(conv_1)\n# pool_1 = layers.TimeDistributed(MaxPool1D(2))(normalizer_2)\n# flatten_1 = flatten = layers.TimeDistributed(layers.Flatten())(pool_1)\n\n# conv_2 = layers.Conv2D(5, (1000, 30), padding = 'same')(pool_1)\n# pool_2 = layers.TimeDistributed(layers.MaxPool1D(2))(conv_2)\n\n# Flatten each filter layer for each timesteps\nflatten = layers.TimeDistributed(Flatten())(normalizer_2)\n\ndrop_1 = layers.Dropout(0.3)(flatten)\n# # Feed in the flattened layers to multiple layers of Dense layer,\n# # Performing embedding\n# Dense_1 = layers.TimeDistributed(Dense(300, activation = 'relu'))(flatten)\n# normalizer_3 = layers.BatchNormalization()(Dense_1)\n# drop_1 = layers.Dropout(0.2)(normalizer_3)\n\n# Dense_2 = layers.TimeDistributed(Dense(150, activation = 'relu'))(drop_1)\n# normalizer_4 = layers.BatchNormalization()(Dense_2)\n# drop_2 = layers.Dropout(0.2)(normalizer_4)\n\n# Dense_3 = layers.TimeDistributed(Dense(50, activation = 'relu'))(drop_2)\n# normalizer_5 = layers.BatchNormalization()(Dense_3)\n# drop_3 = layers.Dropout(0.2)(normalizer_5)\n\n# Dense_3 = layers.TimeDistributed(Dense(200, activation = 'relu'))(drop_2)\n# normalizer_5 = layers.BatchNormalization()(Dense_3)\n# drop_3 = layers.Dropout(0.2)(normalizer_5)\n\n# Lastly, put the decoded features in to LSTM layer\nlast_lstm = LSTM(500, return_sequences=True, dropout = 0.3)(drop_1)\n\n# Outputing to final Dense layer with sigmoid activation,\n# To predict the note labels for each timesteps\nconv_to_lstm_model = Model(inp, [instrument_layer_simple(last_lstm,\n                                f\"instrument_{ins}\") for ins in instrument_list])\n\n\ntf.keras.utils.plot_model(conv_to_lstm_model, show_shapes=True, show_dtype=True)\n\n\n\n\n\nclassic_train_generator =  classic_generator(mode='train', batch_size=BATCH_SIZE)\n                        # (tf.dtypes.float32, tf.dtypes.bool))\n\nclassic_eval_generator = classic_generator(mode='test', batch_size=BATCH_SIZE, preprocess = False)\n\n\ndef weighted_cross_entropy_with_logits(labels, logits):\n    loss = tf.nn.weighted_cross_entropy_with_logits(\n        labels, logits, pos_weight = 5\n        )\n    return loss\n\ndef my_loss():\n    return weighted_cross_entropy_with_logits\n\n# As mentioned before, since our prediction is a sparse multilabel problem, \n# the accuracy might not makes muc of sense, in addition, we will be adding\n# AUC for each instrument to gauge how well the model is performing\nconv_to_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001),\n                          loss={f\"instrument_{ins}\": my_loss() \n                                for ins in instrument_list},\n                          metrics=['accuracy', tf.keras.metrics.AUC()])\n\n\nfrom datetime import datetime\n\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    f\"../models/classic_direct_conv_to_lstm/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_classic_direct_conv_to_lstm\",\n    monitor='val_accuracy',\n    save_freq='epoch')\n    \nearly_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)\n\n\nconv_to_lstm_history = conv_to_lstm_model.fit(classic_train_generator, \n                                                epochs=10,\n                                                validation_data=classic_eval_generator,\n                                                validation_freq=1,\n                                                # use_multiprocessing= True,\n                                                # workers= 3,\n                                                verbose=1,\n                                                # class_weight= {0: 0.11, 1: 0.89}, \n                                                callbacks=[ckpt_callback])\n\nEpoch 1/10\n\n\n2022-08-07 18:44:58.981777: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n2022-08-07 18:44:59.811714: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-07 18:44:59.813228: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-07 18:44:59.813276: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n2022-08-07 18:44:59.814714: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-07 18:44:59.814829: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\nRelying on driver to perform ptx compilation. \nModify $PATH to customize ptxas location.\nThis message will be only logged once.\n\n\n40/40 [==============================] - ETA: 0s - loss: 9.4632 - instrument_1_loss: 0.9001 - instrument_7_loss: 0.8716 - instrument_41_loss: 0.8497 - instrument_42_loss: 0.8685 - instrument_43_loss: 0.8520 - instrument_44_loss: 0.8687 - instrument_61_loss: 0.8508 - instrument_69_loss: 0.8303 - instrument_71_loss: 0.8733 - instrument_72_loss: 0.8566 - instrument_74_loss: 0.8417 - instrument_1_accuracy: 0.0041 - instrument_1_auc_1: 0.4734 - instrument_7_accuracy: 0.0077 - instrument_7_auc_1: 0.3513 - instrument_41_accuracy: 0.0077 - instrument_41_auc_1: 0.5359 - instrument_42_accuracy: 0.0058 - instrument_42_auc_1: 0.5681 - instrument_43_accuracy: 0.0010 - instrument_43_auc_1: 0.5255 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.5647 - instrument_61_accuracy: 0.0016 - instrument_61_auc_1: 0.4531 - instrument_69_accuracy: 0.0015 - instrument_69_auc_1: 0.6652 - instrument_71_accuracy: 0.0340 - instrument_71_auc_1: 0.6068 - instrument_72_accuracy: 6.7188e-04 - instrument_72_auc_1: 0.5844 - instrument_74_accuracy: 0.0421 - instrument_74_auc_1: 0.5877 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n2022-08-07 19:00:44.902483: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-08-07 19:00:44.963806: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-08-07 19:00:44.996746: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_01_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_01_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 977s 24s/step - loss: 9.4632 - instrument_1_loss: 0.9001 - instrument_7_loss: 0.8716 - instrument_41_loss: 0.8497 - instrument_42_loss: 0.8685 - instrument_43_loss: 0.8520 - instrument_44_loss: 0.8687 - instrument_61_loss: 0.8508 - instrument_69_loss: 0.8303 - instrument_71_loss: 0.8733 - instrument_72_loss: 0.8566 - instrument_74_loss: 0.8417 - instrument_1_accuracy: 0.0041 - instrument_1_auc_1: 0.4734 - instrument_7_accuracy: 0.0077 - instrument_7_auc_1: 0.3513 - instrument_41_accuracy: 0.0077 - instrument_41_auc_1: 0.5359 - instrument_42_accuracy: 0.0058 - instrument_42_auc_1: 0.5681 - instrument_43_accuracy: 0.0010 - instrument_43_auc_1: 0.5255 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.5647 - instrument_61_accuracy: 0.0016 - instrument_61_auc_1: 0.4531 - instrument_69_accuracy: 0.0015 - instrument_69_auc_1: 0.6652 - instrument_71_accuracy: 0.0340 - instrument_71_auc_1: 0.6068 - instrument_72_accuracy: 6.7188e-04 - instrument_72_auc_1: 0.5844 - instrument_74_accuracy: 0.0421 - instrument_74_auc_1: 0.5877 - val_loss: 8.4378 - val_instrument_1_loss: 0.8021 - val_instrument_7_loss: 0.7764 - val_instrument_41_loss: 0.7603 - val_instrument_42_loss: 0.7667 - val_instrument_43_loss: 0.7576 - val_instrument_44_loss: 0.7714 - val_instrument_61_loss: 0.7615 - val_instrument_69_loss: 0.7426 - val_instrument_71_loss: 0.7806 - val_instrument_72_loss: 0.7687 - val_instrument_74_loss: 0.7498 - val_instrument_1_accuracy: 0.0000e+00 - val_instrument_1_auc_1: 0.4493 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0069 - val_instrument_41_auc_1: 0.5197 - val_instrument_42_accuracy: 0.0031 - val_instrument_42_auc_1: 0.5051 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5542 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4583 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5306 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5087 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 2/10\n40/40 [==============================] - ETA: 0s - loss: 8.1047 - instrument_1_loss: 0.7867 - instrument_7_loss: 0.7376 - instrument_41_loss: 0.7359 - instrument_42_loss: 0.7357 - instrument_43_loss: 0.7339 - instrument_44_loss: 0.7341 - instrument_61_loss: 0.7287 - instrument_69_loss: 0.7197 - instrument_71_loss: 0.7377 - instrument_72_loss: 0.7307 - instrument_74_loss: 0.7239 - instrument_1_accuracy: 0.0014 - instrument_1_auc_1: 0.4879 - instrument_7_accuracy: 0.0012 - instrument_7_auc_1: 0.4978 - instrument_41_accuracy: 0.0033 - instrument_41_auc_1: 0.5315 - instrument_42_accuracy: 0.0027 - instrument_42_auc_1: 0.5510 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5280 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.4818 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3864 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6101 - instrument_71_accuracy: 0.0000e+00 - instrument_71_auc_1: 0.5646 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5410 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5612     \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n2022-08-07 19:17:20.517681: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-08-07 19:17:20.554600: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_02_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_02_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 995s 25s/step - loss: 8.1047 - instrument_1_loss: 0.7867 - instrument_7_loss: 0.7376 - instrument_41_loss: 0.7359 - instrument_42_loss: 0.7357 - instrument_43_loss: 0.7339 - instrument_44_loss: 0.7341 - instrument_61_loss: 0.7287 - instrument_69_loss: 0.7197 - instrument_71_loss: 0.7377 - instrument_72_loss: 0.7307 - instrument_74_loss: 0.7239 - instrument_1_accuracy: 0.0014 - instrument_1_auc_1: 0.4879 - instrument_7_accuracy: 0.0012 - instrument_7_auc_1: 0.4978 - instrument_41_accuracy: 0.0033 - instrument_41_auc_1: 0.5315 - instrument_42_accuracy: 0.0027 - instrument_42_auc_1: 0.5510 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5280 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.4818 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3864 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6101 - instrument_71_accuracy: 0.0000e+00 - instrument_71_auc_1: 0.5646 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5410 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5612 - val_loss: 7.8978 - val_instrument_1_loss: 0.7504 - val_instrument_7_loss: 0.7148 - val_instrument_41_loss: 0.7218 - val_instrument_42_loss: 0.7141 - val_instrument_43_loss: 0.7118 - val_instrument_44_loss: 0.7135 - val_instrument_61_loss: 0.7121 - val_instrument_69_loss: 0.7065 - val_instrument_71_loss: 0.7230 - val_instrument_72_loss: 0.7210 - val_instrument_74_loss: 0.7088 - val_instrument_1_accuracy: 0.0037 - val_instrument_1_auc_1: 0.4846 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0100 - val_instrument_41_auc_1: 0.5245 - val_instrument_42_accuracy: 0.0031 - val_instrument_42_auc_1: 0.5151 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5917 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.2997 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5111 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4152 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 3/10\n40/40 [==============================] - ETA: 0s - loss: 7.8624 - instrument_1_loss: 0.7675 - instrument_7_loss: 0.7092 - instrument_41_loss: 0.7183 - instrument_42_loss: 0.7135 - instrument_43_loss: 0.7129 - instrument_44_loss: 0.7078 - instrument_61_loss: 0.7065 - instrument_69_loss: 0.7039 - instrument_71_loss: 0.7093 - instrument_72_loss: 0.7083 - instrument_74_loss: 0.7054 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4918 - instrument_7_accuracy: 0.0010 - instrument_7_auc_1: 0.5659 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5233 - instrument_42_accuracy: 0.0012 - instrument_42_auc_1: 0.5637 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5550 - instrument_44_accuracy: 0.0020 - instrument_44_auc_1: 0.5860 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3889 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6318 - instrument_71_accuracy: 1.5625e-05 - instrument_71_auc_1: 0.5802 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4675 - instrument_74_accuracy: 1.5625e-05 - instrument_74_auc_1: 0.5780 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_03_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_03_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 956s 24s/step - loss: 7.8624 - instrument_1_loss: 0.7675 - instrument_7_loss: 0.7092 - instrument_41_loss: 0.7183 - instrument_42_loss: 0.7135 - instrument_43_loss: 0.7129 - instrument_44_loss: 0.7078 - instrument_61_loss: 0.7065 - instrument_69_loss: 0.7039 - instrument_71_loss: 0.7093 - instrument_72_loss: 0.7083 - instrument_74_loss: 0.7054 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4918 - instrument_7_accuracy: 0.0010 - instrument_7_auc_1: 0.5659 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5233 - instrument_42_accuracy: 0.0012 - instrument_42_auc_1: 0.5637 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5550 - instrument_44_accuracy: 0.0020 - instrument_44_auc_1: 0.5860 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3889 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6318 - instrument_71_accuracy: 1.5625e-05 - instrument_71_auc_1: 0.5802 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4675 - instrument_74_accuracy: 1.5625e-05 - instrument_74_auc_1: 0.5780 - val_loss: 7.7954 - val_instrument_1_loss: 0.7227 - val_instrument_7_loss: 0.7037 - val_instrument_41_loss: 0.7197 - val_instrument_42_loss: 0.7082 - val_instrument_43_loss: 0.7105 - val_instrument_44_loss: 0.7031 - val_instrument_61_loss: 0.7064 - val_instrument_69_loss: 0.7001 - val_instrument_71_loss: 0.7094 - val_instrument_72_loss: 0.7105 - val_instrument_74_loss: 0.7012 - val_instrument_1_accuracy: 0.0031 - val_instrument_1_auc_1: 0.4640 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0031 - val_instrument_41_auc_1: 0.5485 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.6343 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4931 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4119 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5317 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5208 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 4/10\n40/40 [==============================] - ETA: 0s - loss: 7.8032 - instrument_1_loss: 0.7629 - instrument_7_loss: 0.7022 - instrument_41_loss: 0.7141 - instrument_42_loss: 0.7073 - instrument_43_loss: 0.7084 - instrument_44_loss: 0.7014 - instrument_61_loss: 0.7015 - instrument_69_loss: 0.6996 - instrument_71_loss: 0.7028 - instrument_72_loss: 0.7026 - instrument_74_loss: 0.7005 - instrument_1_accuracy: 0.0195 - instrument_1_auc_1: 0.4969 - instrument_7_accuracy: 5.4688e-04 - instrument_7_auc_1: 0.5081 - instrument_41_accuracy: 0.0056 - instrument_41_auc_1: 0.5313 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5713 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5383 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.5499 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4050 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5505 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5238 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4662 - instrument_74_accuracy: 3.1250e-05 - instrument_74_auc_1: 0.5639 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_04_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_04_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 840s 21s/step - loss: 7.8032 - instrument_1_loss: 0.7629 - instrument_7_loss: 0.7022 - instrument_41_loss: 0.7141 - instrument_42_loss: 0.7073 - instrument_43_loss: 0.7084 - instrument_44_loss: 0.7014 - instrument_61_loss: 0.7015 - instrument_69_loss: 0.6996 - instrument_71_loss: 0.7028 - instrument_72_loss: 0.7026 - instrument_74_loss: 0.7005 - instrument_1_accuracy: 0.0195 - instrument_1_auc_1: 0.4969 - instrument_7_accuracy: 5.4688e-04 - instrument_7_auc_1: 0.5081 - instrument_41_accuracy: 0.0056 - instrument_41_auc_1: 0.5313 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5713 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5383 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.5499 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4050 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5505 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5238 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4662 - instrument_74_accuracy: 3.1250e-05 - instrument_74_auc_1: 0.5639 - val_loss: 7.7528 - val_instrument_1_loss: 0.7379 - val_instrument_7_loss: 0.6998 - val_instrument_41_loss: 0.7096 - val_instrument_42_loss: 0.7011 - val_instrument_43_loss: 0.7045 - val_instrument_44_loss: 0.6996 - val_instrument_61_loss: 0.6999 - val_instrument_69_loss: 0.6976 - val_instrument_71_loss: 0.7014 - val_instrument_72_loss: 0.7032 - val_instrument_74_loss: 0.6983 - val_instrument_1_accuracy: 0.0113 - val_instrument_1_auc_1: 0.4940 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 6.2500e-04 - val_instrument_41_auc_1: 0.5468 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5828 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5291 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.3873 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.6024 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4427 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 5/10\n40/40 [==============================] - ETA: 0s - loss: 7.7747 - instrument_1_loss: 0.7576 - instrument_7_loss: 0.6996 - instrument_41_loss: 0.7121 - instrument_42_loss: 0.7049 - instrument_43_loss: 0.7059 - instrument_44_loss: 0.6988 - instrument_61_loss: 0.6990 - instrument_69_loss: 0.6979 - instrument_71_loss: 0.7001 - instrument_72_loss: 0.7005 - instrument_74_loss: 0.6984 - instrument_1_accuracy: 0.0180 - instrument_1_auc_1: 0.4997 - instrument_7_accuracy: 9.6875e-04 - instrument_7_auc_1: 0.4890 - instrument_41_accuracy: 0.0024 - instrument_41_auc_1: 0.5324 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5521 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5446 - instrument_44_accuracy: 6.8750e-04 - instrument_44_auc_1: 0.4677 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3171 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6400 - instrument_71_accuracy: 0.0011 - instrument_71_auc_1: 0.5348 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4833 - instrument_74_accuracy: 9.3750e-05 - instrument_74_auc_1: 0.5742 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_05_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_05_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 809s 20s/step - loss: 7.7747 - instrument_1_loss: 0.7576 - instrument_7_loss: 0.6996 - instrument_41_loss: 0.7121 - instrument_42_loss: 0.7049 - instrument_43_loss: 0.7059 - instrument_44_loss: 0.6988 - instrument_61_loss: 0.6990 - instrument_69_loss: 0.6979 - instrument_71_loss: 0.7001 - instrument_72_loss: 0.7005 - instrument_74_loss: 0.6984 - instrument_1_accuracy: 0.0180 - instrument_1_auc_1: 0.4997 - instrument_7_accuracy: 9.6875e-04 - instrument_7_auc_1: 0.4890 - instrument_41_accuracy: 0.0024 - instrument_41_auc_1: 0.5324 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5521 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5446 - instrument_44_accuracy: 6.8750e-04 - instrument_44_auc_1: 0.4677 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3171 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6400 - instrument_71_accuracy: 0.0011 - instrument_71_auc_1: 0.5348 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4833 - instrument_74_accuracy: 9.3750e-05 - instrument_74_auc_1: 0.5742 - val_loss: 7.7501 - val_instrument_1_loss: 0.7305 - val_instrument_7_loss: 0.6980 - val_instrument_41_loss: 0.7178 - val_instrument_42_loss: 0.7033 - val_instrument_43_loss: 0.7052 - val_instrument_44_loss: 0.6978 - val_instrument_61_loss: 0.7014 - val_instrument_69_loss: 0.6965 - val_instrument_71_loss: 0.7022 - val_instrument_72_loss: 0.7005 - val_instrument_74_loss: 0.6969 - val_instrument_1_accuracy: 0.0094 - val_instrument_1_auc_1: 0.4634 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0000e+00 - val_instrument_41_auc_1: 0.5274 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5837 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4972 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.2460 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5497 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4621 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 6/10\n40/40 [==============================] - ETA: 0s - loss: 7.7649 - instrument_1_loss: 0.7601 - instrument_7_loss: 0.6981 - instrument_41_loss: 0.7116 - instrument_42_loss: 0.7037 - instrument_43_loss: 0.7046 - instrument_44_loss: 0.6974 - instrument_61_loss: 0.6981 - instrument_69_loss: 0.6968 - instrument_71_loss: 0.6986 - instrument_72_loss: 0.6990 - instrument_74_loss: 0.6970 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4878 - instrument_7_accuracy: 7.5000e-04 - instrument_7_auc_1: 0.4831 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5172 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5709 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5191 - instrument_44_accuracy: 2.3437e-04 - instrument_44_auc_1: 0.3375 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3742 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5869 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5242 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 5.1562e-04 - instrument_74_auc_1: 0.5644 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_06_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_06_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 823s 20s/step - loss: 7.7649 - instrument_1_loss: 0.7601 - instrument_7_loss: 0.6981 - instrument_41_loss: 0.7116 - instrument_42_loss: 0.7037 - instrument_43_loss: 0.7046 - instrument_44_loss: 0.6974 - instrument_61_loss: 0.6981 - instrument_69_loss: 0.6968 - instrument_71_loss: 0.6986 - instrument_72_loss: 0.6990 - instrument_74_loss: 0.6970 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4878 - instrument_7_accuracy: 7.5000e-04 - instrument_7_auc_1: 0.4831 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5172 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5709 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5191 - instrument_44_accuracy: 2.3437e-04 - instrument_44_auc_1: 0.3375 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3742 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5869 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5242 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 5.1562e-04 - instrument_74_auc_1: 0.5644 - val_loss: 7.7343 - val_instrument_1_loss: 0.7160 - val_instrument_7_loss: 0.6968 - val_instrument_41_loss: 0.7101 - val_instrument_42_loss: 0.7015 - val_instrument_43_loss: 0.7050 - val_instrument_44_loss: 0.6966 - val_instrument_61_loss: 0.7034 - val_instrument_69_loss: 0.6956 - val_instrument_71_loss: 0.7063 - val_instrument_72_loss: 0.7070 - val_instrument_74_loss: 0.6960 - val_instrument_1_accuracy: 0.0050 - val_instrument_1_auc_1: 0.5094 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0000e+00 - val_instrument_41_auc_1: 0.5444 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.6109 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4586 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0025 - val_instrument_61_auc_1: 0.4560 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0012 - val_instrument_71_auc_1: 0.5428 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4793 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 7/10\n40/40 [==============================] - ETA: 0s - loss: 7.7545 - instrument_1_loss: 0.7566 - instrument_7_loss: 0.6973 - instrument_41_loss: 0.7104 - instrument_42_loss: 0.7032 - instrument_43_loss: 0.7040 - instrument_44_loss: 0.6965 - instrument_61_loss: 0.6975 - instrument_69_loss: 0.6961 - instrument_71_loss: 0.6978 - instrument_72_loss: 0.6986 - instrument_74_loss: 0.6965 - instrument_1_accuracy: 0.0227 - instrument_1_auc_1: 0.5040 - instrument_7_accuracy: 9.5313e-04 - instrument_7_auc_1: 0.5582 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5480 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5572 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5104 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.4126 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4421 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5621 - instrument_71_accuracy: 1.4062e-04 - instrument_71_auc_1: 0.5336 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5092 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5229 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_07_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_07_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1015s 26s/step - loss: 7.7545 - instrument_1_loss: 0.7566 - instrument_7_loss: 0.6973 - instrument_41_loss: 0.7104 - instrument_42_loss: 0.7032 - instrument_43_loss: 0.7040 - instrument_44_loss: 0.6965 - instrument_61_loss: 0.6975 - instrument_69_loss: 0.6961 - instrument_71_loss: 0.6978 - instrument_72_loss: 0.6986 - instrument_74_loss: 0.6965 - instrument_1_accuracy: 0.0227 - instrument_1_auc_1: 0.5040 - instrument_7_accuracy: 9.5313e-04 - instrument_7_auc_1: 0.5582 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5480 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5572 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5104 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.4126 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4421 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5621 - instrument_71_accuracy: 1.4062e-04 - instrument_71_auc_1: 0.5336 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5092 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5229 - val_loss: 7.7320 - val_instrument_1_loss: 0.7148 - val_instrument_7_loss: 0.6960 - val_instrument_41_loss: 0.7158 - val_instrument_42_loss: 0.7021 - val_instrument_43_loss: 0.7050 - val_instrument_44_loss: 0.6959 - val_instrument_61_loss: 0.6996 - val_instrument_69_loss: 0.6951 - val_instrument_71_loss: 0.7057 - val_instrument_72_loss: 0.7067 - val_instrument_74_loss: 0.6953 - val_instrument_1_accuracy: 0.0056 - val_instrument_1_auc_1: 0.4874 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0081 - val_instrument_41_auc_1: 0.5351 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5699 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4698 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.5076 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5836 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5175 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 8/10\n40/40 [==============================] - ETA: 0s - loss: 7.7481 - instrument_1_loss: 0.7563 - instrument_7_loss: 0.6965 - instrument_41_loss: 0.7097 - instrument_42_loss: 0.7023 - instrument_43_loss: 0.7032 - instrument_44_loss: 0.6959 - instrument_61_loss: 0.6970 - instrument_69_loss: 0.6959 - instrument_71_loss: 0.6972 - instrument_72_loss: 0.6981 - instrument_74_loss: 0.6960 - instrument_1_accuracy: 0.0177 - instrument_1_auc_1: 0.5015 - instrument_7_accuracy: 6.2500e-04 - instrument_7_auc_1: 0.5371 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5368 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5660 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5082 - instrument_44_accuracy: 8.2812e-04 - instrument_44_auc_1: 0.4698 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4768 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5771 - instrument_71_accuracy: 1.8750e-04 - instrument_71_auc_1: 0.5660 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5181 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5266 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_08_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_08_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1063s 27s/step - loss: 7.7481 - instrument_1_loss: 0.7563 - instrument_7_loss: 0.6965 - instrument_41_loss: 0.7097 - instrument_42_loss: 0.7023 - instrument_43_loss: 0.7032 - instrument_44_loss: 0.6959 - instrument_61_loss: 0.6970 - instrument_69_loss: 0.6959 - instrument_71_loss: 0.6972 - instrument_72_loss: 0.6981 - instrument_74_loss: 0.6960 - instrument_1_accuracy: 0.0177 - instrument_1_auc_1: 0.5015 - instrument_7_accuracy: 6.2500e-04 - instrument_7_auc_1: 0.5371 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5368 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5660 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5082 - instrument_44_accuracy: 8.2812e-04 - instrument_44_auc_1: 0.4698 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4768 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5771 - instrument_71_accuracy: 1.8750e-04 - instrument_71_auc_1: 0.5660 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5181 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5266 - val_loss: 7.7214 - val_instrument_1_loss: 0.7360 - val_instrument_7_loss: 0.6954 - val_instrument_41_loss: 0.7096 - val_instrument_42_loss: 0.6991 - val_instrument_43_loss: 0.7022 - val_instrument_44_loss: 0.6953 - val_instrument_61_loss: 0.6950 - val_instrument_69_loss: 0.6947 - val_instrument_71_loss: 0.6990 - val_instrument_72_loss: 0.7001 - val_instrument_74_loss: 0.6949 - val_instrument_1_accuracy: 0.0050 - val_instrument_1_auc_1: 0.4999 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0025 - val_instrument_41_auc_1: 0.5021 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5982 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5448 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.0000e+00 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5107 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4850 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 9/10\n40/40 [==============================] - ETA: 0s - loss: 7.7462 - instrument_1_loss: 0.7569 - instrument_7_loss: 0.6962 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7022 - instrument_43_loss: 0.7034 - instrument_44_loss: 0.6954 - instrument_61_loss: 0.6962 - instrument_69_loss: 0.6956 - instrument_71_loss: 0.6970 - instrument_72_loss: 0.6976 - instrument_74_loss: 0.6956 - instrument_1_accuracy: 0.0090 - instrument_1_auc_1: 0.4916 - instrument_7_accuracy: 8.7500e-04 - instrument_7_auc_1: 0.4746 - instrument_41_accuracy: 0.0042 - instrument_41_auc_1: 0.5434 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5592 - instrument_43_accuracy: 1.5625e-05 - instrument_43_auc_1: 0.5093 - instrument_44_accuracy: 0.0019 - instrument_44_auc_1: 0.5424 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4239 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5228 - instrument_71_accuracy: 6.8750e-04 - instrument_71_auc_1: 0.5023 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5054 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.4956 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_09_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_09_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1070s 27s/step - loss: 7.7462 - instrument_1_loss: 0.7569 - instrument_7_loss: 0.6962 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7022 - instrument_43_loss: 0.7034 - instrument_44_loss: 0.6954 - instrument_61_loss: 0.6962 - instrument_69_loss: 0.6956 - instrument_71_loss: 0.6970 - instrument_72_loss: 0.6976 - instrument_74_loss: 0.6956 - instrument_1_accuracy: 0.0090 - instrument_1_auc_1: 0.4916 - instrument_7_accuracy: 8.7500e-04 - instrument_7_auc_1: 0.4746 - instrument_41_accuracy: 0.0042 - instrument_41_auc_1: 0.5434 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5592 - instrument_43_accuracy: 1.5625e-05 - instrument_43_auc_1: 0.5093 - instrument_44_accuracy: 0.0019 - instrument_44_auc_1: 0.5424 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4239 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5228 - instrument_71_accuracy: 6.8750e-04 - instrument_71_auc_1: 0.5023 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5054 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.4956 - val_loss: 7.7105 - val_instrument_1_loss: 0.7222 - val_instrument_7_loss: 0.6950 - val_instrument_41_loss: 0.7136 - val_instrument_42_loss: 0.6994 - val_instrument_43_loss: 0.7035 - val_instrument_44_loss: 0.6950 - val_instrument_61_loss: 0.6960 - val_instrument_69_loss: 0.6945 - val_instrument_71_loss: 0.6972 - val_instrument_72_loss: 0.6994 - val_instrument_74_loss: 0.6946 - val_instrument_1_accuracy: 0.0031 - val_instrument_1_auc_1: 0.4976 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0106 - val_instrument_41_auc_1: 0.5649 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5388 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5100 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4717 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.4343 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4660 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 10/10\n40/40 [==============================] - ETA: 0s - loss: 7.7444 - instrument_1_loss: 0.7565 - instrument_7_loss: 0.6958 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7021 - instrument_43_loss: 0.7035 - instrument_44_loss: 0.6952 - instrument_61_loss: 0.6963 - instrument_69_loss: 0.6953 - instrument_71_loss: 0.6967 - instrument_72_loss: 0.6973 - instrument_74_loss: 0.6954 - instrument_1_accuracy: 0.0147 - instrument_1_auc_1: 0.4936 - instrument_7_accuracy: 8.5937e-04 - instrument_7_auc_1: 0.5001 - instrument_41_accuracy: 0.0035 - instrument_41_auc_1: 0.5259 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5330 - instrument_43_accuracy: 3.1250e-05 - instrument_43_auc_1: 0.4885 - instrument_44_accuracy: 0.0027 - instrument_44_auc_1: 0.5748 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4723 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5207 - instrument_71_accuracy: 0.0010 - instrument_71_auc_1: 0.4945 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4725 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5151 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_10_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_10_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1063s 27s/step - loss: 7.7444 - instrument_1_loss: 0.7565 - instrument_7_loss: 0.6958 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7021 - instrument_43_loss: 0.7035 - instrument_44_loss: 0.6952 - instrument_61_loss: 0.6963 - instrument_69_loss: 0.6953 - instrument_71_loss: 0.6967 - instrument_72_loss: 0.6973 - instrument_74_loss: 0.6954 - instrument_1_accuracy: 0.0147 - instrument_1_auc_1: 0.4936 - instrument_7_accuracy: 8.5937e-04 - instrument_7_auc_1: 0.5001 - instrument_41_accuracy: 0.0035 - instrument_41_auc_1: 0.5259 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5330 - instrument_43_accuracy: 3.1250e-05 - instrument_43_auc_1: 0.4885 - instrument_44_accuracy: 0.0027 - instrument_44_auc_1: 0.5748 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4723 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5207 - instrument_71_accuracy: 0.0010 - instrument_71_auc_1: 0.4945 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4725 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5151 - val_loss: 7.7124 - val_instrument_1_loss: 0.7159 - val_instrument_7_loss: 0.6948 - val_instrument_41_loss: 0.7116 - val_instrument_42_loss: 0.6982 - val_instrument_43_loss: 0.7023 - val_instrument_44_loss: 0.6947 - val_instrument_61_loss: 0.6964 - val_instrument_69_loss: 0.6943 - val_instrument_71_loss: 0.7038 - val_instrument_72_loss: 0.7060 - val_instrument_74_loss: 0.6944 - val_instrument_1_accuracy: 0.0044 - val_instrument_1_auc_1: 0.5049 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0131 - val_instrument_41_auc_1: 0.5142 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5568 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4590 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.4428 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.4287 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4621 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\n\n\n\nconv_to_lstm_model.save('../models/new_classic_conv_to_lstm/')\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/new_classic_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/new_classic_conv_to_lstm/assets\n\n\n\nsave_history(conv_to_lstm_history.history, '../models/new_classic_conv_to_lstm.pkl')\n\n\nmusicnet_eval(conv_to_lstm_model, classic_eval_generator, 2)\n\n1/1 [==============================] - 0s 25ms/step\n\n\n\n\n\nAgain, same behavior is observed, we have concluded that the input data might be incompatible for our model, further investigation on the input pipeline should be taken."
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/music_transcription_2conv.html",
    "href": "posts/music/Music_transcription_fastai/notebooks/music_transcription_2conv.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import glob\nimport os\nimport random\nfrom datetime import datetime\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.stats\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow.python.platform.build_info as build\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.python.client import device_lib\n\nfrom spec_generator_sequence import spec_generator\nfrom spec_input_generator import gen, gen_eval\n# import spectrogram\n# from spectrogram import generate_spec\n# from spectrogram import truncate_spec\n# from spectrogram import mask_spec\n# from spectrogram import add_noise\n# from spectrogram import path_to_preprocessing\nfrom spectrogram_class import spectrogram\n\n\nOrchideaSOL CNN\nThis notebooks is succesive from the baseline model notebook from music_transcription_class.ipynb.\nIn the notebook, we had demonstrate the limitation of simple CNN model on classifying instruments from raw audio files.\nIn this notebook, we will be running a similar CNN architecture, but with deeper layers, and applying regularization, batch normalization and dropout techniques.\n\nprint(build.build_info['cuda_version'])\n\n11.2\n\n\n\ndevice_lib.list_local_devices()\n\n2022-08-08 07:35:52.733084: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-08 07:35:52.777510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:52.813076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:52.813878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.814786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.815302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.815685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.816454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3085 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 13176899990423004209\n xla_global_id: -1,\n name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 3235774464\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 4598158092110336817\n physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n xla_global_id: 416903419]\n\n\n\ntf.test.is_gpu_available()\n\nWARNING:tensorflow:From /tmp/ipykernel_49617/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\n\n\n2022-08-08 07:35:53.919564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\nTrue\n\n\n2022-08-08 07:35:53.920078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.920462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.920851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.921170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.921402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3085 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n\ntf.config.list_physical_devices('GPU')\n\n2022-08-08 07:35:54.029967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:54.030428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:54.030817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\ngpu_devices\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\nWe first start by generating training and testing dataset as usual.\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\n\n\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\n\n# Splitting data for training and testing\ntrain_df, test_df = train_test_split(meta_df, stratify=meta_df['Instrument (in full)'], \n                                        train_size=0.8)\n\n\ntrain_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      4961\n      Strings/Contrabass/pizzicato_l_vib/Cb-pizz_lv-...\n      Strings\n      Violin Family\n      Cb\n      Contrabass\n      pizz_lv\n      pizzicato_l_vib\n      E1\n      28.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      2\n    \n    \n      3401\n      PluckedStrings/Guitar/ordinario/Gtr-ord-F#5-ff...\n      PluckedStrings\n      Plucked Strings\n      Gtr\n      Guitar\n      ord\n      ordinario\n      F#5\n      78.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      1.0\n      True\n      1\n    \n  \n\n\n\n\n\nsample = next(gen(train_df, return_class = True))\n\n\n# Getting the shape of input from generator\nspec_shape = sample[0].spec.shape\nspec_shape\n\n(256, 500, 1)\n\n\n\nnext(gen_eval(test_df))[0].shape\n\n(256, 500, 1)\n\n\nWe will be using the same Sequence data generator, as in previous notebook. However, to optimize our performance, we will be converting the generator in to a tf Dataset object, and optimized the performance by prefetching the dataset while fitting is in progress.\nThe process of prefecthing and training can be visualized using tf.data API. Which is in the todo list of this project.\n\nBATCH_SIZE = 32\n\ntrain_generator = (tf.data.Dataset.from_generator(lambda: spec_generator(train_df, BATCH_SIZE, \n                  add_channel = True), output_types=(tf.float32, tf.int32), \n                 output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                 (BATCH_SIZE, 16)))).prefetch((tf.data.experimental.AUTOTUNE))\n\neval_generator = (tf.data.Dataset.from_generator(lambda: spec_generator(test_df, BATCH_SIZE, \n                  add_channel = True), \n                    output_types=(tf.float32, tf.int32), \n                 output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                 (BATCH_SIZE, 16)))).prefetch((tf.data.experimental.AUTOTUNE))\n\n2022-08-08 07:35:55.370698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.371338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.371817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.372308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.372680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.372983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3085 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n\n# Sanity check for shape\ntrain_generator\n\n<PrefetchDataset element_spec=(TensorSpec(shape=(32, 256, 500, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 16), dtype=tf.int32, name=None))>\n\n\nNow we can finally start to build our model, the idea is same as before, a deeper convolutional model, including dropouts and regularization.\n\n# Starting the model\nmodel_2conv = models.Sequential()\n\n# Adding the first convoluton-pooling layer\nmodel_2conv.add(layers.InputLayer((spec_shape[0], spec_shape[1], 1),\n                     batch_size = BATCH_SIZE, dtype = tf.float32))\nmodel_2conv.add(layers.Conv2D(30, (150, 300), activation='relu', \n                    kernel_regularizer = tf.keras.regularizers.L2(l2=0.01)))\nmodel_2conv.add(layers.MaxPool2D((2, 3)))\nmodel_2conv.add(layers.BatchNormalization())\n\n# Addig the second convolutional-pooling layer\nmodel_2conv.add(layers.Conv2D(15, (15, 30), activation = 'relu',\n                                kernel_regularizer = tf.keras.regularizers.L2(l2=0.01)))\nmodel_2conv.add(layers.MaxPool2D(2, 3))\nmodel_2conv.add(layers.BatchNormalization())\n\n# Combined the filter layers into 1\nmodel_2conv.add(layers.Flatten())\nmodel_2conv.add(layers.Dropout(0.2))\nmodel_2conv.add(layers.Dense(200, activation = 'relu'))\nmodel_2conv.add(layers.Dropout(0.2))\nmodel_2conv.add(layers.Dense(50, activation = 'relu'))\nmodel_2conv.add(layers.Dropout(0.2))\n\n# Final layer to classify the 16 instruments\n# We are using softmax activation since this is a multiclass classification problem\nmodel_2conv.add(layers.Dense(16, activation = 'softmax'))\n\nmodel_2conv.build()\n\n\n# Visualizing the CNN architecture\ntf.keras.utils.plot_model(model_2conv, show_shapes = True, show_dtype= True)\n\n\n\n\nOur input shape will be the same as in the previous notebook, with 256 frequency bins and 500 timesteps\nOur output layer consists of 16 neurons, each representing one possible instrument class.\nSince we will be predicting a multiclass label (Only one true label over multiple options), we will be using categorical cross entropy as our loss function, note the this is the reason why we have chose to use softmax as the activation function of our output layer.\n\n\nmodel_2conv.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.03),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy'])\n\n\nmodel_2conv.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (32, 107, 201, 30)        1350030   \n                                                                 \n max_pooling2d (MaxPooling2D  (32, 53, 67, 30)         0         \n )                                                               \n                                                                 \n batch_normalization (BatchN  (32, 53, 67, 30)         120       \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (32, 39, 38, 15)          202515    \n                                                                 \n max_pooling2d_1 (MaxPooling  (32, 13, 13, 15)         0         \n 2D)                                                             \n                                                                 \n batch_normalization_1 (Batc  (32, 13, 13, 15)         60        \n hNormalization)                                                 \n                                                                 \n flatten (Flatten)           (32, 2535)                0         \n                                                                 \n dropout (Dropout)           (32, 2535)                0         \n                                                                 \n dense (Dense)               (32, 200)                 507200    \n                                                                 \n dropout_1 (Dropout)         (32, 200)                 0         \n                                                                 \n dense_1 (Dense)             (32, 50)                  10050     \n                                                                 \n dropout_2 (Dropout)         (32, 50)                  0         \n                                                                 \n dense_2 (Dense)             (32, 16)                  816       \n                                                                 \n=================================================================\nTotal params: 2,070,791\nTrainable params: 2,070,701\nNon-trainable params: 90\n_________________________________________________________________\n\n\n\n#  model_2conv = tf.keras.models.load_model('../models/2conv/six/')\n\n\n\nModel training\nNow that we have done the setup we can finally train our model\n\n# ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n#     f\"../models/baseline_checkpoint/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_model_2conv\", \n#                                                     monitor='val_accuracy')\n# early_callback = tf.keras.callbacks.EarlyStopping(monitor = 'accuracy', patience = 2)\n\n\n# history = model_2conv.fit(train_generator, epochs = 4, verbose=1, \n#                     validation_data = eval_generator,\n#                     validation_steps = 10, validation_freq= 2, \n#                     use_multiprocessing=True, workers = 2, callbacks=[ckpt_callback, early_callback])\n\n\n# model_2conv.save('../models/new_OrchideaSOL_2conv/')\n\n\n\nModel Evaluating\n\n# plt.plot([2, 4], history.history['val_accuracy'], label = 'Validation accuracy')\n# plt.plot([1, 2, 3, 4], history.history['accuracy'], label = 'Training accuracy')\n# plt.title('Accuracy of baseline model after early callback')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy')\n# plt.legend()\n# plt.show()\n\nThe figure above shows the accuracy of 4 epochs we have ran, combining with the our baseline model of one convolutional layers, we can conclude that basic CNN since to be performing poorly on this problem. Since\n\nBoth the validation and training accuracy starts to drop after 3 epochs\nTraining time is around 30 minutes per epoch\n\nWe can still look at the loos function\n\n# plt.plot([2, 4], history.history['val_loss'], label = 'Validation loss')\n# plt.plot([1, 2, 3, 4], history.history['loss'], label = 'Training loss')\n# plt.title('Loss of baseline model after early callback')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.show()\n\nI have mistakenly run the code without saving the history, and lost the plot\nThe general shape of the figure is a sharp drop at first spoch, and remained fairly strat with minimal decrease after that\nThe loss function hardly decrease after the first epoch, however, both the decerase of loss function and accuracy represents that the categorical cross entropy might not be the best choice for such classification problem.\nBut the model might inprove after several epoch, this notebook is yet to be run again with longer time after the more important preceeding notebooks (Sequential music transcription with LSTM) had been done.\nAlso the hyperparameter such as the number of frequency bins, optimizer and regularization coefficient still can be optimized. Due to the limited time and high training time, this notebook had been added to the to do list, and decreased in priority.\nNow lets look at the confusion matrix\n\ndef orchidea_confusion_matrix(model, generator, instrument_list):\n    '''\n    Plot confusion matrix for OrchideaSOL dataset\n    \n    Input:\n    model: Model to be used\n    generator: Sequence class, generator to generate feature and labels for \n                OrchideaSOL dataset\n    instrument_list: list of instrument in alphabetical order, used to label the plot\n    \n    Output:\n    predict: np.array, Predicted output\n    prediction_label: np.array, True label\n    '''\n    prediction_feature, prediction_label = generator.__getitem__(0)\n    predict = predict = model.predict(prediction_feature)\n    assert prediction_label.shape == predict.shape\n    from sklearn.metrics import confusion_matrix\n\n    plt.figure(figsize = (12, 10))\n    sns.heatmap(confusion_matrix(np.argmax(prediction_label, axis=1), \n                        np.argmax(predict, axis = 1)), annot = True, \n                        xticklabels=instrument_list, \n                        yticklabels=instrument_list)\n    plt.title('Confusion matrix for baseline model')\n    plt.show()\n\n    return predict, prediction_label\n\n\nmodel = tf.keras.models.load_model('../models/baseline_checkpoint/20220807_052111_02_model_2conv')\n\n\n# Defining generator to be used in our evaluation\n# Since our gpu memory space is very limited, we wil be using 1/10 of \n# testing dataset, and omitting any data augmentation on the spectrogram\nprediction_generator = spec_generator(test_df, test_df.shape[0] // 5, add_channel=True,\n                                        live_generation = True, preprocess = False, \n                                        n_mels = 256)\n\n\n\ninstrument_list = sorted(meta_df['Instrument (in full)'].unique())\ninstrument_list\n\n['Accordion',\n 'Alto Saxophone',\n 'Bass Tuba',\n 'Bassoon',\n 'Cello',\n 'Clarinet in Bb',\n 'Contrabass',\n 'Flute',\n 'French Horn',\n 'Guitar',\n 'Harp',\n 'Oboe',\n 'Trombone',\n 'Trumpet in C',\n 'Viola',\n 'Violin']\n\n\n\npredict, predict_label = orchidea_confusion_matrix(model, \n                                prediction_generator, instrument_list)\n\n2022-08-08 07:37:20.613548: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 271360000 exceeds 10% of free system memory.\n2022-08-08 07:37:20.955501: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 271360000 exceeds 10% of free system memory.\n2022-08-08 07:37:21.382134: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory.\n2022-08-08 07:37:21.382196: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory.\n2022-08-08 07:37:21.382234: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory.\n2022-08-08 07:37:22.156406: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n2022-08-08 07:37:25.464340: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-08 07:37:25.467227: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-08 07:37:25.467562: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n2022-08-08 07:37:25.469313: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-08 07:37:25.469628: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\nRelying on driver to perform ptx compilation. \nModify $PATH to customize ptxas location.\nThis message will be only logged once.\n\n\n17/17 [==============================] - 87s 3s/step\n\n\n\n\n\n\n\nERROR\nFurther inspection is needed, as the model converged to predicting the same class at the end od epoch."
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/music_transcription_RNN.html",
    "href": "posts/music/Music_transcription_fastai/notebooks/music_transcription_RNN.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import Input\nfrom keras import backend as K\nfrom keras import layers, models\nfrom keras.layers import LSTM, Dense\nfrom keras.models import Model, Sequential\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n\nfrom spec_generator_sequence import spec_generator\nfrom spec_generator_sequence_multilabel import (spec_generator_multi,\n                                                spec_generator_multioutput)\nfrom spec_input_generator import gen, gen_eval\nfrom spectrogram_class import spectrogram"
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/music_transcription_RNN.html#understanding-unrolled-lstm-under-10-seconds",
    "href": "posts/music/Music_transcription_fastai/notebooks/music_transcription_RNN.html#understanding-unrolled-lstm-under-10-seconds",
    "title": "Most interesting blog on Earth!",
    "section": "Understanding unrolled LSTM, under 10 seconds!",
    "text": "Understanding unrolled LSTM, under 10 seconds!\n\n\n\nImage from this fantastic blog.\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nIf you are still here"
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/music_transcription_RNN.html#understanding-lstm-under-1-minute",
    "href": "posts/music/Music_transcription_fastai/notebooks/music_transcription_RNN.html#understanding-lstm-under-1-minute",
    "title": "Most interesting blog on Earth!",
    "section": "Understanding LSTM, under 1 minute!",
    "text": "Understanding LSTM, under 1 minute!\nLSTM, or more generally RNN, is useful to find the relation between sequential data. Instead of having a fixed length of inputs, and trying to learn the weights for each input in a model, RNN uses previous inputs as a predictor for current prediction!\nEach RNN cells can have 2 outputs (or 3 for LSMT, more on that later), and the first output will be feed in to the next LSTM cell, undergoes matrix multiplication, and combined with the input of the next sequential timestep. The cycle goes on. The final output will then be fed into the next layer\nIf the parameter return_sequence of LSTM is set to true, then each LSTM cell will have a second output, to be fed to the next layer, hence we will have a output with the same length along the sequence. This architecture is called an unrolled LSTM.\nIn this notebook, we will be using the single output version of LSTM, then feeding the output to layers of Dense layers, to classify the instrument and notes of the audio files.\nSummary: * LSTM works will flexible length of input data * LSTM uses the previous data as input, transformed using kernel to be learned * LSTM can produce output of same sequence length.\n\n\ntf.test.is_gpu_available()\n\nWARNING:tensorflow:From /tmp/ipykernel_4020/4084812110.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\n\n\n2022-08-07 18:41:28.718001: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-07 18:41:28.791105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:28.828697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:28.829898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.878813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.879839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.880616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.881097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3370 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\nTrue\n\n\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\n\n\nmeta_df.shape\n\n(13162, 17)\n\n\n\nmeta_df = meta_df[~meta_df['Pitch ID (if applicable)'].isna()]\n\n\nmeta_df.isna().sum()\n\nPath                              0\nFamily (abbr.)                    0\nFamily (in full)                  0\nInstrument (abbr.)                0\nInstrument (in full)              0\nTechnique (abbr.)                 0\nTechnique (in full)               0\nPitch                             0\nPitch ID (if applicable)          0\nDynamics                          0\nDynamics ID (if applicable)     568\nInstance ID                       0\nMute (abbr.)                      0\nMute (in full)                    0\nString ID (if applicable)      5666\nNeeded digital retuning           0\nFold                              0\ndtype: int64\n\n\n\nmeta_df.shape\n\n(13162, 17)\n\n\n\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\nAs of before, the columns we are interested in (instruments and pitch id) doesnt have any null vales, we can just perform the train test split and the rest of the preprocessing is taken by our spectrogram class module.\n\nfrom random import random\n\n\ntrain_df, test_df = train_test_split(meta_df, stratify=meta_df['Instrument (in full)'], \n                                            train_size=0.7, random_state= 42)\n\n\nmulti_generator = spec_generator_multi(train_df, 32)\n\n\n_, num_target = multi_generator.__getitem__(2)[1].shape\nprint(num_target)\n\n107\n\n\n\n_, num_row, num_col= multi_generator.__getitem__(2)[0].shape\nprint(num_row)\nprint(num_col)\n\n500\n256"
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/music_transcription_class.html",
    "href": "posts/music/Music_transcription_fastai/notebooks/music_transcription_class.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "!which python\n\n/home/shiya/anaconda3/envs/music/bin/python\n\n\n\nimport glob\nimport os\nimport random\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.stats\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import datasets, layers, models\n\nfrom spec_generator_sequence import spec_generator\nfrom spectrogram_class import spectrogram\n\n\nPreprocessing\nThis is a successive notebook after the EDA notebooks.\nAfter the EDA analysis, we can proceed to perform the preprocessing for audio. We will be working will spectrogram, as a spectrogram contains the spatial and temporal information of a audio files.\nWe will be utilizing Librosa package for spectrogram generation and augmentation. For easier implementation, we have defined a spectrogram class to perform all the procedures. Please refer to spectrogram_class.py in the same folder for more information.\n\nspectrogram\n\nspectrogram_class.spectrogram\n\n\nLets have a look at what a spectrogram looks like\n\nprint(tf.config.list_physical_devices('GPU'))\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n2022-08-08 01:38:04.723562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:04.752704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:04.753369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\nhop_length = 4096\nwin_length = 1024\nn_fft = 1024\n\nWe have randomly picked a audio file in OrchideaSOL to showcase the spectrogram.\n\n%%time\ntest_spec = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\n\n# To demonstrate that the test_spec is an instance on our custom defined class\n# let's check its type!\ntype(test_spec)\n\nCPU times: user 47.1 ms, sys: 76.8 ms, total: 124 ms\nWall time: 302 ms\n\n\nspectrogram_class.spectrogram\n\n\nWe have a support function in the class to help us visualized the spectrogram, now lets have a look at what a harps looks like in spectrogram\n\ntest_spec.plot_spec() \n\n\n\n\nNow I owe you some explanation, why am I showing you this? There are some important points to observe:\n\nYou probably heard about the relation between a note and its soundwave frequency, for example, in piano the middle G3 note corresponds to 196 HZ. But we are not exactly seeing one horizontal line on 196 HZ. Why is that? The main reason is that the magnitude of sound wave/magnitude is additive! Lets have a closer look:\n\n\n\n\n(screenshot captured from our lord and savior, 3Blue1Brown.\n\nAlthought the resulting soundwave is of frequency of 196 HZ, it is actually a combination of soundwave of different frequencies. This is exactly the same note for piano and violin sounds different! Imagine a world where a tuba, harp, tuna and your cat sound the same, what a horrible world\n\n\nThis is actually why throughout the project, we have decided to use neural network to perform the music transcription, a convolutional network or RNN are able to capture the feature between spatial and temporal information across the spectrogram.\n\n\n\n(A demonstration of how a CNN is able to capture different features on a face in different layers.)\nBut before we can use the dataset into directly, we have to consider about the data augmentation. Our model should be able to perform tasks include:\n\nTranscribe music in a noisy environment\nTranscribe music is moderate data loss\nStill be able to tell both audio files contain violins, although the violins have different sound signature\n\nBut how can our model learn these difference if our data is perfectly clean, and only contain audio files from the same instruments. That is where data augmentation comes in.\n\n\nData augmentation\nWe have to augmentate our data so that our training dataset match the scenario above. To achieve this, we have decided to add noise, masking and shifting to the spectrogram.\nAll of these function had been defined under the spectrogram class. Now lets try to visualize this.\n\nAdding noise\nRemeber this is how our signal and spectrogram for harps looks like:\n\ndef plt_signal_spec(spec, xlim = None):\n    fig = plt.figure(figsize = (14, 6))\n    ax_1 = fig.add_subplot(121)\n    ax_1.plot(spec.signal)\n    if xlim:\n        ax_1.set_xlim((0, xlim))\n    \n    ax_2 = fig.add_subplot(122)\n    spec.plot_spec(ax = ax_2, db_off = True)\n    if xlim:\n        ax_2.set_xlim((0, xlim))\n    plt.show()\n\n\nplt_signal_spec(test_spec, xlim = 2000)\n\n\n\n\nThe figure on the left is the signal directly converted from raw audio file, which represents the magnitude/pressure of the recorded audio. Whereas the figure on the right is the corresponding converted spectrogram.\nWe will then add random noise to the signal, before converting to the spectrogram, the signal is added based on the normal distribution of the maximum value of the signal.\n\ntest_spec_noise = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\ntest_spec_noise.add_noise(noise_factor = 0.2)\n\n\nplt_signal_spec(test_spec_noise, xlim = 2000)\n\n\n\n\nNow there is some fuzziness going on in the signal! However, since the magnitude of the mel spectrogram is log scaled, it isnt obvious in the right figure.\n\nNote that the in practice, we dont add so much fuzziness in our training, the increased noise factor is for demonstration purpose only.\n\n\n\nSpectrogram masking\nAnother technique we used is the spectrogram masking, the masking function will randomly set the magnitude of spectrogram into 0, across the time and frequency of the spectrogram.\n\ntest_spec_mask = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\ntest_spec_mask.mask_spec()\n\n\nplt_signal_spec(test_spec_mask, xlim = 2000)\n\n\n\n\nNote how we have remove chunks of the frequency and time dimension of the spectrogram. If its not, run a few more time, its random anyway.\n\n\nSpectrogram shifting\nSince in our audio files, the recording always starts at the beginning, and there is always a few seconds of silence before the recording ends, we will need to shift the spectrogram, so that the machine learning models doesnt depends too much on the beginning of the time slices.\n\ntest_spec_shift = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\ntest_spec_shift.shift_spec(max_sec = 5)\n\n\nplt_signal_spec(test_spec_shift, xlim = 2000)\n\n\n\n\nNote the now the spectrogram is now shift to the right of time, we have set the maximum time it can shift for 5 seconds for demonstration purpose.\n\n\nSpectrogram truncation\nSince we have audio files of different length, we will need to preprocess it in a way that all of the sample have the same dimension as a numpy array. To do this we will simply be padding the numpy array as zeros, or trimming the last few seconds of the recording. The truncation is applied by default when the spectrogram class initialization is called.\nNow to get the size of our input (to be fed into the model), we wil simply run a sample, and acquire the shape from the numpy array.\n\n%%time\nsample = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', \n                    hop_length = hop_length, n_mels = 512,\n                    n_fft = n_fft)\n\nCPU times: user 143 ms, sys: 80 ms, total: 223 ms\nWall time: 325 ms\n\n\n\nspec_shape = sample.spec.shape\nspec_shape\n\n(512, 500)\n\n\nNow that we have the spectrogram ready for out training, we can use the instrument and pitch data in the metadata dataframe.\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\nmeta_df.head()\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n    \n      2\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#1\n      34.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      1\n    \n    \n      3\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#1\n      34.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      2\n    \n    \n      4\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#2\n      46.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      1\n    \n  \n\n\n\n\n\nmeta_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13265 entries, 0 to 13264\nData columns (total 17 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   Path                         13265 non-null  object \n 1   Family (abbr.)               13265 non-null  object \n 2   Family (in full)             13265 non-null  object \n 3   Instrument (abbr.)           13265 non-null  object \n 4   Instrument (in full)         13265 non-null  object \n 5   Technique (abbr.)            13265 non-null  object \n 6   Technique (in full)          13265 non-null  object \n 7   Pitch                        13265 non-null  object \n 8   Pitch ID (if applicable)     13162 non-null  float64\n 9   Dynamics                     13265 non-null  object \n 10  Dynamics ID (if applicable)  12646 non-null  float64\n 11  Instance ID                  13262 non-null  float64\n 12  Mute (abbr.)                 13265 non-null  object \n 13  Mute (in full)               13265 non-null  object \n 14  String ID (if applicable)    7516 non-null   float64\n 15  Needed digital retuning      13265 non-null  bool   \n 16  Fold                         13265 non-null  int64  \ndtypes: bool(1), float64(4), int64(1), object(11)\nmemory usage: 1.6+ MB\n\n\n\nmeta_df.describe()\n\n\n\n\n\n  \n    \n      \n      Pitch ID (if applicable)\n      Dynamics ID (if applicable)\n      Instance ID\n      String ID (if applicable)\n      Fold\n    \n  \n  \n    \n      count\n      13162.000000\n      12646.000000\n      13262.000000\n      7516.000000\n      13265.000000\n    \n    \n      mean\n      63.842653\n      2.073857\n      0.848138\n      2.360298\n      2.000000\n    \n    \n      std\n      16.512067\n      1.329919\n      1.177874\n      1.196041\n      1.414267\n    \n    \n      min\n      20.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n    \n    \n      25%\n      52.000000\n      2.000000\n      0.000000\n      1.000000\n      1.000000\n    \n    \n      50%\n      64.000000\n      2.000000\n      0.000000\n      2.000000\n      2.000000\n    \n    \n      75%\n      76.000000\n      3.000000\n      2.000000\n      3.000000\n      3.000000\n    \n    \n      max\n      109.000000\n      4.000000\n      12.000000\n      6.000000\n      4.000000\n    \n  \n\n\n\n\n\nmeta_df.isnull().sum()\n\nPath                              0\nFamily (abbr.)                    0\nFamily (in full)                  0\nInstrument (abbr.)                0\nInstrument (in full)              0\nTechnique (abbr.)                 0\nTechnique (in full)               0\nPitch                             0\nPitch ID (if applicable)        103\nDynamics                          0\nDynamics ID (if applicable)     619\nInstance ID                       3\nMute (abbr.)                      0\nMute (in full)                    0\nString ID (if applicable)      5749\nNeeded digital retuning           0\nFold                              0\ndtype: int64\n\n\n\nmeta_df['Instrument (in full)'].value_counts()\n\nViolin            1987\nViola             1952\nContrabass        1636\nCello             1593\nAccordion          872\nTrombone           670\nTrumpet in C       590\nFrench Horn        589\nFlute              529\nHarp               507\nBass Tuba          500\nClarinet in Bb     406\nAlto Saxophone     377\nBassoon            358\nGuitar             353\nOboe               346\nName: Instrument (in full), dtype: int64\n\n\nWe donreally care about the pitchID, Dynamics Id and String ID.\nWe can see some degree of bias in the instrument classes.\n\n\n\nModel fitting\n\ntrain_df, test_df = train_test_split(meta_df, train_size = 0.7, random_state = 42)\n\n\nprint('The number of rows for the training data is ', train_df.shape[0])\nprint('The number of rows for the test data is ', test_df.shape[0])\n\nThe number of rows for the training data is  9285\nThe number of rows for the test data is  3980\n\n\n\nBATCH_SIZE = 32\n\ntrain_generator = tf.data.Dataset.from_generator(lambda: spec_generator(train_df, BATCH_SIZE, \n                    add_channel = True, live_generation = True), \n                    output_types=(tf.float32, tf.int32),\n                    output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                    (BATCH_SIZE, 16))).prefetch(5)\neval_generator = tf.data.Dataset.from_generator(lambda: spec_generator(test_df, BATCH_SIZE, \n                    add_channel = True, live_generation = True,), \n                    output_types=(tf.float32, tf.int32), \n                    output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                    (BATCH_SIZE, 16))).prefetch(5)\n\n2022-08-08 01:38:09.491735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-08 01:38:09.493556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:09.494346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:09.495138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.299483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.299828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.300078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.300294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3370 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n\ntrain_generator\n\n<PrefetchDataset element_spec=(TensorSpec(shape=(32, 512, 500, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 16), dtype=tf.int32, name=None))>\n\n\n\nmodel = models.Sequential()\nmodel.add(layers.InputLayer((spec_shape[0], spec_shape[1], 1), batch_size = BATCH_SIZE, \n                                    dtype = tf.float32))\nmodel.add(layers.Conv2D(15, (15, 200), strides=(10, 10), activation='relu'))\n# , input_shape = (spec_shape[0], spec_shape[1], 1)))\nmodel.add(layers.MaxPool2D((5, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(16, activation = 'sigmoid'))\nmodel.build()\n\n\ntf.keras.utils.plot_model(model, show_shapes = True, show_dtype= True)\n\n\n\n\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\nprint(model.metrics)\n\n[]\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (32, 50, 31, 15)          45015     \n                                                                 \n max_pooling2d (MaxPooling2D  (32, 10, 15, 15)         0         \n )                                                               \n                                                                 \n flatten (Flatten)           (32, 2250)                0         \n                                                                 \n dense (Dense)               (32, 16)                  36016     \n                                                                 \n=================================================================\nTotal params: 81,031\nTrainable params: 81,031\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom datetime import datetime\ndatetime.now().strftime('%Y%m%d_%H%M%S')\n\n'20220808_013811'\n\n\n\ntrain_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      4851\n      Strings/Contrabass/pizzicato_bartok/Cb-pizz_ba...\n      Strings\n      Violin Family\n      Cb\n      Contrabass\n      pizz_bartok\n      pizzicato_bartok\n      F3\n      53.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      1.0\n      False\n      1\n    \n    \n      12565\n      Winds/Oboe+sordina/ordinario/Ob+S-ord-E6-mf-N-...\n      Winds\n      Woodwinds\n      Ob\n      Oboe\n      ord\n      ordinario\n      E6\n      88.0\n      mf\n      2.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      4\n    \n  \n\n\n\n\n\n%load_ext tensorboard\n\n\n# ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n#     f\"../models/baseline_checkpoint/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_model\", \n#                                                     monitor='val_accuracy')\n# early_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 2, \n#                                                         restore_best_weights = True)\n# log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n\n# history = model.fit(train_generator, epochs = 10, verbose=1, \n#                     validation_data = eval_generator, \n#                     validation_freq= 1, \n#                     # use_multiprocessing=True, workers = 2, \n#                     callbacks=[ckpt_callback, early_callback, tensorboard_callback])\n\n\n# model.save('../models/new_baseline/')\n\nNow that we have an working model, we need to save the model and the history object, we have defined a short checkpoint callback to save the model automatically, now lets define a function to save the history.\n\nimport pickle\n\ndef save_history(history, path):\n    with open(path, 'wb+') as f:\n        pickle.dump(history, f)\n        \ndef load_history(path):\n    with open(path, 'rb') as f:\n        return pickle.load(f)\n\n\n# save_history(history.history, '../models/new_baseline/history.pkl')\n\n\nhistory = load_history('../models/new_baseline/history.pkl')\n\n\nmodel = tf.keras.models.load_model('../models/new_baseline/')\n\n\nhistory.keys()\n\ndict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n\n\n\n\nModel evaluation\nIts time to check if our model is good! drum rolling\n\nplt.plot(history['val_accuracy'], label = 'Validation accuracy')\nplt.plot(history['accuracy'], label = 'Training accuracy')\nplt.title('Accuracy of baseline model after early callback')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nThe figure above shows the both the training and testing accuracy for our instrument classification. We can see that the testing accuracy starts to decrease after 2 epoch, and only peaks at 26% accuracy.\nOur current options are to apply dropout layer or regularizer for out current layers. However, due to the time constrant, we have decided to apply these technique on more complicated models, as this baseline model should only be serving the purpose of quick gauge of how well CNN can perform on out dataset.\nThis indicate the limitation of our fairly simple baseline model. To get an idea of how the classification if performed over class, lets have a look at the prediction across all classes.\nNow we are interested in the confusion matrix to represents the prediction for each class, due to gpu memory limitation, we will only be taking the snapshot of the test data as the classification report.\n\nprediction_generator = spec_generator(test_df, test_df.shape[0] // 5, add_channel=True,\n                                        live_generation = True, preprocess = False)\n\n\ninstrument_list = sorted(meta_df['Instrument (in full)'].unique())\ninstrument_list\n\n['Accordion',\n 'Alto Saxophone',\n 'Bass Tuba',\n 'Bassoon',\n 'Cello',\n 'Clarinet in Bb',\n 'Contrabass',\n 'Flute',\n 'French Horn',\n 'Guitar',\n 'Harp',\n 'Oboe',\n 'Trombone',\n 'Trumpet in C',\n 'Viola',\n 'Violin']\n\n\n\ndef orchidea_confusion_matrix(model, generator, instrument_list):\n    '''\n    Plot confusion matrix for OrchideaSOL dataset\n    \n    Input:\n    model: Model to be used\n    generator: Sequence class, generator to generate feature and labels for \n                OrchideaSOL dataset\n    instrument_list: list of instrument in alphabetical order, used to label the plot\n    \n    Output:\n    predict: np.array, Predicted output\n    prediction_label: np.array, True label\n    '''\n    prediction_feature, prediction_label = generator.__getitem__(0)\n    predict = predict = model.predict(prediction_feature)\n    assert prediction_label.shape == predict.shape\n    from sklearn.metrics import confusion_matrix\n\n    plt.figure(figsize = (12, 10))\n    sns.heatmap(confusion_matrix(np.argmax(prediction_label, axis=1), \n                        np.argmax(predict, axis = 1)), annot = True, \n                        xticklabels=instrument_list, \n                        yticklabels=instrument_list)\n    plt.title('Confusion matrix for baseline model')\n    plt.show()\n\n    return predict, prediction_label\n\n\npredict, prediction_label = orchidea_confusion_matrix(model, \n                                prediction_generator, instrument_list)\n\n25/25 [==============================] - 1s 23ms/step\n\n\n\n\n\nAccording to our baseline model, everything is a violin or viola, with the exception of good prediction on controbass. We will need to adjsut our loss function by assinging weighted entropy, or by creating more data for other classes using data augmentation.\nHowever, the last option is unfavourable, since it will increase the training time per epochs.\n\nprint(classification_report(np.argmax(prediction_label, axis=1), \n                        np.argmax(predict, axis = 1)))\n\n              precision    recall  f1-score   support\n\n           0       0.50      0.02      0.03        65\n           1       0.00      0.00      0.00        25\n           2       0.00      0.00      0.00        32\n           3       0.00      0.00      0.00        20\n           4       0.15      0.06      0.08        88\n           5       0.17      0.04      0.07        23\n           6       0.36      0.45      0.40        84\n           7       0.14      0.06      0.08        36\n           8       0.67      0.06      0.10        36\n           9       0.33      0.05      0.08        21\n          10       0.33      0.11      0.17        27\n          11       0.00      0.00      0.00        18\n          12       0.33      0.23      0.27        35\n          13       0.40      0.06      0.10        34\n          14       0.20      0.67      0.31       119\n          15       0.21      0.29      0.24       133\n\n    accuracy                           0.23       796\n   macro avg       0.24      0.13      0.12       796\nweighted avg       0.25      0.23      0.17       796\n\n\n\nThe precisio nnad recall is bad all across the board, with the exception of 67% recall on instrument 14 (viola)"
  },
  {
    "objectID": "posts/music/Music_transcription_fastai/notebooks/test.html",
    "href": "posts/music/Music_transcription_fastai/notebooks/test.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nfrom sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n\nfrom spec_generator_sequence import _get_spec, spec_generator\nfrom spec_generator_sequence_multilabel import (spec_generator_multi,\n                                                spec_generator_multioutput)\nfrom spec_input_generator import gen, gen_eval\nfrom spectrogram_class import spectrogram\nfrom classic_generator import classic_generator\n\n\nfrom classic_generator import _instrument_label_generator, get_full_path, classic_train_generator, classic_generator\n\n\nget_full_path('/train_data/2335.wav', mode='train')\n\n'/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav'\n\n\n\ny, sr = librosa.load('/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav', \n                        sr = 44100)\n\n\nplt.plot(y)\n\n\n\n\n\nmel_spec = librosa.feature.melspectrogram(y, n_mels = 128, sr = 44100)\n\n\nsns.heatmap(mel_spec)\n\n<AxesSubplot:>\n\n\n\n\n\n\nlibrosa.display.specshow(mel_spec)\n\n<matplotlib.collections.QuadMesh at 0x7fa87c058e20>\n\n\n\n\n\n\nclassic_generator_test = classic_generator(batch_size = 1)\n\n\nx_list = [path.rsplit('/', 1)[-1].rsplit('.')[0] for path in classic_generator_test.x]\ny_list = [path.rsplit('/', 1)[-1].rsplit('.')[0] for path in classic_generator_test.y]\n\n\nx_list == y_list\n\nTrue\n\n\n\nclassic_generator_test.y[0]\n\n'/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_labels/1727.csv'\n\n\n\nclassic_generator_test.x[122]\n\n'/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2366.wav'\n\n\n\n# classic_train_generator('/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav')\n\n\ntest_classic_gen = classic_generator_test.__getitem__(3)\n\n\ntest_classic_gen[0][0].shape\n\n(128, 200, 1)\n\n\n\ntest_classic_gen[1]['instrument_1'].shape\n\n(1, 200, 83)\n\n\n\nsns.heatmap(np.squeeze(test_classic_gen[0][0], -1))\n\n<AxesSubplot:>\n\n\n\n\n\n\ntest_classic_gen[0][0].shape\n\n(3769, 128, 1)\n\n\n\nsns.heatmap(test_classic_gen[1]['1'][1])\n\n\n%run classic_generator\n\n\ndef ger_instrument_frame(file, ins, num_freq, num_time):\n    _df = pd.read_csv(file)\n    _df = _df[_df['instrument'] == ins]\n    tmp_arr = np.zeros((num_freq, num_time))\n    for i in _df.iterrows():\n        start_time = i['start_time']\n\n/bin/bash: /home/shiya/anaconda3/envs/music/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/home/shiya/Documents/music_transcription/notebooks\n\n\n\ncwd = os.getcwd()\n\n\ncwd\n\n'/home/shiya/Documents/music_transcription/notebooks'\n\n\n\nos.path.join(os.getcwd(), '/../data/classic/musicnet/train_labels/1727.csv')\n\n'/../data/classic/musicnet/train_labels/1727.csv'\n\n\n\nlibrosa.get_duration(filename = '../data/classic/musicnet/train_data/1727.wav')\n\n447.0595918367347\n\n\n\n'../data/classic/musicnet/train_labels/1727.csv'.rsplit('/', maxsplit=1)\n\n['../data/classic/musicnet/train_labels', '1727.csv']\n\n\n\nos.path.join('../data/classic/musicnet/train_labels/', \n                '../train_data/', \n                '1727.wav')\n\n'../data/classic/musicnet/train_labels/../train_data/1727.wav'\n\n\n\ntest_inst_generator = _instrument_label_generator('../data/classic/musicnet/train_labels/1727.csv', 1, 9000, mode = 'train')\n\n\nsns.heatmap(test_inst_generator)\n\n<AxesSubplot:>\n\n\n\n\n\n\nclassic_label = pd.read_csv('../data/classic/musicnet/train_labels/1727.csv')\nclassic_label.head()\n\n\n\n\n\n  \n    \n      \n      start_time\n      end_time\n      instrument\n      note\n      start_beat\n      end_beat\n      note_value\n    \n  \n  \n    \n      0\n      9182\n      90078\n      43\n      53\n      4.0\n      1.5\n      Dotted Quarter\n    \n    \n      1\n      9182\n      33758\n      42\n      65\n      4.0\n      0.5\n      Eighth\n    \n    \n      2\n      9182\n      62430\n      1\n      69\n      4.0\n      1.0\n      Quarter\n    \n    \n      3\n      9182\n      202206\n      44\n      41\n      4.0\n      3.5\n      Whole\n    \n    \n      4\n      9182\n      62430\n      1\n      81\n      4.0\n      1.0\n      Quarter\n    \n  \n\n\n\n\n\nclassic_1_inst = classic_label[classic_label['instrument'] == 1]\nclassic_1_inst.head()\n\n\n\n\n\n  \n    \n      \n      start_time\n      end_time\n      instrument\n      note\n      start_beat\n      end_beat\n      note_value\n    \n  \n  \n    \n      2\n      9182\n      62430\n      1\n      69\n      4.0\n      1.0\n      Quarter\n    \n    \n      4\n      9182\n      62430\n      1\n      81\n      4.0\n      1.0\n      Quarter\n    \n    \n      7\n      62430\n      119774\n      1\n      84\n      5.0\n      1.0\n      Quarter\n    \n    \n      8\n      62430\n      119774\n      1\n      72\n      5.0\n      1.0\n      Quarter\n    \n    \n      11\n      119774\n      145886\n      1\n      74\n      6.0\n      0.5\n      Eighth\n    \n  \n\n\n\n\n\nfor i in classic_label.head(2).iterrows():\n    print(i)\n    print(type(i[1]))\n\n(0, start_time              9182\nend_time               90078\ninstrument                43\nnote                      53\nstart_beat               4.0\nend_beat                 1.5\nnote_value    Dotted Quarter\nName: 0, dtype: object)\n<class 'pandas.core.series.Series'>\n(1, start_time      9182\nend_time       33758\ninstrument        42\nnote              65\nstart_beat       4.0\nend_beat         0.5\nnote_value    Eighth\nName: 1, dtype: object)\n<class 'pandas.core.series.Series'>\n\n\n\nclassic_spec = spectrogram('../data/classic/musicnet/train_data/1727.wav', \n                            trunc_off=True)\n\n\nlabel_list = os.listdir('../data/classic/musicnet/train_labels/')\nlabel_list[:3]\n\n['2422.csv', '2114.csv', '2335.csv']\n\n\n\ndf_list = []\nfor i in label_list:\n    df_list.append(pd.read_csv('../data/classic/musicnet/train_labels/' + i))\nlabel_df = pd.concat(df_list)\nlabel_df.head(2)\n\n\n\n\n\n  \n    \n      \n      start_time\n      end_time\n      instrument\n      note\n      start_beat\n      end_beat\n      note_value\n    \n  \n  \n    \n      0\n      90078\n      124382\n      1\n      60\n      0.5\n      0.489583\n      Quarter\n    \n    \n      1\n      124382\n      138718\n      1\n      65\n      1.0\n      0.489583\n      Quarter\n    \n  \n\n\n\n\n\nlabel_df['instrument'].unique()\n\narray([ 1, 43, 41, 61, 71, 72, 74, 69, 42, 44,  7])\n\n\n\nlen(label_df['note'].unique())\n\n83\n\n\n\n19233758/ len(classic_spec.signal) * 9627\n\n9391.849238622863\n\n\n\n19421150/ len(classic_spec.signal) * 9627\n\n9483.352802956157\n\n\n\nclassic_spec.sr\n\n44100\n\n\n\nclassic_spec.spec.shape\n\n(256, 9627)\n\n\n\ny = np.array([['a', 'r'], ['b', 'q'], ['c', 'z']])\n\nnb = MultiLabelBinarizer()\nnb.fit(y)\n\nnb.transform(np.array([['a', np.nan], ['d']]))\n\narray([[1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0]])\n\n\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\n\n\nmultioutput_generator = spec_generator_multioutput(meta_df, 32)\n\n\nmultioutput_generator.__getitem__(2)[1][1].shape\n\nKeyError: 1\n\n\n\nmeta_df['Pitch ID (if applicable)'][meta_df['Pitch ID (if applicable)'].isna()]\n\n138     NaN\n142     NaN\n233     NaN\n234     NaN\n235     NaN\n         ..\n13027   NaN\n13028   NaN\n13029   NaN\n13030   NaN\n13043   NaN\nName: Pitch ID (if applicable), Length: 103, dtype: float64\n\n\n\ngenerator = spec_generator(meta_df, 32)\n\n\nprint(generator.indices)\n\n[9012 3451 6714 ... 9387 4120 7367]\n\n\n\n\n%run spec_generator_sequence_multilabel.py\n\n\ngenerator\n\n<spec_generator_sequence.spec_generator at 0x7f58509d07c0>\n\n\n\ngenerate_multi = spec_generator_multi(meta_df, 32)\n\n\ngenerate_multi.__getitem__(2)[1].shape\n\n(32, 107)\n\n\n\ngenerate_multi.\n\nSyntaxError: invalid syntax (446800042.py, line 1)\n\n\n\nimport random\n\n\n\n%%timeit\nrandom_num = random.randint(1, 50)\ngenerator.__getitem__(random_num)[1][1]\n\n40.6 ms  8.01 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n\n\n\ntest = pd.DataFrame({'test':['d', 'z', 'r', 'e', 'y']})\n\n\ntest\n\n\n\n\n\n  \n    \n      \n      test\n    \n  \n  \n    \n      0\n      d\n    \n    \n      1\n      z\n    \n    \n      2\n      r\n    \n    \n      3\n      e\n    \n    \n      4\n      y\n    \n  \n\n\n\n\n\nhot = OneHotEncoder(sparse=False)\nhot.fit_transform(test)\n\narray([[1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       [0., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0.]])\n\n\n\nhot.categories_\n\n[array(['d', 'e', 'r', 'y', 'z'], dtype=object)]\n\n\n\n(np.random.randint(0, 2, size=10000) == np.random.randint(0, 2, size=10000)).mean()\n\n0.4947\n\n\n\nhop_length = 2048\nwin_length = 512\nn_fft = 1024\n\n\nmeta_df['Path'].sample(1).values[0]\n\n'Brass/Trumpet_C+sordina_wah/flatterzunge_open/TpC+SW-flatt_open-G#3-mf-N-N.wav'\n\n\n\n%%timeit\npath = meta_df['Path'].sample(1).values[0]\ntest = _get_spec('Winds/Flute/ordinario/Fl-ord-D6-ff-N-T20d.wav', test_verbose=False)\n\n962 s  37.6 s per loop (mean  std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%run spectrogram_class.py\n\nAttributeError: 'spectrogram' object has no attribute 'spec'\n\n\n\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\n\nmeta_test = meta_df[['Instrument (in full)']]\n\n\none_hot = OneHotEncoder(sparse= False)\n\none_hot.fit_transform(meta_test)\n\narray([[0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.]])\n\n\n\nmeta_df['Path'][2:6].values\n\narray(['Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#1-f-N-T20u.wav',\n       'Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#1-p-N-T22u.wav',\n       'Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#2-f-N-T29u.wav',\n       'Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#2-p-N-T31u.wav'],\n      dtype=object)\n\n\n\none_hot.categories_\n\n[array(['Accordion', 'Alto Saxophone', 'Bass Tuba', 'Bassoon', 'Cello',\n        'Clarinet in Bb', 'Contrabass', 'Flute', 'French Horn', 'Guitar',\n        'Harp', 'Oboe', 'Trombone', 'Trumpet in C', 'Viola', 'Violin'],\n       dtype=object)]\n\n\n\nmeta_freq = 1/meta_df.groupby('Instrument (in full)')['Instrument (in full)'].transform('count')\n\n\nmeta_freq\n\n0        0.002000\n1        0.002000\n2        0.002000\n3        0.002000\n4        0.002000\n           ...   \n13260    0.002653\n13261    0.002653\n13262    0.002653\n13263    0.002653\n13264    0.002653\nName: Instrument (in full), Length: 13265, dtype: float64\n\n\n\n%%time\nmeta_df.sample(32, )\n            # replace = True, \n            # weights=meta_freq)[['Instrument (in full)']].value_counts(normalize=True)\n\nCPU times: user 1.75 ms, sys: 0 ns, total: 1.75 ms\nWall time: 1.54 ms\n\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      978\n      Brass/Horn/sforzato/Hn-sfz-F#2-f-N-N.wav\n      Brass\n      Brass\n      Hn\n      French Horn\n      sfz\n      sforzato\n      F#2\n      42.0\n      f\n      3.0\n      0.0\n      N\n      None\n      NaN\n      False\n      0\n    \n    \n      748\n      Brass/Horn/flatterzunge_stopped/Hn-flatt_stopp...\n      Brass\n      Brass\n      Hn\n      French Horn\n      flatt_stopped\n      flatterzunge_stopped\n      B3\n      59.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      4\n    \n    \n      7327\n      Strings/Viola/sul_tasto_tremolo/Va-tasto_trem-...\n      Strings\n      Violin Family\n      Va\n      Viola\n      tasto_trem\n      sul_tasto_tremolo\n      E5\n      76.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      True\n      0\n    \n    \n      2757\n      Keyboards/Accordion/ordinario/Acc-ord-D#4-mf-a...\n      Keyboards\n      Keyboards\n      Acc\n      Accordion\n      ord\n      ordinario\n      D#4\n      63.0\n      mf\n      2.0\n      4.0\n      N\n      None\n      NaN\n      False\n      4\n    \n    \n      3972\n      PluckedStrings/Harp/ordinario/Hp-ord-G#4-mf-N-...\n      PluckedStrings\n      Plucked Strings\n      Hp\n      Harp\n      ord\n      ordinario\n      G#4\n      68.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      4\n    \n    \n      7147\n      Strings/Viola/sul_ponticello/Va-pont-F#5-mf-3c...\n      Strings\n      Violin Family\n      Va\n      Viola\n      pont\n      sul_ponticello\n      F#5\n      78.0\n      mf\n      2.0\n      2.0\n      N\n      None\n      3.0\n      False\n      0\n    \n    \n      10638\n      Strings/Violoncello/pizzicato_secco/Vc-pizz_se...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      pizz_sec\n      pizzicato_secco\n      A3\n      57.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      1.0\n      False\n      0\n    \n    \n      7542\n      Strings/Viola/tremolo/Va-trem-D6-mf-1c-T12u.wav\n      Strings\n      Violin Family\n      Va\n      Viola\n      trem\n      tremolo\n      D6\n      86.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      True\n      3\n    \n    \n      7457\n      Strings/Viola/tremolo/Va-trem-C#5-pp-2c-N.wav\n      Strings\n      Violin Family\n      Va\n      Viola\n      trem\n      tremolo\n      C#5\n      73.0\n      pp\n      0.0\n      1.0\n      N\n      None\n      2.0\n      False\n      3\n    \n    \n      8171\n      Strings/Violin/artificial_harmonic/Vn-art_harm...\n      Strings\n      Violin Family\n      Vn\n      Violin\n      art_harm\n      artificial_harmonic\n      B7\n      107.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      False\n      3\n    \n    \n      12890\n      Winds/Sax_Alto/aeolian/ASax-aeol-A3-p-N-R100u.wav\n      Winds\n      Woodwinds\n      ASax\n      Alto Saxophone\n      aeol\n      aeolian\n      A3\n      57.0\n      p\n      1.0\n      0.0\n      N\n      None\n      NaN\n      True\n      2\n    \n    \n      482\n      Brass/Bass_Tuba/slap_pitched/BTb-slap-F#1-f-N-...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      slap\n      slap_pitched\n      F#1\n      30.0\n      f\n      3.0\n      0.0\n      N\n      None\n      NaN\n      False\n      0\n    \n    \n      9260\n      Strings/Violin/sul_ponticello/Vn-pont-A3-mf-4c...\n      Strings\n      Violin Family\n      Vn\n      Violin\n      pont\n      sul_ponticello\n      A3\n      57.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      0\n    \n    \n      2346\n      Brass/Trumpet_C/slap_pitched/TpC-slap-G#4-p-N-...\n      Brass\n      Brass\n      TpC\n      Trumpet in C\n      slap\n      slap_pitched\n      G#4\n      68.0\n      p\n      1.0\n      0.0\n      N\n      None\n      NaN\n      False\n      1\n    \n    \n      3475\n      PluckedStrings/Guitar/ordinario_high_register/...\n      PluckedStrings\n      Plucked Strings\n      Gtr\n      Guitar\n      ord_hi_reg\n      ordinario_high_register\n      D6\n      86.0\n      mf\n      2.0\n      1.0\n      N\n      None\n      2.0\n      False\n      2\n    \n    \n      3089\n      Keyboards/Accordion/ordinario/Acc-ord-G1-pp-N-...\n      Keyboards\n      Keyboards\n      Acc\n      Accordion\n      ord\n      ordinario\n      G1\n      31.0\n      pp\n      0.0\n      0.0\n      N\n      None\n      NaN\n      False\n      0\n    \n    \n      8444\n      Strings/Violin/ordinario/Vn-ord-B4-mf-4c-N.wav\n      Strings\n      Violin Family\n      Vn\n      Violin\n      ord\n      ordinario\n      B4\n      71.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      1\n    \n    \n      5873\n      Strings/Viola+sordina/ordinario/Va+S-ord-G3-mf...\n      Strings\n      Violin Family\n      Va\n      Viola\n      ord\n      ordinario\n      G3\n      55.0\n      mf\n      2.0\n      3.0\n      S\n      Sordina\n      4.0\n      True\n      1\n    \n    \n      6683\n      Strings/Viola/pizzicato_l_vib/Va-pizz_lv-C#4-m...\n      Strings\n      Violin Family\n      Va\n      Viola\n      pizz_lv\n      pizzicato_l_vib\n      C#4\n      61.0\n      mf\n      2.0\n      2.0\n      N\n      None\n      3.0\n      False\n      2\n    \n    \n      9961\n      Strings/Violoncello+sordina_piombo/tremolo/Vc+...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      trem\n      tremolo\n      G#4\n      68.0\n      mf\n      2.0\n      0.0\n      SP\n      Piombo\n      1.0\n      False\n      0\n    \n    \n      1317\n      Brass/Trombone+sordina_wah/flatterzunge_open/T...\n      Brass\n      Brass\n      Tbn\n      Trombone\n      flatt_open\n      flatterzunge_open\n      B3\n      59.0\n      mf\n      2.0\n      0.0\n      SW\n      Wah\n      NaN\n      False\n      1\n    \n    \n      10597\n      Strings/Violoncello/pizzicato_l_vib/Vc-pizz_lv...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      pizz_lv\n      pizzicato_l_vib\n      F2\n      41.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      3\n    \n    \n      715\n      Brass/Horn/flatterzunge/Hn-flatt-F3-mf-N-N.wav\n      Brass\n      Brass\n      Hn\n      French Horn\n      flatt\n      flatterzunge\n      F3\n      53.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      2\n    \n    \n      6528\n      Strings/Viola/ordinario/Va-ord-G#3-pp-4c-T18u.wav\n      Strings\n      Violin Family\n      Va\n      Viola\n      ord\n      ordinario\n      G#3\n      56.0\n      pp\n      0.0\n      3.0\n      N\n      None\n      4.0\n      True\n      2\n    \n    \n      5542\n      Strings/Contrabass/tremolo/Cb-trem-B3-pp-1c-T1...\n      Strings\n      Violin Family\n      Cb\n      Contrabass\n      trem\n      tremolo\n      B3\n      59.0\n      pp\n      0.0\n      0.0\n      N\n      None\n      1.0\n      True\n      1\n    \n    \n      11158\n      Strings/Violoncello/tremolo/Vc-trem-D4-pp-2c-N...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      trem\n      tremolo\n      D4\n      62.0\n      pp\n      0.0\n      1.0\n      N\n      None\n      2.0\n      False\n      4\n    \n    \n      4033\n      PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_b...\n      PluckedStrings\n      Plucked Strings\n      Hp\n      Harp\n      pizz_bartok\n      pizzicato_bartok\n      D#2\n      39.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      NaN\n      True\n      0\n    \n    \n      10073\n      Strings/Violoncello/col_legno_battuto/Vc-legno...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      legno_batt\n      col_legno_battuto\n      C#2\n      37.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      2\n    \n    \n      12233\n      Winds/Flute/flatterzunge/Fl-flatt-D#6-ff-N-N.wav\n      Winds\n      Woodwinds\n      Fl\n      Flute\n      flatt\n      flatterzunge\n      D#6\n      87.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      NaN\n      False\n      1\n    \n    \n      11602\n      Winds/Bassoon/vibrato/Bn-vib-G#3-mf-N-N.wav\n      Winds\n      Woodwinds\n      Bn\n      Bassoon\n      vib\n      vibrato\n      G#3\n      56.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      2\n    \n    \n      10654\n      Strings/Violoncello/pizzicato_secco/Vc-pizz_se...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      pizz_sec\n      pizzicato_secco\n      B4\n      71.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      False\n      0\n    \n    \n      3546\n      PluckedStrings/Guitar/sul_ponticello/Gtr-pont-...\n      PluckedStrings\n      Plucked Strings\n      Gtr\n      Guitar\n      pont\n      sul_ponticello\n      C4\n      60.0\n      mf\n      2.0\n      1.0\n      N\n      None\n      2.0\n      False\n      3\n    \n  \n\n\n\n\n\nmeta_df.sample(1)['Path'].values[0]\n\n'Strings/Viola/ordinario/Va-ord-A4-ff-3c-R100d.wav'\n\n\n\ntest_spec = spectrogram(meta_df.sample(1)['Path'].values[0])\n\n\ntest_spec.plot_spec()\n\n\n\n\n\ntest, _ = librosa.load('../data/_OrchideaSOL2020_release/OrchideaSOL2020/PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', \n                    sr = None)\n\n\ntest.shape\n\n(826215,)\n\n\n\ndef mask_spec(arr, inplace = False):\n    loop = random.randint(1, 2)\n    tmp = arr.copy()\n    for i in range(loop):\n        start = random.randint(0, arr.shape[1])\n        duration = random.randint(25, 60)\n        if inplace == True:\n            arr[:, start:start + duration] = 0\n        else:\n            tmp[:, start:start+duration] = 0\n    freq_loop = random.randint(1, 3)\n    for freq in range(freq_loop):\n        start = random.randint(0, arr.shape[0])\n        duration = random.randint(25, 60)\n        if inplace == True:\n            arr[start:start + duration, :] = 0\n        else:\n            tmp[start:start + duration, :] = 0\n\n    return None if inplace == True else tmp\n\n\n# librosa.display.specshow(librosa.amplitude_to_db(mask_spec(spec_sample)), y_axis='log', x_axis = 's')\n\n\nimport random\nprint(random.randint(0, 9))\n\n9\n\n\n\nmeta_df['_ins'] = meta_df['Instrument (in full)']\n\n\nprint(random.randint.__doc__)\n\nReturn random integer in range [a, b], including both end points.\n        \n\n\n\ntest, _ = next(gen(meta_df, test_verbose = True))\n\nHIT\nSUCCESS\n\n\n\nprint(test)\n\n[[[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n ...\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]]\n\n\n\nnp.load('/home/shiya/Documents/Music_transcription_fastai/data/_OrchideaSOL2020_release/OrchideaSOL2020/Strings/Violin/ordinario/Vn-ord-A3-ff-4c-N.npy', \n            allow_pickle=True)\n\nFileNotFoundError: [Errno 2] No such file or directory: '/home/shiya/Documents/Music_transcription_fastai/data/_OrchideaSOL2020_release/OrchideaSOL2020/Strings/Violin/ordinario/Vn-ord-A3-ff-4c-N.npy'\n\n\n\n0 in test\n\nTrue\n\n\n\nprint(test.shape)\n\n(256, 500, 1)\n\n\n\nlibrosa.display.specshow(librosa.amplitude_to_db(np.reshape(test, newshape = test.shape[:2])), x_axis = 's', \n                                                y_axis = 'mel', sr=44100, hop_length=2048, \n                                                n_fft=2048)\n\n<matplotlib.collections.QuadMesh at 0x7f7700c99760>\n\n\n\n\n\n\nsample = meta_df.sample(1)\n\n\nsample['Path'].values\n\narray(['Strings/Violoncello+sordina_piombo/ordinario/Vc+SP-ord-D3-mf-2c-N.wav'],\n      dtype=object)\n\n\n\nspec = spectrogram(sample['Path'].values[0])\n\nAttributeError: 'spectrogram' object has no attribute 'spec'\n\n\n\nsample['Path']\n\n8462    Strings/Violin/ordinario/Vn-ord-C#4-pp-4c-N.wav\nName: Path, dtype: object\n\n\n\nnp.save('testnig.npy', spec.spec)\n\n\n!ls\n\n/bin/bash: /home/shiya/anaconda3/envs/music/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n__init__.py          spectrogram_class.py\nmodel.png            spectrogram.py\nmusic_transcription_2conv.ipynb  test.ipynb\nmusic_transcription_class.ipynb  testnig.npy\nmusic_transcription.ipynb    wav_converter_class.py\n__pycache__          wav_converter.py\nspec_input_generator.py\n\n\n\nload_test = np.load('testnig.npy', allow_pickle = True)\n\n\nload_test\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nload_test.shape\n\n()\n\n\n\nsample_path = meta_df.sample(1)['Path'].values[0]\nsample_path\n\n'Strings/Violin/ordinario/Vn-ord-A4-ff-2c-N.wav'\n\n\n\ntest_sample = spectrogram(sample_path, \n                preprocess = False, trunc_off = True)\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntest_sample.add_noise()\n\n\ntest_sample.generate_spec()\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntest_sample.mask_spec()\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntest_sample.shift_spec()\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntesttest.plot_spec()\n\nNameError: name 'testtest' is not defined"
  },
  {
    "objectID": "posts/sales/cycle-sales-kaggle.html",
    "href": "posts/sales/cycle-sales-kaggle.html",
    "title": "Bike sales analysis",
    "section": "",
    "text": "This is a notebook preceeding after SQL script and Tableau analysis. The main focus on this notebook is to perform statistical analysis, to determine if a relation between predoctors exists statistically.\nClustering analysis is also performed to determine if customers with certain demographic information can be grouped. \n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels as sm\nimport scipy\n\n\n\n\nCode\n!pip install optuna\n\n\nLets have a look at the dataframe we will be working with:\n\n\nCode\nsales_df = pd.read_csv('./sales_cleaned.csv', index_col='index')\nsales_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Date\n      Year\n      Month\n      Customer Age\n      Customer Gender\n      Country\n      State\n      Product Category\n      Sub Category\n      Quantity\n      Unit Cost\n      Unit Price\n      Cost\n      Revenue\n      net_profit\n      Day\n      Month_numeric\n    \n    \n      index\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      02/19/16\n      2016\n      February\n      29\n      F\n      United States\n      Washington\n      Accessories\n      Tires and Tubes\n      1.0\n      80.00\n      109.000000\n      80.0\n      109.0\n      29\n      19\n      2\n    \n    \n      1\n      02/20/16\n      2016\n      February\n      29\n      F\n      United States\n      Washington\n      Clothing\n      Gloves\n      2.0\n      24.50\n      28.500000\n      49.0\n      57.0\n      8\n      20\n      2\n    \n    \n      2\n      02/27/16\n      2016\n      February\n      29\n      F\n      United States\n      Washington\n      Accessories\n      Tires and Tubes\n      3.0\n      3.67\n      5.000000\n      11.0\n      15.0\n      4\n      27\n      2\n    \n    \n      3\n      03/12/16\n      2016\n      March\n      29\n      F\n      United States\n      Washington\n      Accessories\n      Tires and Tubes\n      2.0\n      87.50\n      116.500000\n      175.0\n      233.0\n      58\n      12\n      3\n    \n    \n      4\n      03/12/16\n      2016\n      March\n      29\n      F\n      United States\n      Washington\n      Accessories\n      Tires and Tubes\n      3.0\n      35.00\n      41.666667\n      105.0\n      125.0\n      20\n      12\n      3\n    \n  \n\n\n\n\n\n\nCode\nprint(f\"The dataframe has {sales_df.shape[0]} rows and {sales_df.shape[1]} features\")\n\n\nThe dataframe has 34866 rows and 17 features\n\n\nFrom the dataframe, we can see that the interesting columns are categorical features: * Countries/States * Product categories/sub-categories * Customers gender\nand the numberic features: * Cost * Revenue * Profit * Customers age\nALthough datetime features are also included in the dataframe, we will be ignoring these since the time-series analysis was analysed in our Tableau and SQL analysis.\n\n\nCode\nsales_df.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 34866 entries, 0 to 34865\nData columns (total 17 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Date              34866 non-null  object \n 1   Year              34866 non-null  int64  \n 2   Month             34866 non-null  object \n 3   Customer Age      34866 non-null  int64  \n 4   Customer Gender   34866 non-null  object \n 5   Country           34866 non-null  object \n 6   State             34866 non-null  object \n 7   Product Category  34866 non-null  object \n 8   Sub Category      34866 non-null  object \n 9   Quantity          34866 non-null  float64\n 10  Unit Cost         34866 non-null  float64\n 11  Unit Price        34866 non-null  float64\n 12  Cost              34866 non-null  float64\n 13  Revenue           34866 non-null  float64\n 14  net_profit        34866 non-null  int64  \n 15  Day               34866 non-null  int64  \n 16  Month_numeric     34866 non-null  int64  \ndtypes: float64(5), int64(5), object(7)\nmemory usage: 4.8+ MB\n\n\n\n\nCode\nsales_df.describe()\n\n\n\n\n\n\n  \n    \n      \n      Year\n      Customer Age\n      Quantity\n      Unit Cost\n      Unit Price\n      Cost\n      Revenue\n      net_profit\n      Day\n      Month_numeric\n    \n  \n  \n    \n      count\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n    \n    \n      mean\n      2015.569237\n      36.382895\n      2.002524\n      349.880567\n      389.232485\n      576.004532\n      640.870074\n      64.865542\n      15.667671\n      6.317845\n    \n    \n      std\n      0.495190\n      11.112902\n      0.813936\n      490.015846\n      525.319091\n      690.500395\n      736.650597\n      152.879908\n      8.770677\n      3.465317\n    \n    \n      min\n      2015.000000\n      17.000000\n      1.000000\n      0.670000\n      0.666667\n      2.000000\n      2.000000\n      -937.000000\n      1.000000\n      1.000000\n    \n    \n      25%\n      2015.000000\n      28.000000\n      1.000000\n      45.000000\n      53.666667\n      85.000000\n      102.000000\n      5.000000\n      8.000000\n      3.000000\n    \n    \n      50%\n      2016.000000\n      35.000000\n      2.000000\n      150.000000\n      179.000000\n      261.000000\n      319.000000\n      27.000000\n      16.000000\n      6.000000\n    \n    \n      75%\n      2016.000000\n      44.000000\n      3.000000\n      455.000000\n      521.000000\n      769.000000\n      902.000000\n      96.000000\n      23.000000\n      9.000000\n    \n    \n      max\n      2016.000000\n      87.000000\n      3.000000\n      3240.000000\n      5082.000000\n      3600.000000\n      5082.000000\n      1842.000000\n      31.000000\n      12.000000\n    \n  \n\n\n\n\nIt appears that most of the distribution are a long tailed distribution, skewed to the lower end.\n\n\nCode\nsns.pairplot(sales_df)\n\n\n<seaborn.axisgrid.PairGrid at 0x7ff808d9a7f0>\n\n\n\n\n\n\n\nCode\nsales_df.duplicated().mean()\n\n\n2.868123673492801e-05\n\n\n\n\nCode\nsales_df[sales_df.duplicated(keep=False)].sort_values(['Date', 'Revenue'])\n\n\n\n\n\n\n  \n    \n      \n      Date\n      Year\n      Month\n      Customer Age\n      Customer Gender\n      Country\n      State\n      Product Category\n      Sub Category\n      Quantity\n      Unit Cost\n      Unit Price\n      Cost\n      Revenue\n      net_profit\n      Day\n      Month_numeric\n    \n    \n      index\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      868\n      08/12/15\n      2015\n      August\n      43\n      F\n      Germany\n      Bayern\n      Accessories\n      Bottles and Cages\n      2.0\n      10.0\n      14.0\n      20.0\n      28.0\n      8\n      12\n      8\n    \n    \n      869\n      08/12/15\n      2015\n      August\n      43\n      F\n      Germany\n      Bayern\n      Accessories\n      Bottles and Cages\n      2.0\n      10.0\n      14.0\n      20.0\n      28.0\n      8\n      12\n      8\n    \n  \n\n\n\n\nOut of 34866 entries, there was one entry that appears to be duplicated. It is quite possible that one person with the same demographic information purchased the exact same item on the same day. We will leave the duplicated entry as it is only one of them."
  },
  {
    "objectID": "posts/sales/cycle-sales-kaggle.html#t-test-on-revenues-generated-for-different-gender.",
    "href": "posts/sales/cycle-sales-kaggle.html#t-test-on-revenues-generated-for-different-gender.",
    "title": "Bike sales analysis",
    "section": "T-test on revenues generated for different gender.",
    "text": "T-test on revenues generated for different gender.\nTo determine if different gender had different spending habits, we will be running a unpaired t-test to investigate if the two distribution is the same.\nThe hypothesis of a unpaired t-test is as follows: \\[H_0: \\bar{X} - \\bar{Y} = 0\\] \\[H_1: \\bar{X} - \\bar{Y} \\neq 0\\]\nwhere X and Y are the two distribution compared.\n\n\nCode\nttest_stat, ttest_p = scipy.stats.ttest_ind(sales_df[sales_df['Customer Gender'] == 'M'].Revenue, \n                   sales_df[sales_df['Customer Gender'] == 'F'].Revenue )\n\nprint(f'The p-vale for unpaired t-test for both gender is {ttest_p:.4f}')\n\n\nThe p-vale for unpaired t-test for both gender is 0.9855\n\n\nIndeed, we can see that the p-value is around 0.98, which is highly unlikely that the mean of revenue spent for each gender were different.\n\nHowever, from the distribution above, we can see that it is highly not normal. Although we do have 34688 entries, and the central limit theorem works in our favour, and the assumption of similar means from any sample drawn from the distribution is the same.\n\nJust to be safe, we will be running a Wilcoxon test (alternative non-parametric t-test)."
  },
  {
    "objectID": "posts/sales/cycle-sales-kaggle.html#mann-whitney-u-test-for-revenues-generated-by-eahc-gender.",
    "href": "posts/sales/cycle-sales-kaggle.html#mann-whitney-u-test-for-revenues-generated-by-eahc-gender.",
    "title": "Bike sales analysis",
    "section": "Mann-whitney U test for revenues generated by eahc gender.",
    "text": "Mann-whitney U test for revenues generated by eahc gender.\n\n\nCode\nwhitney_stat, whitney_p = scipy.stats.mannwhitneyu(sales_df[sales_df['Customer Gender'] == 'M'].Revenue, \n                   sales_df[sales_df['Customer Gender'] == 'F'].Revenue )\n\nprint(f'The p-vale for unpaired t-test for both gender is {whitney_p:.4f}')\n\n\nThe p-vale for unpaired t-test for both gender is 0.2144\n\n\n\nConclusion\nIndeed, from the p-value above, we can see that it is still higher than the threshold of 5%.\n\n\nThere is no statistical significant that each gender had different spending habit!"
  },
  {
    "objectID": "posts/sales/cycle-sales-kaggle.html#conclusion-1",
    "href": "posts/sales/cycle-sales-kaggle.html#conclusion-1",
    "title": "Bike sales analysis",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nThere is no statistical significant proof that gender is related to product bought.\n\n\n\n\nCode\ndef chi2_test(df, col1, col2, show_table = True):\n    assert col1 in df.columns and col2 in df.columns, 'Column names provide can not be found in dataframe!'\n    _cont = df.groupby([col1, col2])[col2].count().unstack()\n    _result = scipy.stats.chi2_contingency(_cont)\n    if show_table:\n        print('------------------------------')\n        print('Observed contigency table')\n        display(_cont)\n        print('------------------------------')\n        print('Expected frequency table')\n        display(pd.DataFrame(_result[-1], index = _cont.index, \n                    columns = _cont.columns))\n    print(f'The p-value of chi2 independence test is {_result[1]:.4f}')\n\n\n\n\nCode\nchi2_test(sales_df, 'Country', 'Product Category')\n\n\n------------------------------\nObserved contigency table\n\n\n\n\n\n\n  \n    \n      Product Category\n      Accessories\n      Bikes\n      Clothing\n    \n    \n      Country\n      \n      \n      \n    \n  \n  \n    \n      France\n      3293\n      1152\n      723\n    \n    \n      Germany\n      3200\n      1291\n      710\n    \n    \n      United Kingdom\n      3986\n      1497\n      938\n    \n    \n      United States\n      12055\n      3153\n      2868\n    \n  \n\n\n\n\n------------------------------\nExpected frequency table\n\n\n\n\n\n\n  \n    \n      Product Category\n      Accessories\n      Bikes\n      Clothing\n    \n    \n      Country\n      \n      \n      \n    \n  \n  \n    \n      France\n      3340.093845\n      1051.357311\n      776.548844\n    \n    \n      Germany\n      3361.421844\n      1058.070699\n      781.507457\n    \n    \n      United Kingdom\n      4149.911490\n      1306.262634\n      964.825876\n    \n    \n      United States\n      11682.572822\n      3677.309356\n      2716.117823\n    \n  \n\n\n\n\nThe p-value of chi2 independence test is 0.0000\n\n\n\n\nFrom the above result, we can see that there was deviation in sales, especially between United States with other countries. Infact, the corresponding p-value is 0, indicating there is a relation between the distribution of product sold and countries."
  },
  {
    "objectID": "posts/sales/cycle-sales-kaggle.html#k-prototype-clustering",
    "href": "posts/sales/cycle-sales-kaggle.html#k-prototype-clustering",
    "title": "Bike sales analysis",
    "section": "K-prototype clustering",
    "text": "K-prototype clustering\nIn the previous sub-section, we have explored the option of clustering the dataframe by one-hot-encoding. However, this method is not all sensible. The distance metric was arbitrary, although the Mahalanobis distance was used, there is no guarantee that the distance calculated is a good representation of the datapoints clustering (how is the distance between California and England comparable with the distance of $200 difference in revenue?).\nAnother method we will be using is the K-Prototypes clustering, which is a combination of K-means and K-modes algorithms.\n\n\nCode\nfrom kmodes.kprototypes import KPrototypes\n\n\n\n\nCode\nclustering_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Customer Age\n      Customer Gender\n      Product Category\n      Cost\n      Revenue\n      net_profit\n      Country_with_California\n    \n    \n      index\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      29\n      F\n      Accessories\n      80.0\n      109.0\n      29\n      United States\n    \n    \n      1\n      29\n      F\n      Clothing\n      49.0\n      57.0\n      8\n      United States\n    \n    \n      2\n      29\n      F\n      Accessories\n      11.0\n      15.0\n      4\n      United States\n    \n    \n      3\n      29\n      F\n      Accessories\n      175.0\n      233.0\n      58\n      United States\n    \n    \n      4\n      29\n      F\n      Accessories\n      105.0\n      125.0\n      20\n      United States\n    \n  \n\n\n\n\n\n\nCode\nnumeric_scaler = ColumnTransformer([('std_scaler', StandardScaler(), \n                        make_column_selector(dtype_include='number'))], \n                        remainder = 'passthrough', \n                        verbose_feature_names_out=False)\n\nclustering_scaled_df = pd.DataFrame(numeric_scaler.fit_transform(clustering_df),  \n                                    columns = numeric_scaler.get_feature_names_out()).apply(lambda x: pd.to_numeric(x, errors='ignore'))\n\nclustering_scaled_df\n\n\n\n\n\n\n  \n    \n      \n      Customer Age\n      Cost\n      Revenue\n      net_profit\n      Customer Gender\n      Product Category\n      Country_with_California\n    \n  \n  \n    \n      0\n      -0.664363\n      -0.718337\n      -0.722022\n      -0.234603\n      F\n      Accessories\n      United States\n    \n    \n      1\n      -0.664363\n      -0.763232\n      -0.792612\n      -0.371968\n      F\n      Clothing\n      United States\n    \n    \n      2\n      -0.664363\n      -0.818265\n      -0.849628\n      -0.398132\n      F\n      Accessories\n      United States\n    \n    \n      3\n      -0.664363\n      -0.580753\n      -0.553690\n      -0.044909\n      F\n      Accessories\n      United States\n    \n    \n      4\n      -0.664363\n      -0.682130\n      -0.700301\n      -0.293473\n      F\n      Accessories\n      United States\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      34861\n      0.145518\n      2.133260\n      1.148634\n      -4.100438\n      M\n      Bikes\n      France\n    \n    \n      34862\n      0.145518\n      2.525734\n      1.805672\n      -2.707167\n      M\n      Bikes\n      France\n    \n    \n      34863\n      0.145518\n      2.133260\n      1.278956\n      -3.472485\n      M\n      Bikes\n      France\n    \n    \n      34864\n      0.145518\n      2.133260\n      1.413350\n      -2.824908\n      M\n      Bikes\n      France\n    \n    \n      34865\n      0.145518\n      2.525734\n      1.258593\n      -5.343261\n      M\n      Bikes\n      France\n    \n  \n\n34866 rows  7 columns\n\n\n\n\n\nCode\nclustering_scaled_df.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 34866 entries, 0 to 34865\nData columns (total 7 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Customer Age             34866 non-null  float64\n 1   Cost                     34866 non-null  float64\n 2   Revenue                  34866 non-null  float64\n 3   net_profit               34866 non-null  float64\n 4   Customer Gender          34866 non-null  object \n 5   Product Category         34866 non-null  object \n 6   Country_with_California  34866 non-null  object \ndtypes: float64(4), object(3)\nmemory usage: 1.9+ MB\n\n\n\n\nCode\ncat_position = [clustering_scaled_df.columns.get_loc(col) for col in \n                    clustering_scaled_df.select_dtypes('object').columns]\ncat_position\n\n\n[4, 5, 6]\n\n\n\n\nCode\n\nclustering_scaled_df.select_dtypes('object').columns\n\n\nIndex(['Customer Gender', 'Product Category', 'Country_with_California'], dtype='object')\n\n\n\n\nCode\n# kproto = KPrototypes()\n\n# kproto.fit_predict(clustering_scaled_df, categorical = cat_position )\n\n\n\n\nCode\nkproto.cost_\n\n\nNameError: name 'kproto' is not defined\n\n\n\n\nCode\nkproto_results ={'cluster_size': [], 'cost':[]} \n\nclustering_scaled_df_sample = clustering_scaled_df.sample(frac=0.5)\n\nfor cluster_size in range(1, 8):\n    print(f'Fitting Kprototypes for cluster size :{cluster_size}')\n    _kproto = KPrototypes(n_clusters=cluster_size)\n    _labels = _kproto.fit_predict(clustering_scaled_df_sample, categorical = cat_position)\n    kproto_results['cluster_size'].append(cluster_size)\n    kproto_results['cost'].append(_kproto.cost_)\n    print(f'Cost: {_kproto.cost_}')\n    print('----------------------------------------')\n\n\nFitting Kprototypes for cluster size :1\nCost: 81947.90456327623\n----------------------------------------\nFitting Kprototypes for cluster size :2\nCost: 54558.85795930906\n----------------------------------------\nFitting Kprototypes for cluster size :3\nCost: 44219.11078464947\n----------------------------------------\nFitting Kprototypes for cluster size :4\nCost: 35877.56991156543\n----------------------------------------\nFitting Kprototypes for cluster size :5\nCost: 31335.38134042823\n----------------------------------------\nFitting Kprototypes for cluster size :6\nCost: 28857.25619255692\n----------------------------------------\nFitting Kprototypes for cluster size :7\nCost: 26973.43389824587\n----------------------------------------\n\n\n\n\nCode\n# import pickle\n# import os\n\n# with open('kproto_result.pkl', 'wb') as file:\n#     # assert not os.path.getsize(file) > 0, f\"{file} already exists! Prone to overwriting.\"\n#     pickle.dump(kproto_results, file=file)\n\n\n\n\n\nCode\nimport pickle\n\nwith open('./kproto_result.pkl', 'rb') as file:\n    kproto_results = pickle.load(file)\n\n\n\n\nCode\nplt.plot(kproto_results['cluster_size'], kproto_results['cost'])\nplt.title('Elbow graph for Kprototypes clustering')\nplt.xlabel('Cluster size')\nplt.ylabel('Cost value')\n\n\nText(0, 0.5, 'Cost value')\n\n\n\n\n\n\n\nCode\nkproto_best = KPrototypes(5, random_state=43)\nkproto_best_label = kproto_best.fit_predict(clustering_scaled_df, categorical = cat_position)\n\nclustering_scaled_df['kproto_labels'] = kproto_best_label\n\n\n\n\nCode\nclustering_scaled_df\n\n\n\n\n\n\n  \n    \n      \n      Customer Age\n      Cost\n      Revenue\n      net_profit\n      Customer Gender\n      Product Category\n      Country_with_California\n      kproto_labels\n    \n  \n  \n    \n      0\n      -0.664363\n      -0.718337\n      -0.722022\n      -0.234603\n      F\n      Accessories\n      United States\n      2\n    \n    \n      1\n      -0.664363\n      -0.763232\n      -0.792612\n      -0.371968\n      F\n      Clothing\n      United States\n      2\n    \n    \n      2\n      -0.664363\n      -0.818265\n      -0.849628\n      -0.398132\n      F\n      Accessories\n      United States\n      2\n    \n    \n      3\n      -0.664363\n      -0.580753\n      -0.553690\n      -0.044909\n      F\n      Accessories\n      United States\n      2\n    \n    \n      4\n      -0.664363\n      -0.682130\n      -0.700301\n      -0.293473\n      F\n      Accessories\n      United States\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      34861\n      0.145518\n      2.133260\n      1.148634\n      -4.100438\n      M\n      Bikes\n      France\n      1\n    \n    \n      34862\n      0.145518\n      2.525734\n      1.805672\n      -2.707167\n      M\n      Bikes\n      France\n      1\n    \n    \n      34863\n      0.145518\n      2.133260\n      1.278956\n      -3.472485\n      M\n      Bikes\n      France\n      1\n    \n    \n      34864\n      0.145518\n      2.133260\n      1.413350\n      -2.824908\n      M\n      Bikes\n      France\n      1\n    \n    \n      34865\n      0.145518\n      2.525734\n      1.258593\n      -5.343261\n      M\n      Bikes\n      France\n      1\n    \n  \n\n34866 rows  8 columns\n\n\n\n\n\nCode\nclustering_ohe_df = ct.fit_transform(clustering_df)\n\nclustering_ohe_df = pd.DataFrame(clustering_ohe_df, columns = ct.get_feature_names_out())\n\n\n\n\nCode\nclustering_ohe_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Customer Gender_F\n      Customer Gender_M\n      Country_with_California_California\n      Country_with_California_France\n      Country_with_California_Germany\n      Country_with_California_United Kingdom\n      Country_with_California_United States\n      Product Category_Accessories\n      Product Category_Bikes\n      Product Category_Clothing\n      Customer Age\n      Cost\n      Revenue\n      net_profit\n    \n  \n  \n    \n      0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      29.0\n      80.0\n      109.0\n      29.0\n    \n    \n      1\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      29.0\n      49.0\n      57.0\n      8.0\n    \n    \n      2\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      29.0\n      11.0\n      15.0\n      4.0\n    \n    \n      3\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      29.0\n      175.0\n      233.0\n      58.0\n    \n    \n      4\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      29.0\n      105.0\n      125.0\n      20.0\n    \n  \n\n\n\n\n\n\nCode\nfrom sklearn.decomposition import PCA\n\nclustering_pca = PCA(2)\nclustering_ohe_pca_df = clustering_pca.fit_transform(clustering_ohe_df)\nclustering_ohe_pca_df = pd.DataFrame(clustering_ohe_pca_df, \n                                    columns = clustering_pca.get_feature_names_out())\n\n\n\n\nCode\nclustering_ohe_pca_df.head()\n\n\n\n\n\n\n  \n    \n      \n      pca0\n      pca1\n    \n  \n  \n    \n      0\n      -728.143905\n      -1.451126\n    \n    \n      1\n      -788.234011\n      -23.703172\n    \n    \n      2\n      -844.992284\n      -25.293388\n    \n    \n      3\n      -571.479188\n      24.971618\n    \n    \n      4\n      -699.844978\n      -14.146409\n    \n  \n\n\n\n\n\n\nCode\nclustering_ohe_pca_df['cluster'] = kproto_best_label\n\n\n\n\nCode\nsns.scatterplot(clustering_ohe_pca_df, x='pca0', y='pca1', hue = 'cluster')\n\n\n<AxesSubplot: xlabel='pca0', ylabel='pca1'>\n\n\n\n\n\n\n\nCode\nclustering_ohe_df['cluster'] = kproto_best_label\n\n\n\n\nCode\nclustering_ohe_df.groupby('cluster').mean()\n\n\n\n\n\n\n  \n    \n      \n      Customer Gender_F\n      Customer Gender_M\n      Country_with_California_California\n      Country_with_California_France\n      Country_with_California_Germany\n      Country_with_California_United Kingdom\n      Country_with_California_United States\n      Product Category_Accessories\n      Product Category_Bikes\n      Product Category_Clothing\n      Customer Age\n      Cost\n      Revenue\n      net_profit\n    \n    \n      cluster\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.560676\n      0.439324\n      0.338068\n      0.122899\n      0.107246\n      0.172367\n      0.259420\n      0.796522\n      0.063768\n      0.139710\n      47.984638\n      230.139227\n      261.949855\n      31.810628\n    \n    \n      1\n      0.485354\n      0.514646\n      0.340378\n      0.170560\n      0.021505\n      0.239896\n      0.227660\n      0.001483\n      0.952169\n      0.046348\n      37.150908\n      2166.809047\n      2041.329996\n      -125.479051\n    \n    \n      2\n      0.450244\n      0.549756\n      0.294088\n      0.161551\n      0.132099\n      0.189254\n      0.223008\n      0.771889\n      0.086170\n      0.141941\n      28.147263\n      205.310418\n      227.842458\n      22.532041\n    \n    \n      3\n      0.523350\n      0.476650\n      0.136548\n      0.186802\n      0.449239\n      0.127919\n      0.099492\n      0.100000\n      0.695431\n      0.204569\n      37.460406\n      1975.811168\n      2433.172081\n      457.360914\n    \n    \n      4\n      0.447162\n      0.552838\n      0.262883\n      0.138943\n      0.217873\n      0.186236\n      0.194064\n      0.570939\n      0.214123\n      0.214938\n      34.539465\n      839.621331\n      1012.600130\n      172.978800\n    \n  \n\n\n\n\n\n\nCode\nclustering_scaled_df\n\n\n\n\n\n\n  \n    \n      \n      Customer Age\n      Cost\n      Revenue\n      net_profit\n      Customer Gender\n      Product Category\n      Country_with_California\n      kproto_labels\n    \n  \n  \n    \n      0\n      -0.664363\n      -0.718337\n      -0.722022\n      -0.234603\n      F\n      Accessories\n      United States\n      2\n    \n    \n      1\n      -0.664363\n      -0.763232\n      -0.792612\n      -0.371968\n      F\n      Clothing\n      United States\n      2\n    \n    \n      2\n      -0.664363\n      -0.818265\n      -0.849628\n      -0.398132\n      F\n      Accessories\n      United States\n      2\n    \n    \n      3\n      -0.664363\n      -0.580753\n      -0.553690\n      -0.044909\n      F\n      Accessories\n      United States\n      2\n    \n    \n      4\n      -0.664363\n      -0.682130\n      -0.700301\n      -0.293473\n      F\n      Accessories\n      United States\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      34861\n      0.145518\n      2.133260\n      1.148634\n      -4.100438\n      M\n      Bikes\n      France\n      1\n    \n    \n      34862\n      0.145518\n      2.525734\n      1.805672\n      -2.707167\n      M\n      Bikes\n      France\n      1\n    \n    \n      34863\n      0.145518\n      2.133260\n      1.278956\n      -3.472485\n      M\n      Bikes\n      France\n      1\n    \n    \n      34864\n      0.145518\n      2.133260\n      1.413350\n      -2.824908\n      M\n      Bikes\n      France\n      1\n    \n    \n      34865\n      0.145518\n      2.525734\n      1.258593\n      -5.343261\n      M\n      Bikes\n      France\n      1\n    \n  \n\n34866 rows  8 columns\n\n\n\n\n\nCode\nplt.figure(figsize = (8, 15))\nclustering_ohe_df.groupby('cluster').mean().T.plot(kind = 'barh', figsize = (8, 15))\n\n\n<AxesSubplot: >\n\n\n<Figure size 800x1500 with 0 Axes>\n\n\n\n\n\n\n\nCode\ncluster_mean = clustering_ohe_df.groupby('cluster').mean()\n\ncluster_mean_scaler = MaxAbsScaler()\ncluster_mean_scaler = ColumnTransformer([('scaler', MaxAbsScaler(), ['Customer Age', 'Cost', 'Revenue', 'net_profit'])], \n                                        remainder='passthrough', \n                                        verbose_feature_names_out=False)\ncluster_mean_scaled = pd.DataFrame(cluster_mean_scaler.fit_transform(cluster_mean), \n                                    columns = cluster_mean_scaler.get_feature_names_out())\ncluster_mean_scaled\n\n\n\n\n\n\n  \n    \n      \n      Customer Age\n      Cost\n      Revenue\n      net_profit\n      Customer Gender_F\n      Customer Gender_M\n      Country_with_California_California\n      Country_with_California_France\n      Country_with_California_Germany\n      Country_with_California_United Kingdom\n      Country_with_California_United States\n      Product Category_Accessories\n      Product Category_Bikes\n      Product Category_Clothing\n    \n  \n  \n    \n      0\n      1.000000\n      0.106211\n      0.107658\n      0.069553\n      0.560676\n      0.439324\n      0.338068\n      0.122899\n      0.107246\n      0.172367\n      0.259420\n      0.796522\n      0.063768\n      0.139710\n    \n    \n      1\n      0.774225\n      1.000000\n      0.838958\n      -0.274355\n      0.485354\n      0.514646\n      0.340378\n      0.170560\n      0.021505\n      0.239896\n      0.227660\n      0.001483\n      0.952169\n      0.046348\n    \n    \n      2\n      0.586589\n      0.094752\n      0.093640\n      0.049265\n      0.450244\n      0.549756\n      0.294088\n      0.161551\n      0.132099\n      0.189254\n      0.223008\n      0.771889\n      0.086170\n      0.141941\n    \n    \n      3\n      0.780675\n      0.911853\n      1.000000\n      1.000000\n      0.523350\n      0.476650\n      0.136548\n      0.186802\n      0.449239\n      0.127919\n      0.099492\n      0.100000\n      0.695431\n      0.204569\n    \n    \n      4\n      0.719803\n      0.387492\n      0.416165\n      0.378211\n      0.447162\n      0.552838\n      0.262883\n      0.138943\n      0.217873\n      0.186236\n      0.194064\n      0.570939\n      0.214123\n      0.214938\n    \n  \n\n\n\n\n\n\nCode\ncluster_mean_scaled.T.plot(kind='barh', figsize = (8, 15))\n\n\n<AxesSubplot: >\n\n\n\n\n\nThe most interesting result from above is the cluster with negative net_profit. From the mean of the analysis, we can see tha the sales were contributed from bike sales, which this cluster also almost contains no sales from Germany.\nNot only that, cluster with really high percent of sales from Germany (cluster 3) has a much higher net_profit, comparing to other clusters.It suggested that bike sales in Germany generates profit, however, bike sales in other countries actually results in loss in profit.\n\n\nFurther investigation is needed to determine the relation between the types of bikes being sold in Germany, and the profit generated."
  },
  {
    "objectID": "posts/health_backup/index.html",
    "href": "posts/health_backup/index.html",
    "title": "Health data analysis",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/music_backup/index.html",
    "href": "posts/music_backup/index.html",
    "title": "Music transcription",
    "section": "",
    "text": "Music transcription\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nMusic transcription - MusicNet LSTM\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nOrchideaSOL CNN\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nOrchideaSOL RNN\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nPreprocessing\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/health_backup/Open_Health_Data_Analysis/Fitbit/Fitbit_to_json.html",
    "href": "posts/health_backup/Open_Health_Data_Analysis/Fitbit/Fitbit_to_json.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import pandas as pd\nimport glob\nimport time\n\nstart_time = time.time()\n\n\n\n\ndef get_json_to_df(file_list = []):\n    df_list = []\n    for json_file in file_list[1]:\n        df_list.append(pd.read_json(json_file))\n    df = pd.concat(df_list)\n    return df\n\ndef merge_dataframes(df1,df2):\n    merged = pd.merge(df1, df2,how='outer', on='dateTime')\n    return merged\n\ndef merged_to_datetime(merged):\n    merged['dateTime'] = pd.to_datetime(merged['dateTime'], format='%m/%d/%y %H:%M:%S')\n    merged = merged.sort_values(by='dateTime')\n    return merged\n\ndef make_new_df_value(x='',column_name=''):\n    try:\n        x = x[column_name]\n    except Exception as e:\n        print(e)\n        x = 0.0\n    return x\n    \n\n\n\n## Creating lists of all the respective files in the directory\nheart_rate_file_list = glob.glob('data/Fitbit_data_export/user-site-export/heart_rate-*')\nsteps_file_list = glob.glob('data/Fitbit_data_export/user-site-export/steps-*')\naltitude_file_list = glob.glob('data/Fitbit_data_export/user-site-export/altitude-*')\ncalories_file_list = glob.glob('data/Fitbit_data_export/user-site-export/calories-*')\n\n\nmaking_file_lists = time.time()\nprint(\"###Time taken to make files lists\",making_file_lists-start_time)\n\n###Time taken to make files lists 17.79209280014038\n\n\n\ndisplay(heart_rate_file_list)\n\n['data/Fitbit_data_export/user-site-export/heart_rate-2018-12-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-11.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-16.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-02-01.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-06.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-17.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-07.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-10.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-04.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-19.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-29.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-30.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-05-20.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-03.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-06-26.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-09-15.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2019-01-31.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-12.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-07-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-10-09.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-14.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-11-24.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-27.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-10-23.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-13.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-12-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-08-28.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-02-08.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-11-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-03-25.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-01-18.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-09-05.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-08-02.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2017-12-22.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-04-21.json',\n 'data/Fitbit_data_export/user-site-export/heart_rate-2018-07-31.json']\n\n\n\n## reading json into dataframes\nheart_rate_df = get_json_to_df(file_list = heart_rate_file_list).reset_index()\n## Heart rate contains a sub json that are explicitly converted into column\nheart_rate_df['bpm'] = heart_rate_df['value'].transform(lambda x: make_new_df_value(x,'bpm'))\nheart_rate_df['confidence'] = heart_rate_df['value'].transform(lambda x: make_new_df_value(x,'confidence'))\nheart_rate_df = heart_rate_df.drop(['value','index'],axis=1)\n\n\nsteps_df = get_json_to_df(file_list = steps_file_list).rename(columns={'value': 'steps'})\n\naltitude_df = get_json_to_df(file_list = altitude_file_list).rename(columns={'value': 'altitude'})\ncalories_df = get_json_to_df(file_list = calories_file_list).rename(columns={'value': 'calories'})\n\nload_json_time = time.time()\nprint(\"###Time taken to load and transform json files\",load_json_time-making_file_lists)\n\n\n\nprint('## Time taken',function_time) \n\n\nmerged = merge_dataframes(heart_rate_df,steps_df)\nmerged = merge_dataframes(merged,altitude_df)\nmerged = merge_dataframes(merged,calories_df)\nmerged = merged_to_datetime(merged)\n\nmerged.to_csv('merged_export_data.csv')\n\nmerging_time = time.time()\nprint(\"###Time taken to merge dataframess\",merging_time-load_json_time)\n\n\n\nfunction_time = time.time() - start_time"
  },
  {
    "objectID": "posts/health_backup/Open_Health_Data_Analysis/Fitbit/data_analysis.html",
    "href": "posts/health_backup/Open_Health_Data_Analysis/Fitbit/data_analysis.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "!pip install requests_oauthlib\n!pip install cherrypy\n!pip inastall pandas_profiling\n\n\nRequirement already satisfied: requests_oauthlib in /Users/christopher/anaconda/lib/python3.6/site-packages\nRequirement already satisfied: oauthlib>=0.6.2 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests_oauthlib)\nRequirement already satisfied: requests>=2.0.0 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests_oauthlib)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib)\nRequirement already satisfied: idna<2.6,>=2.5 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib)\nRequirement already satisfied: urllib3<1.22,>=1.21.1 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/christopher/anaconda/lib/python3.6/site-packages (from requests>=2.0.0->requests_oauthlib)\nYou are using pip version 9.0.1, however version 19.0.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nRequirement already satisfied: cherrypy in /Users/christopher/anaconda/lib/python3.6/site-packages\nYou are using pip version 9.0.1, however version 19.0.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nERROR: unknown command \"inastall\" - maybe you meant \"install\"\n\n\n\nimport pandas as pd\nimport numpy as np\nimport pandas_profiling\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n\n%%html\n<style>\n.output_wrapper, .output {\n    height:auto !important;\n    max-height:30000px; \n}\n.output_scroll {}\n    box-shadow:none !important;\n    webkit-box-shadow:none !important;\n}\n</style>\n\n\n\n\n\ntry:\n    df = pd.read_csv('merged_hr_sleep_steps.csv')\nexcept Exception as e:\n    print('Failed reading file, does it exists?',e)\n    print('Running data creation script..')\n    from get_fitbit_steps_and_hr_data import *\n    df = pd.read_csv('merged_hr_sleep_steps.csv')\n\n\ndisplay(df.describe())\ndisplay(df.tail(10))\n\n\n\n\n\n  \n    \n      \n      heart_rate\n      steps\n      sleep_stage\n    \n  \n  \n    \n      count\n      6815505.00000\n      6834023.00000\n      6834023.00000\n    \n    \n      mean\n      70.73566\n      0.82106\n      0.38562\n    \n    \n      std\n      17.09187\n      7.67283\n      0.53980\n    \n    \n      min\n      36.00000\n      0.00000\n      0.00000\n    \n    \n      25%\n      59.00000\n      0.00000\n      0.00000\n    \n    \n      50%\n      66.00000\n      0.00000\n      0.00000\n    \n    \n      75%\n      76.00000\n      0.00000\n      1.00000\n    \n    \n      max\n      203.00000\n      187.00000\n      3.00000\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      timestamp\n      heart_rate\n      steps\n      sleep_stage\n    \n  \n  \n    \n      6834013\n      2019-01-30 23:50:00\n      nan\n      0.00000\n      1.00000\n    \n    \n      6834014\n      2019-01-30 23:51:00\n      nan\n      0.00000\n      1.00000\n    \n    \n      6834015\n      2019-01-30 23:52:00\n      nan\n      0.00000\n      2.00000\n    \n    \n      6834016\n      2019-01-30 23:53:00\n      nan\n      0.00000\n      2.00000\n    \n    \n      6834017\n      2019-01-30 23:54:00\n      nan\n      0.00000\n      2.00000\n    \n    \n      6834018\n      2019-01-30 23:55:00\n      nan\n      0.00000\n      2.00000\n    \n    \n      6834019\n      2019-01-30 23:56:00\n      nan\n      0.00000\n      1.00000\n    \n    \n      6834020\n      2019-01-30 23:57:00\n      nan\n      0.00000\n      1.00000\n    \n    \n      6834021\n      2019-01-30 23:58:00\n      nan\n      0.00000\n      1.00000\n    \n    \n      6834022\n      2019-01-30 23:59:00\n      nan\n      0.00000\n      1.00000\n    \n  \n\n\n\n\n\npandas_profiling.ProfileReport(df)\n\n\n\n\n\n\n\n    \n        Overview\n    \n    \n    \n        Dataset info\n        \n            \n            \n                Number of variables\n                4 \n            \n            \n                Number of observations\n                6834023 \n            \n            \n                Total Missing (%)\n                0.1% \n            \n            \n                Total size in memory\n                208.6 MiB \n            \n            \n                Average record size in memory\n                32.0 B \n            \n            \n        \n    \n    \n        Variables types\n        \n            \n            \n                Numeric\n                3 \n            \n            \n                Categorical\n                0 \n            \n            \n                Boolean\n                0 \n            \n            \n                Date\n                0 \n            \n            \n                Text (Unique)\n                1 \n            \n            \n                Rejected\n                0 \n            \n            \n                Unsupported\n                0 \n            \n            \n        \n    \n    \n        \n        Warnings\n        sleep_stage has 4378621 / 64.1% zeros Zerossteps has 6691184 / 97.9% zeros Zeros \n    \n\n    \n        Variables\n    \n    \n    \n        heart_rate\n            Numeric\n        \n    \n    \n        \n            \n                \n                    Distinct count\n                    274\n                \n                \n                    Unique (%)\n                    0.0%\n                \n                \n                    Missing (%)\n                    0.3%\n                \n                \n                    Missing (n)\n                    18518\n                \n                \n                    Infinite (%)\n                    0.0%\n                \n                \n                    Infinite (n)\n                    0\n                \n            \n\n        \n        \n            \n\n                \n                    Mean\n                    70.736\n                \n                \n                    Minimum\n                    36\n                \n                \n                    Maximum\n                    203\n                \n                \n                    Zeros (%)\n                    0.0%\n                \n            \n        \n    \n\n\n    \n\n\n\n    \n        Toggle details\n    \n\n\n    \n        Statistics\n        Histogram\n        Common Values\n        Extreme Values\n\n    \n\n    \n        \n            \n                Quantile statistics\n                \n                    \n                        Minimum\n                        36\n                    \n                    \n                        5-th percentile\n                        54\n                    \n                    \n                        Q1\n                        59\n                    \n                    \n                        Median\n                        66\n                    \n                    \n                        Q3\n                        76\n                    \n                    \n                        95-th percentile\n                        109\n                    \n                    \n                        Maximum\n                        203\n                    \n                    \n                        Range\n                        167\n                    \n                    \n                        Interquartile range\n                        17\n                    \n                \n            \n            \n                Descriptive statistics\n                \n                    \n                        Standard deviation\n                        17.092\n                    \n                    \n                        Coef of variation\n                        0.24163\n                    \n                    \n                        Kurtosis\n                        2.7405\n                    \n                    \n                        Mean\n                        70.736\n                    \n                    \n                        MAD\n                        12.664\n                    \n                    \n                        Skewness\n                        1.6186\n                    \n                    \n                        Sum\n                        482100000\n                    \n                    \n                        Variance\n                        292.13\n                    \n                    \n                        Memory size\n                        52.1 MiB\n                    \n                \n            \n        \n        \n            \n        \n        \n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        58.0\n        281839\n        4.1%\n        \n            \n        \n\n        57.0\n        278298\n        4.1%\n        \n            \n        \n\n        59.0\n        277774\n        4.1%\n        \n            \n        \n\n        60.0\n        263826\n        3.9%\n        \n            \n        \n\n        56.0\n        259650\n        3.8%\n        \n            \n        \n\n        61.0\n        254255\n        3.7%\n        \n            \n        \n\n        62.0\n        249826\n        3.7%\n        \n            \n        \n\n        63.0\n        247628\n        3.6%\n        \n            \n        \n\n        64.0\n        238302\n        3.5%\n        \n            \n        \n\n        55.0\n        231245\n        3.4%\n        \n            \n        \n\n        Other values (263)\n        4232862\n        61.9%\n        \n            \n        \n\n\n        \n        \n            Minimum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        36.0\n        8\n        0.0%\n        \n            \n        \n\n        37.0\n        87\n        0.0%\n        \n            \n        \n\n        37.5\n        2\n        0.0%\n        \n            \n        \n\n        38.0\n        203\n        0.0%\n        \n            \n        \n\n        38.5\n        1\n        0.0%\n        \n            \n        \n\n\n            Maximum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        196.0\n        3\n        0.0%\n        \n            \n        \n\n        197.0\n        1\n        0.0%\n        \n            \n        \n\n        201.0\n        2\n        0.0%\n        \n            \n        \n\n        202.0\n        1\n        0.0%\n        \n            \n        \n\n        203.0\n        1\n        0.0%\n        \n            \n        \n\n\n        \n    \n\n\n    \n        sleep_stage\n            Numeric\n        \n    \n    \n        \n            \n                \n                    Distinct count\n                    6\n                \n                \n                    Unique (%)\n                    0.0%\n                \n                \n                    Missing (%)\n                    0.0%\n                \n                \n                    Missing (n)\n                    0\n                \n                \n                    Infinite (%)\n                    0.0%\n                \n                \n                    Infinite (n)\n                    0\n                \n            \n\n        \n        \n            \n\n                \n                    Mean\n                    0.38562\n                \n                \n                    Minimum\n                    0\n                \n                \n                    Maximum\n                    3\n                \n                \n                    Zeros (%)\n                    64.1%\n                \n            \n        \n    \n\n\n    \n\n\n\n    \n        Toggle details\n    \n\n\n    \n        Statistics\n        Histogram\n        Common Values\n        Extreme Values\n\n    \n\n    \n        \n            \n                Quantile statistics\n                \n                    \n                        Minimum\n                        0\n                    \n                    \n                        5-th percentile\n                        0\n                    \n                    \n                        Q1\n                        0\n                    \n                    \n                        Median\n                        0\n                    \n                    \n                        Q3\n                        1\n                    \n                    \n                        95-th percentile\n                        1\n                    \n                    \n                        Maximum\n                        3\n                    \n                    \n                        Range\n                        3\n                    \n                    \n                        Interquartile range\n                        1\n                    \n                \n            \n            \n                Descriptive statistics\n                \n                    \n                        Standard deviation\n                        0.5398\n                    \n                    \n                        Coef of variation\n                        1.3998\n                    \n                    \n                        Kurtosis\n                        0.33213\n                    \n                    \n                        Mean\n                        0.38562\n                    \n                    \n                        MAD\n                        0.49415\n                    \n                    \n                        Skewness\n                        1.0307\n                    \n                    \n                        Sum\n                        2635400\n                    \n                    \n                        Variance\n                        0.29139\n                    \n                    \n                        Memory size\n                        52.1 MiB\n                    \n                \n            \n        \n        \n            \n        \n        \n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        0.0\n        4378621\n        64.1%\n        \n            \n        \n\n        1.0\n        2266369\n        33.2%\n        \n            \n        \n\n        2.0\n        134154\n        2.0%\n        \n            \n        \n\n        1.5\n        41627\n        0.6%\n        \n            \n        \n\n        3.0\n        10231\n        0.1%\n        \n            \n        \n\n        2.5\n        3021\n        0.0%\n        \n            \n        \n\n\n        \n        \n            Minimum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        0.0\n        4378621\n        64.1%\n        \n            \n        \n\n        1.0\n        2266369\n        33.2%\n        \n            \n        \n\n        1.5\n        41627\n        0.6%\n        \n            \n        \n\n        2.0\n        134154\n        2.0%\n        \n            \n        \n\n        2.5\n        3021\n        0.0%\n        \n            \n        \n\n\n            Maximum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        1.0\n        2266369\n        33.2%\n        \n            \n        \n\n        1.5\n        41627\n        0.6%\n        \n            \n        \n\n        2.0\n        134154\n        2.0%\n        \n            \n        \n\n        2.5\n        3021\n        0.0%\n        \n            \n        \n\n        3.0\n        10231\n        0.1%\n        \n            \n        \n\n\n        \n    \n\n\n    \n        steps\n            Numeric\n        \n    \n    \n        \n            \n                \n                    Distinct count\n                    172\n                \n                \n                    Unique (%)\n                    0.0%\n                \n                \n                    Missing (%)\n                    0.0%\n                \n                \n                    Missing (n)\n                    0\n                \n                \n                    Infinite (%)\n                    0.0%\n                \n                \n                    Infinite (n)\n                    0\n                \n            \n\n        \n        \n            \n\n                \n                    Mean\n                    0.82106\n                \n                \n                    Minimum\n                    0\n                \n                \n                    Maximum\n                    187\n                \n                \n                    Zeros (%)\n                    97.9%\n                \n            \n        \n    \n\n\n    \n\n\n\n    \n        Toggle details\n    \n\n\n    \n        Statistics\n        Histogram\n        Common Values\n        Extreme Values\n\n    \n\n    \n        \n            \n                Quantile statistics\n                \n                    \n                        Minimum\n                        0\n                    \n                    \n                        5-th percentile\n                        0\n                    \n                    \n                        Q1\n                        0\n                    \n                    \n                        Median\n                        0\n                    \n                    \n                        Q3\n                        0\n                    \n                    \n                        95-th percentile\n                        0\n                    \n                    \n                        Maximum\n                        187\n                    \n                    \n                        Range\n                        187\n                    \n                    \n                        Interquartile range\n                        0\n                    \n                \n            \n            \n                Descriptive statistics\n                \n                    \n                        Standard deviation\n                        7.6728\n                    \n                    \n                        Coef of variation\n                        9.3451\n                    \n                    \n                        Kurtosis\n                        155.97\n                    \n                    \n                        Mean\n                        0.82106\n                    \n                    \n                        MAD\n                        1.6078\n                    \n                    \n                        Skewness\n                        11.93\n                    \n                    \n                        Sum\n                        5611100\n                    \n                    \n                        Variance\n                        58.872\n                    \n                    \n                        Memory size\n                        52.1 MiB\n                    \n                \n            \n        \n        \n            \n        \n        \n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        0.0\n        6691184\n        97.9%\n        \n            \n        \n\n        7.0\n        6279\n        0.1%\n        \n            \n        \n\n        8.0\n        5984\n        0.1%\n        \n            \n        \n\n        9.0\n        5058\n        0.1%\n        \n            \n        \n\n        6.0\n        5050\n        0.1%\n        \n            \n        \n\n        10.0\n        4297\n        0.1%\n        \n            \n        \n\n        11.0\n        3783\n        0.1%\n        \n            \n        \n\n        12.0\n        3430\n        0.1%\n        \n            \n        \n\n        4.0\n        3308\n        0.0%\n        \n            \n        \n\n        13.0\n        3144\n        0.0%\n        \n            \n        \n\n        Other values (162)\n        102506\n        1.5%\n        \n            \n        \n\n\n        \n        \n            Minimum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        0.0\n        6691184\n        97.9%\n        \n            \n        \n\n        1.0\n        651\n        0.0%\n        \n            \n        \n\n        2.0\n        483\n        0.0%\n        \n            \n        \n\n        3.0\n        373\n        0.0%\n        \n            \n        \n\n        4.0\n        3308\n        0.0%\n        \n            \n        \n\n\n            Maximum 5 values\n            \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        168.0\n        1\n        0.0%\n        \n            \n        \n\n        173.0\n        1\n        0.0%\n        \n            \n        \n\n        176.0\n        1\n        0.0%\n        \n            \n        \n\n        178.0\n        1\n        0.0%\n        \n            \n        \n\n        187.0\n        1\n        0.0%\n        \n            \n        \n\n\n        \n    \n\n\n    \n        timestamp\n            Categorical, Unique\n        \n    \n  \n    \n      First 3 values\n    \n  \n  \n    \n      2018-05-14 22:43:00\n    \n    \n      2018-02-03 12:59:49\n    \n    \n      2017-11-23 17:53:42\n    \n  \n\n\n  \n    \n      Last 3 values\n    \n  \n  \n    \n      2018-09-19 06:13:13\n    \n    \n      2018-08-29 20:26:45\n    \n    \n      2017-12-11 03:22:00\n    \n  \n\n\n    \n        Toggle details\n    \n\n\n    First 10 values\n    \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        2017-07-12 00:00:00\n        1\n        0.0%\n        \n            \n        \n\n        2017-07-12 00:01:00\n        1\n        0.0%\n        \n            \n        \n\n        2017-07-12 00:02:00\n        1\n        0.0%\n        \n            \n        \n\n        2017-07-12 00:03:00\n        1\n        0.0%\n        \n            \n        \n\n        2017-07-12 00:04:00\n        1\n        0.0%\n        \n            \n        \n\n\n    Last 10 values\n    \n\n    \n    \n        Value\n        Count\n        Frequency (%)\n        \n    \n    \n    \n        2019-01-30 23:55:00\n        1\n        0.0%\n        \n            \n        \n\n        2019-01-30 23:56:00\n        1\n        0.0%\n        \n            \n        \n\n        2019-01-30 23:57:00\n        1\n        0.0%\n        \n            \n        \n\n        2019-01-30 23:58:00\n        1\n        0.0%\n        \n            \n        \n\n        2019-01-30 23:59:00\n        1\n        0.0%\n        \n            \n        \n\n\n\n\n    \n        Correlations\n    \n    \n    \n    \n\n    \n        Sample\n    \n    \n    \n        \n  \n    \n      \n      timestamp\n      heart_rate\n      steps\n      sleep_stage\n    \n  \n  \n    \n      0\n      2017-07-12 00:00:00\n      nan\n      0.00000\n      0.00000\n    \n    \n      1\n      2017-07-12 00:01:00\n      nan\n      0.00000\n      0.00000\n    \n    \n      2\n      2017-07-12 00:02:00\n      nan\n      0.00000\n      0.00000\n    \n    \n      3\n      2017-07-12 00:03:00\n      nan\n      0.00000\n      0.00000\n    \n    \n      4\n      2017-07-12 00:04:00\n      nan\n      0.00000\n      0.00000"
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/EDA.html",
    "href": "posts/music/Music_trainscription_fastai/notebooks/EDA.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\n\nfrom spectrogram_class import spectrogram"
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/EDA.html#meta-data-orchideasol",
    "href": "posts/music/Music_trainscription_fastai/notebooks/EDA.html#meta-data-orchideasol",
    "title": "Most interesting blog on Earth!",
    "section": "Meta data (OrchideaSOL)",
    "text": "Meta data (OrchideaSOL)\nThe main dataset, other than the raw audio files, we are going to explore is the metadata dataframe. Lets first explore the Orchetral dataset.\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\n\nmeta_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13265 entries, 0 to 13264\nData columns (total 17 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   Path                         13265 non-null  object \n 1   Family (abbr.)               13265 non-null  object \n 2   Family (in full)             13265 non-null  object \n 3   Instrument (abbr.)           13265 non-null  object \n 4   Instrument (in full)         13265 non-null  object \n 5   Technique (abbr.)            13265 non-null  object \n 6   Technique (in full)          13265 non-null  object \n 7   Pitch                        13265 non-null  object \n 8   Pitch ID (if applicable)     13162 non-null  float64\n 9   Dynamics                     13265 non-null  object \n 10  Dynamics ID (if applicable)  12646 non-null  float64\n 11  Instance ID                  13262 non-null  float64\n 12  Mute (abbr.)                 13265 non-null  object \n 13  Mute (in full)               13265 non-null  object \n 14  String ID (if applicable)    7516 non-null   float64\n 15  Needed digital retuning      13265 non-null  bool   \n 16  Fold                         13265 non-null  int64  \ndtypes: bool(1), float64(4), int64(1), object(11)\nmemory usage: 1.6+ MB\n\n\n\nmeta_df.nunique().sort_values()\n\nNeeded digital retuning            2\nFold                               5\nFamily (abbr.)                     5\nFamily (in full)                   5\nDynamics ID (if applicable)        5\nString ID (if applicable)          6\nDynamics                           7\nMute (abbr.)                       7\nMute (in full)                     7\nInstance ID                       13\nInstrument (abbr.)                16\nInstrument (in full)              16\nTechnique (in full)               52\nTechnique (abbr.)                 56\nPitch ID (if applicable)          90\nPitch                            105\nPath                           13265\ndtype: int64\n\n\nThe Series above represents the number of unique values across all features. From the object types, unique values and intuition, we can conclude that all bu the path to the audio files are categorical features.\n\n# Check for null values\nmeta_df.isna().sum()\n\nPath                              0\nFamily (abbr.)                    0\nFamily (in full)                  0\nInstrument (abbr.)                0\nInstrument (in full)              0\nTechnique (abbr.)                 0\nTechnique (in full)               0\nPitch                             0\nPitch ID (if applicable)        103\nDynamics                          0\nDynamics ID (if applicable)     619\nInstance ID                       3\nMute (abbr.)                      0\nMute (in full)                    0\nString ID (if applicable)      5749\nNeeded digital retuning           0\nFold                              0\ndtype: int64\n\n\nThe only columns we are interested in are the Path, Instrument and Pitch. We can see that out of all three, only Pitch ID contains numm values. Now lets take a look at what the null values are, represented in pitch.\n\nmeta_df['Pitch'].unique()\n\narray(['A#0', 'A#1', 'A#2', 'A#3', 'A#4', 'A0', 'A1', 'A2', 'A3', 'A4',\n       'B0', 'B1', 'B2', 'B3', 'C#1', 'C#2', 'C#3', 'C#4', 'C1', 'C2',\n       'C3', 'C4', 'D#1', 'D#2', 'D#3', 'D#4', 'D1', 'D2', 'D3', 'D4',\n       'E1', 'E2', 'E3', 'E4', 'F#1', 'F#2', 'F#3', 'F#4', 'F1', 'F2',\n       'F3', 'F4', 'G#0', 'G#1', 'G#2', 'G#3', 'G#4', 'G1', 'G2', 'G3',\n       'G4', 'N', 'C#1_D#1', 'C1_C#1', 'C1_G1', 'D1_D#1', 'D1_F1', 'B4',\n       'C#5', 'C5', 'D#5', 'D5', 'E5', 'F5', 'A#5', 'A5', 'B5', 'F#5',\n       'G#5', 'G5', 'C#6', 'C6', 'D#6', 'D6', 'E6', 'F#6', 'F6', 'G6',\n       'A#6', 'A#7', 'A6', 'A7', 'B6', 'B7', 'C#7', 'C#8', 'C7', 'C8',\n       'D#7', 'D7', 'E7', 'F#7', 'F7', 'G#6', 'G#7', 'G7', 'C#3_B5',\n       'C#3_C#4', 'C#3_C#5', 'C#3_C#6', 'C#3_D#6', 'C#3_F5', 'C#3_F6',\n       'C#3_G#4', 'C#3_G#5'], dtype=object)\n\n\n\n# Getting the unique values of Pitch where the Pitch ID are missing.\nmeta_df[['Pitch', 'Pitch ID (if applicable)']] \\\n            [meta_df['Pitch ID (if applicable)'].isnull()]['Pitch'].unique()\n\narray(['N', 'C#1_D#1', 'C1_C#1', 'C1_G1', 'D1_D#1', 'D1_F1', 'C#3_B5',\n       'C#3_C#4', 'C#3_C#5', 'C#3_C#6', 'C#3_D#6', 'C#3_F5', 'C#3_F6',\n       'C#3_G#4', 'C#3_G#5'], dtype=object)\n\n\nIf you are not familiar with music, let me give you some insight (I am either grade 5 or 6 on piano ABRSM, cant remember which since I lost my certificate):\n\n\nNone of the terms make sense to me!\n\n\nThere is no note N in music, nor can you be C1 and C# at the same time! The documentation on OrchideaSOL doesnt include any useful information on the subject either. The only logical way to deal with the null values is to simple drop them.\nTo justify the decision, lets look at the precentage of the missing values.\n\n# Getting the fractiong of missing values in the Pitch ID feature.\nmeta_df['Pitch ID (if applicable)'].isnull().sum()/meta_df.shape[0]\n\n0.007764794572182435\n\n\nIts not even 1% of our data, we dont lose much training information from losing these unexplanable samples notes."
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/EDA.html#orchideasol-instrument-analysis",
    "href": "posts/music/Music_trainscription_fastai/notebooks/EDA.html#orchideasol-instrument-analysis",
    "title": "Most interesting blog on Earth!",
    "section": "OrchideaSOL instrument analysis",
    "text": "OrchideaSOL instrument analysis\nNow that we have an understading of the note missing values, lets have a look at the instruments distribution.\n\norchidea_instrument = meta_df['Instrument (in full)'].value_counts(normalize=True)\norchidea_instrument\n\nViolin            0.149793\nViola             0.147154\nContrabass        0.123332\nCello             0.120090\nAccordion         0.065737\nTrombone          0.050509\nTrumpet in C      0.044478\nFrench Horn       0.044403\nFlute             0.039879\nHarp              0.038221\nBass Tuba         0.037693\nClarinet in Bb    0.030607\nAlto Saxophone    0.028421\nBassoon           0.026988\nGuitar            0.026611\nOboe              0.026084\nName: Instrument (in full), dtype: float64\n\n\n\nplt.figure(figsize = (18, 8))\nplt.bar(orchidea_instrument.index, orchidea_instrument.values)\nplt.title('Instrument distrubution of OrchideaSOL')\nplt.xlabel('Instruments')\nplt.ylabel('Fractions')\nplt.show()\n\n\n\n\nWe can see that violin, viola, contrabass and cello has more entries comparing to the rest of the instruments.\nSince we have some unbalanced data, we can either comtemplate the bias through adjusting the loss function (increasing the penalty for misclassifying minor classes), or creating more data for the minorities through audio file augmentation, more on that in the other notebooks!"
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/EDA.html#orchideasol-instrumentnotes-analysis",
    "href": "posts/music/Music_trainscription_fastai/notebooks/EDA.html#orchideasol-instrumentnotes-analysis",
    "title": "Most interesting blog on Earth!",
    "section": "OrchideaSOL instrument/notes analysis",
    "text": "OrchideaSOL instrument/notes analysis\nNow that we have a sense of how the instruments distribution looks like, it is also useful to look at the distribution of the instruments corresponding to the notes.\n\n# Creating crosstab for instrument and Pitch ID, \n# which represents the value counts for all instrument/Pitch ID combinations\n\nplt.figure(figsize = (12, 10))\nsns.heatmap(pd.crosstab(meta_df['Instrument (in full)'], \n                                meta_df['Pitch ID (if applicable)']))\n\n<AxesSubplot:xlabel='Pitch ID (if applicable)', ylabel='Instrument (in full)'>\n\n\n\n\n\nWe can see that the majority of notes for viola and violin distributed in the middle of the note range. Wheareas Cello and Contrabass has distribution shift towards the lower ends of the range.\nIt is also useful to note that: * Accoridian, guitar and harp has lower number of note counts, * However, their notes are more evenly distributed across the note range."
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/classic_transcription.html",
    "href": "posts/music/Music_trainscription_fastai/notebooks/classic_transcription.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import librosa\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom keras.layers import LSTM, ConvLSTM1D, Input, MaxPool1D\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\nfrom keras.layers.core import Dense, Dropout, Flatten\nfrom keras.models import Model, Sequential\nfrom librosa import display\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom classic_generator import classic_generator\nfrom spectrogram_class import spectrogram\n\nModuleNotFoundError: No module named 'classic_generator'\n\n\n\n!pwd\n\n/Users/ylee/Downloads\n\n\n\nimport pickle\n\ndef save_history(history, path):\n    with open(path, 'wb+') as f:\n        pickle.dump(history, f)\n        \ndef load_history(path):\n    with open(path, 'rb') as f:\n        return pickle.load(f)\n\n\nMusic transcription - MusicNet LSTM\nThis is a successive notebook from RNN models for OrchideaSOL dataset. In that notebook, we have demonstrate that the RNN model, in particularly LSTM, is able to capture the features of instrument and pitch in a raw audio file.\nThe purpose of this notebook is to apply the same principle on the more complicated MusicNet dataset. On our previous notebook, we have had mild success of predicting more than 50% in both insturment and notes, for short single instrumental audio files.\nAlthough it is the same principle, this time we are going to need to convert the whole sequence of audio file into the transcription in to the same music file length. To be more clear:\n\nFor all timesteps in the spectrogram, we are going to produce the classification of instruments and note.\n\nThe end goal of this project is to convert the corresponding output into a MIDI file.\nThe model architecture will be to be fine tuned since we are facing the challenges below:\n\nWe are making classifying prediction for every timestep,\n\nExcluding the expected dimension of time and notes as output, we also need the same classification for every instruments, which is 11 of them.\nThe audio files are not neccessarily made of single instrument, which means that our RNN model will need to find the relation of the sound signature for each instruments, in the sea of spectrograms magnitude.\nThe audio files has different length, range from 3 minutes to 20 minutes. Padding will be required, however, zeros padding will cause problem of exploding/vanishing gradient in RNN model.\n\n\n# Testing if GPU is enabled, if it's not, I am not running this notebook!\ntf.config.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\nimport tensorflow as tf\ntf.test.is_built_with_cuda()\ntf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n\nWARNING:tensorflow:From /var/folders/n9/gg0_718d4db65yyqhxlk09vh0000gp/T/ipykernel_38191/1822807733.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\nMetal device set to: Apple M1\n\nsystemMemory: 8.00 GB\nmaxCacheSize: 2.67 GB\n\n\n\n2022-09-16 16:43:02.967559: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-16 16:43:02.968025: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\nTrue\n\n\n\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n\n[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 3715140090454835554\nxla_global_id: -1\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nlocality {\n  bus_id: 1\n}\nincarnation: 11023430109333998903\nphysical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\nxla_global_id: -1\n]\n\n\n2022-09-16 16:43:03.107869: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-16 16:43:03.107888: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\nThroughout this notebook, we will be using integer code number for instrument and notes classification. The instrument and note lists below are generated by concatenating the training labels, which are seperated csv files.\n\ninstrument_list = [1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\nnote_list = [21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62,\n             63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]\n\n\n\nRNN-LSTM\nTo begin our construction of RNN model, we introduce the architect idea for our model.\nSince our model has to output the MIDI file equivalence for each instruments, we will be forking our output into 11 Dense layers, with each layer reprenting one instruments.\nEach output will have a dimension of : \\[\n\\text{\\# of instruments} \\times (\\text{\\# of timesteps} \\times \\text{\\# of notes})\n\\]\nThe architect idea for the model is as follow: 1. Passing input spectrogrom into convolutional layers, in order to extract features 1. Passing features in to multiple layers of Dense layers, to embed the features 1. Passing embedded features into LSTM model, to predict labels for each timestep depending on the sequnce of embedded features.\n\nUPDATE:\nAfter countless time wasted on differnet architect and changing parameter, we have decided to change the input shape, instead of feeding the batch of spectrogram which are padded with the longest time dimension across the batch, we will be taken random snipshot of fixed time length across all batch. This workaround is to prevent:\n* Long padding of zeros for shorter audio files, to prevent vanishing gradient * Long training time and * Reduce GPU memory usage (VsCode tends to close unexpectedly and doesnt release allocated GPU space before closing, making a full system restart required.)\nWe have decided to take 200 timeslices for the model input, with our default hop length (number of timestep to skip when computing fast-fourier transform for spectrogram), and using the sample rate of audio, which is 44100HZ, we can calculate that:\n\\[\n\\text{Length in second of inputs} = \\frac{300 \\times 200}{44100} \\\\\n\\approx 16 \\, \\text{seconds}\n\\]\n\ndef instrument_layer_simple(input, out_name):\n    '''\n    Support function from building multiple output models. Returns an output layer\n    of TimeDistributed Dense layer, corresponds to the number of notes in labels\n    \n    Input:\n    input: Preceeding input layer to be fed into.\n    out_name: str, name of output layer names to be assigned\n    '''\n    out = layers.TimeDistributed(Dense(len(note_list), activation='sigmoid'),\n                                 name=out_name)(input)\n    return out\n\n\ninstrument_list\n\n[1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\nNow we can start to build our (hopefully will be working this time) model.\n\nBATCH_SIZE = 8\n\n# We have choose to work with 128 frequency bins,\n# and the None input shape indicates the dynamic length of time dimension\ninp = Input((None, 128, 1), batch_size=BATCH_SIZE)\nnormalizer = layers.BatchNormalization()(inp)\n\n# Extracting features from first convolutional layers.\n# Padding set to same to maintain same time dimension\nconv_1 = Conv2D(10, (50, 40), padding = 'same')(normalizer)\n\nnormalizer_2 = layers.BatchNormalization()(conv_1)\n# pool_1 = layers.TimeDistributed(MaxPool1D(2))(normalizer_2)\n# flatten_1 = flatten = layers.TimeDistributed(layers.Flatten())(pool_1)\n\n# conv_2 = layers.Conv2D(5, (1000, 30), padding = 'same')(pool_1)\n# pool_2 = layers.TimeDistributed(layers.MaxPool1D(2))(conv_2)\n\n# Flatten each filter layer for each timesteps\nflatten = layers.TimeDistributed(Flatten())(normalizer_2)\n\n# Feed in the flattened layers to multiple layers of Dense layer,\n# Performing embedding\nDense_1 = layers.TimeDistributed(Dense(300, activation = 'relu'))(flatten)\nnormalizer_3 = layers.BatchNormalization()(Dense_1)\ndrop_1 = layers.Dropout(0.2)(normalizer_3)\n\nDense_2 = layers.TimeDistributed(Dense(150, activation = 'relu'))(drop_1)\nnormalizer_4 = layers.BatchNormalization()(Dense_2)\ndrop_2 = layers.Dropout(0.2)(normalizer_4)\n\nDense_3 = layers.TimeDistributed(Dense(50, activation = 'relu'))(drop_2)\nnormalizer_5 = layers.BatchNormalization()(Dense_3)\ndrop_3 = layers.Dropout(0.2)(normalizer_5)\n\nDense_3 = layers.TimeDistributed(Dense(200, activation = 'relu'))(drop_2)\nnormalizer_5 = layers.BatchNormalization()(Dense_3)\ndrop_3 = layers.Dropout(0.2)(normalizer_5)\n\n# Lastly, put the decoded features in to LSTM layer\nlast_lstm = LSTM(500, return_sequences=True, dropout = 0.3)(drop_3)\n\n# Outputing to final Dense layer with sigmoid activation,\n# To predict the note labels for each timesteps\nsimple_lstm_model = Model(inp, [instrument_layer_simple(last_lstm,\n                                f\"instrument_{ins}\") for ins in instrument_list])\n\n\n2022-09-16 16:43:04.148904: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-16 16:43:04.148928: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\n\n\ntf.keras.utils.plot_model(simple_lstm_model, show_shapes=True, show_dtype=True)\n\n\n\n\n\nclassic_train_generator =  classic_generator(mode='train', batch_size=BATCH_SIZE)\n                        # (tf.dtypes.float32, tf.dtypes.bool))\n\nclassic_eval_generator = classic_generator(mode='test', batch_size=BATCH_SIZE)\n\n\n# Getting the path to the latest trained model\nnewest_ckpt = !ls -dt $PWD/../models/classic_truc_conv_to_lstm/* | head -1\nnewest_ckpt = newest_ckpt[0]\nnewest_ckpt\n\n'/Users/ylee/Documents/Music_transcription_fastai/notebooks/../models/classic_truc_conv_to_lstm/20220916_135022_02_classic_truc_conv_to_lstm'\n\n\n\n# Run the following code to load the newest model\n\nsimple_lstm_model = tf.keras.models.load_model(newest_ckpt, compile=False)\n\nSince our prediction will be a really sparse metrics, with a few 1s, we will need to define our custom loss function such that the false negatives are hugely reduced. We will be using the weighted_cross_entropy_with_logits in tensorflow, and setting positive weight which is larger than 1.\n\ndef weighted_cross_entropy_with_logits(labels, logits):\n    loss = tf.nn.weighted_cross_entropy_with_logits(\n        labels, logits, pos_weight = 150\n        )\n    return loss\n\ndef my_loss():\n    return weighted_cross_entropy_with_logits\n\n# As mentioned before, since our prediction is a sparse multilabel problem, \n# the accuracy might not makes muc of sense, in addition, we will be adding\n# AUC for each instrument to gauge how well the model is performing\nsimple_lstm_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = 0.0005),\n                          loss={f\"instrument_{ins}\": my_loss()\n                                for ins in instrument_list},\n                          metrics=['accuracy', tf.keras.metrics.AUC()])\n\nNameError: name 'simple_lstm_model' is not defined\n\n\n\nfrom datetime import datetime\n\n# Define checkpoint to save model for every epoch\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    f\"../models/classic_truc_conv_to_lstm/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_classic_truc_conv_to_lstm\",\n    monitor='val_accuracy',\n    save_freq='epoch')\n    \n# Define checkpoint to save model if validation loss is decreasing\nearly_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)\n\n\nsimple_lstm_history = simple_lstm_model.fit(classic_train_generator, \n                                                epochs=5,\n                                                validation_data=classic_eval_generator,\n                                                validation_freq=1,\n                                                # use_multiprocessing= True,\n                                                # workers= 3,\n                                                verbose=1,\n                                                callbacks=[ckpt_callback])\n\nNameError: name 'tf' is not defined\n\n\n\n# simple_lstm_model.save('../models/classic_full_convlstm/')\n\nSince we have multiple output with 2 dimensional output, it is easier to use visualization to gauge how well the model is performing.\n\ndef musicnet_eval(model, generator, ins):\n    '''\n    Supporting function to  plot the predicted label and true labels\n    of for specified instrument.\n    \n    Input:\n    model: model to be used\n    generator: testing Sequence generator to be used\n    ins: int, instrument number, ranged from 0 to 10\n    '''\n    _feature, _label = generator.__getitem__(0)\n    _prediction = model.predict(_feature)\n    fig = plt.figure(figsize= (18, 6))\n\n    ax_1 = fig.add_subplot(121)\n    ax_1.set_title('Prediction')\n    sns.heatmap(_prediction[ins][0], ax = ax_1)\n\n    ax_2 = fig.add_subplot(122)\n    sns.heatmap(_label[f\"instrument_{instrument_list[ins]}\"][0], ax = ax_2)\n    ax_2.set_title('True label')\n\n    plt.xlabel('Frequency bins')\n    plt.ylabel('Timesteps')\n\n    plt.tight_layout()\n    plt.show()\n\n\ntest_generator = classic_generator(mode='test', batch_size=1)\n\n\nmusicnet_eval(simple_lstm_model, test_generator, 4)\n\n1/1 [==============================] - 2s 2s/step\n\n\n\n\n\n\ntest_set = test_generator.__getitem__(2)\n\n\ntest_set[0].shape\n\n(1, 200, 128, 1)\n\n\n\npredict_test = simple_lstm_model.predict(test_set[0])\n\n1/1 [==============================] - 0s 25ms/step\n\n\n\ninstrument_list\n\n[1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\n\nsns.heatmap(test_set[1]['instrument_41'][0])\n\n<AxesSubplot:>\n\n\n\n\n\n\ninstrument_list\n\n[1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\nThe problem with this model is that the model is predicting contant value along timesteps. The model focused and retained the sequential relation across timesteps too much. And eventually learned to predict zeros for all scenarios. Look at figure below for a better visualization.\n\n\n\n2 lstm\nOur first model produced unsatisfactory results, which implies we might have to start from a simpler model, we will be starting again at a model with 2 LSTM layers.\n\nBATCH_SIZE = 16\n\ninp = Input((None, 128), batch_size=BATCH_SIZE)\n\n# 2 LSTM layers\nx = LSTM(500, return_sequences=True, dropout = 0.3, stateful = True)(inp)\nx = LSTM(500, return_sequences=True, dropout = 0.3, stateful = True)(x)\n\nlstm_2 = Model(inp, [instrument_layer_simple(x,\n                                f\"instrument_{ins}\") for ins in instrument_list])\n\n\ntf.keras.utils.plot_model(lstm_2, show_shapes=True, show_dtype=True)\n\n\n\n\n\nlstm2_train_generator =  classic_generator(mode='train', batch_size=BATCH_SIZE, expand_dim = False)\n                        # (tf.dtypes.float32, tf.dtypes.bool))\n\nlstm2_eval_generator = classic_generator(mode='test', batch_size=BATCH_SIZE, expand_dim = False, \n                                            preprocess = False)\n\n\ndef weighted_cross_entropy_with_logits(labels, logits):\n    loss = tf.nn.weighted_cross_entropy_with_logits(\n        labels, logits, pos_weight = 2\n        )\n    return loss\n\ndef my_loss():\n    return weighted_cross_entropy_with_logits\n\n\nlstm_2.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = 0.001),\n                          loss={f\"instrument_{ins}\": my_loss()\n                                for ins in instrument_list},\n                          metrics=['accuracy', tf.keras.metrics.AUC()])\n\n\nfrom datetime import datetime\n\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    f\"../models/classic_lstm2/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_classic_lstm2\",\n    monitor='val_accuracy',\n    save_freq='epoch')\n    \nearly_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)\n\n\nlstm2_history = lstm_2.fit(lstm2_train_generator, \n                            epochs=10,\n                            validation_data=lstm2_eval_generator,\n                            validation_freq=1,\n                            use_multiprocessing= True,\n                            workers= 3,\n                            verbose=1,\n                            callbacks=[ckpt_callback])\n\nNameError: name 'lstm_2' is not defined\n\n\n\nmusicnet_eval(lstm_2, test_generator, 3)\n\n1/1 [==============================] - 0s 37ms/step\n\n\n\n\n\nWe still observe the same pattern, that constant values are passed along the timesteps.\nThe x-axis above represents the 81 notes, and y-axis represetns the timesteps.\nThe figure on the left represents the prediction of our model, whereas the right figure represents the true labels.\n\n\nConv to lstm\nAttempts last architecture of padding filters of convolutional layers to LSTM layers. This is the most basic model that extract features by convolutional layers, and passing the feature through the sequnce to make a prediction.\n\nBATCH_SIZE = 8\n\n# We have choose to work with 128 frequency bins,\n# and the None input shape indicates the dynamic length of time dimension\ninp = Input((None, 128, 1), batch_size=BATCH_SIZE)\nnormalizer = layers.BatchNormalization()(inp)\n\n# Extracting features from first convolutional layers.\n# Padding set to same to maintain same time dimension\nconv_1 = Conv2D(30, (50, 40), padding = 'same')(normalizer)\n\nnormalizer_2 = layers.BatchNormalization()(conv_1)\n# pool_1 = layers.TimeDistributed(MaxPool1D(2))(normalizer_2)\n# flatten_1 = flatten = layers.TimeDistributed(layers.Flatten())(pool_1)\n\n# conv_2 = layers.Conv2D(5, (1000, 30), padding = 'same')(pool_1)\n# pool_2 = layers.TimeDistributed(layers.MaxPool1D(2))(conv_2)\n\n# Flatten each filter layer for each timesteps\nflatten = layers.TimeDistributed(Flatten())(normalizer_2)\n\ndrop_1 = layers.Dropout(0.3)(flatten)\n# # Feed in the flattened layers to multiple layers of Dense layer,\n# # Performing embedding\n# Dense_1 = layers.TimeDistributed(Dense(300, activation = 'relu'))(flatten)\n# normalizer_3 = layers.BatchNormalization()(Dense_1)\n# drop_1 = layers.Dropout(0.2)(normalizer_3)\n\n# Dense_2 = layers.TimeDistributed(Dense(150, activation = 'relu'))(drop_1)\n# normalizer_4 = layers.BatchNormalization()(Dense_2)\n# drop_2 = layers.Dropout(0.2)(normalizer_4)\n\n# Dense_3 = layers.TimeDistributed(Dense(50, activation = 'relu'))(drop_2)\n# normalizer_5 = layers.BatchNormalization()(Dense_3)\n# drop_3 = layers.Dropout(0.2)(normalizer_5)\n\n# Dense_3 = layers.TimeDistributed(Dense(200, activation = 'relu'))(drop_2)\n# normalizer_5 = layers.BatchNormalization()(Dense_3)\n# drop_3 = layers.Dropout(0.2)(normalizer_5)\n\n# Lastly, put the decoded features in to LSTM layer\nlast_lstm = LSTM(500, return_sequences=True, dropout = 0.3)(drop_1)\n\n# Outputing to final Dense layer with sigmoid activation,\n# To predict the note labels for each timesteps\nconv_to_lstm_model = Model(inp, [instrument_layer_simple(last_lstm,\n                                f\"instrument_{ins}\") for ins in instrument_list])\n\n\ntf.keras.utils.plot_model(conv_to_lstm_model, show_shapes=True, show_dtype=True)\n\n\n\n\n\nclassic_train_generator =  classic_generator(mode='train', batch_size=BATCH_SIZE)\n                        # (tf.dtypes.float32, tf.dtypes.bool))\n\nclassic_eval_generator = classic_generator(mode='test', batch_size=BATCH_SIZE, preprocess = False)\n\n\ndef weighted_cross_entropy_with_logits(labels, logits):\n    loss = tf.nn.weighted_cross_entropy_with_logits(\n        labels, logits, pos_weight = 5\n        )\n    return loss\n\ndef my_loss():\n    return weighted_cross_entropy_with_logits\n\n# As mentioned before, since our prediction is a sparse multilabel problem, \n# the accuracy might not makes muc of sense, in addition, we will be adding\n# AUC for each instrument to gauge how well the model is performing\nconv_to_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001),\n                          loss={f\"instrument_{ins}\": my_loss() \n                                for ins in instrument_list},\n                          metrics=['accuracy', tf.keras.metrics.AUC()])\n\n\nfrom datetime import datetime\n\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    f\"../models/classic_direct_conv_to_lstm/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_classic_direct_conv_to_lstm\",\n    monitor='val_accuracy',\n    save_freq='epoch')\n    \nearly_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)\n\n\nconv_to_lstm_history = conv_to_lstm_model.fit(classic_train_generator, \n                                                epochs=10,\n                                                validation_data=classic_eval_generator,\n                                                validation_freq=1,\n                                                # use_multiprocessing= True,\n                                                # workers= 3,\n                                                verbose=1,\n                                                # class_weight= {0: 0.11, 1: 0.89}, \n                                                callbacks=[ckpt_callback])\n\nEpoch 1/10\n\n\n2022-08-07 18:44:58.981777: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n2022-08-07 18:44:59.811714: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-07 18:44:59.813228: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-07 18:44:59.813276: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n2022-08-07 18:44:59.814714: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-07 18:44:59.814829: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\nRelying on driver to perform ptx compilation. \nModify $PATH to customize ptxas location.\nThis message will be only logged once.\n\n\n40/40 [==============================] - ETA: 0s - loss: 9.4632 - instrument_1_loss: 0.9001 - instrument_7_loss: 0.8716 - instrument_41_loss: 0.8497 - instrument_42_loss: 0.8685 - instrument_43_loss: 0.8520 - instrument_44_loss: 0.8687 - instrument_61_loss: 0.8508 - instrument_69_loss: 0.8303 - instrument_71_loss: 0.8733 - instrument_72_loss: 0.8566 - instrument_74_loss: 0.8417 - instrument_1_accuracy: 0.0041 - instrument_1_auc_1: 0.4734 - instrument_7_accuracy: 0.0077 - instrument_7_auc_1: 0.3513 - instrument_41_accuracy: 0.0077 - instrument_41_auc_1: 0.5359 - instrument_42_accuracy: 0.0058 - instrument_42_auc_1: 0.5681 - instrument_43_accuracy: 0.0010 - instrument_43_auc_1: 0.5255 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.5647 - instrument_61_accuracy: 0.0016 - instrument_61_auc_1: 0.4531 - instrument_69_accuracy: 0.0015 - instrument_69_auc_1: 0.6652 - instrument_71_accuracy: 0.0340 - instrument_71_auc_1: 0.6068 - instrument_72_accuracy: 6.7188e-04 - instrument_72_auc_1: 0.5844 - instrument_74_accuracy: 0.0421 - instrument_74_auc_1: 0.5877 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n2022-08-07 19:00:44.902483: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-08-07 19:00:44.963806: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-08-07 19:00:44.996746: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_01_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_01_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 977s 24s/step - loss: 9.4632 - instrument_1_loss: 0.9001 - instrument_7_loss: 0.8716 - instrument_41_loss: 0.8497 - instrument_42_loss: 0.8685 - instrument_43_loss: 0.8520 - instrument_44_loss: 0.8687 - instrument_61_loss: 0.8508 - instrument_69_loss: 0.8303 - instrument_71_loss: 0.8733 - instrument_72_loss: 0.8566 - instrument_74_loss: 0.8417 - instrument_1_accuracy: 0.0041 - instrument_1_auc_1: 0.4734 - instrument_7_accuracy: 0.0077 - instrument_7_auc_1: 0.3513 - instrument_41_accuracy: 0.0077 - instrument_41_auc_1: 0.5359 - instrument_42_accuracy: 0.0058 - instrument_42_auc_1: 0.5681 - instrument_43_accuracy: 0.0010 - instrument_43_auc_1: 0.5255 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.5647 - instrument_61_accuracy: 0.0016 - instrument_61_auc_1: 0.4531 - instrument_69_accuracy: 0.0015 - instrument_69_auc_1: 0.6652 - instrument_71_accuracy: 0.0340 - instrument_71_auc_1: 0.6068 - instrument_72_accuracy: 6.7188e-04 - instrument_72_auc_1: 0.5844 - instrument_74_accuracy: 0.0421 - instrument_74_auc_1: 0.5877 - val_loss: 8.4378 - val_instrument_1_loss: 0.8021 - val_instrument_7_loss: 0.7764 - val_instrument_41_loss: 0.7603 - val_instrument_42_loss: 0.7667 - val_instrument_43_loss: 0.7576 - val_instrument_44_loss: 0.7714 - val_instrument_61_loss: 0.7615 - val_instrument_69_loss: 0.7426 - val_instrument_71_loss: 0.7806 - val_instrument_72_loss: 0.7687 - val_instrument_74_loss: 0.7498 - val_instrument_1_accuracy: 0.0000e+00 - val_instrument_1_auc_1: 0.4493 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0069 - val_instrument_41_auc_1: 0.5197 - val_instrument_42_accuracy: 0.0031 - val_instrument_42_auc_1: 0.5051 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5542 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4583 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5306 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5087 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 2/10\n40/40 [==============================] - ETA: 0s - loss: 8.1047 - instrument_1_loss: 0.7867 - instrument_7_loss: 0.7376 - instrument_41_loss: 0.7359 - instrument_42_loss: 0.7357 - instrument_43_loss: 0.7339 - instrument_44_loss: 0.7341 - instrument_61_loss: 0.7287 - instrument_69_loss: 0.7197 - instrument_71_loss: 0.7377 - instrument_72_loss: 0.7307 - instrument_74_loss: 0.7239 - instrument_1_accuracy: 0.0014 - instrument_1_auc_1: 0.4879 - instrument_7_accuracy: 0.0012 - instrument_7_auc_1: 0.4978 - instrument_41_accuracy: 0.0033 - instrument_41_auc_1: 0.5315 - instrument_42_accuracy: 0.0027 - instrument_42_auc_1: 0.5510 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5280 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.4818 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3864 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6101 - instrument_71_accuracy: 0.0000e+00 - instrument_71_auc_1: 0.5646 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5410 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5612     \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n2022-08-07 19:17:20.517681: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-08-07 19:17:20.554600: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_02_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_02_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 995s 25s/step - loss: 8.1047 - instrument_1_loss: 0.7867 - instrument_7_loss: 0.7376 - instrument_41_loss: 0.7359 - instrument_42_loss: 0.7357 - instrument_43_loss: 0.7339 - instrument_44_loss: 0.7341 - instrument_61_loss: 0.7287 - instrument_69_loss: 0.7197 - instrument_71_loss: 0.7377 - instrument_72_loss: 0.7307 - instrument_74_loss: 0.7239 - instrument_1_accuracy: 0.0014 - instrument_1_auc_1: 0.4879 - instrument_7_accuracy: 0.0012 - instrument_7_auc_1: 0.4978 - instrument_41_accuracy: 0.0033 - instrument_41_auc_1: 0.5315 - instrument_42_accuracy: 0.0027 - instrument_42_auc_1: 0.5510 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5280 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.4818 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3864 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6101 - instrument_71_accuracy: 0.0000e+00 - instrument_71_auc_1: 0.5646 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5410 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5612 - val_loss: 7.8978 - val_instrument_1_loss: 0.7504 - val_instrument_7_loss: 0.7148 - val_instrument_41_loss: 0.7218 - val_instrument_42_loss: 0.7141 - val_instrument_43_loss: 0.7118 - val_instrument_44_loss: 0.7135 - val_instrument_61_loss: 0.7121 - val_instrument_69_loss: 0.7065 - val_instrument_71_loss: 0.7230 - val_instrument_72_loss: 0.7210 - val_instrument_74_loss: 0.7088 - val_instrument_1_accuracy: 0.0037 - val_instrument_1_auc_1: 0.4846 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0100 - val_instrument_41_auc_1: 0.5245 - val_instrument_42_accuracy: 0.0031 - val_instrument_42_auc_1: 0.5151 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5917 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.2997 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5111 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4152 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 3/10\n40/40 [==============================] - ETA: 0s - loss: 7.8624 - instrument_1_loss: 0.7675 - instrument_7_loss: 0.7092 - instrument_41_loss: 0.7183 - instrument_42_loss: 0.7135 - instrument_43_loss: 0.7129 - instrument_44_loss: 0.7078 - instrument_61_loss: 0.7065 - instrument_69_loss: 0.7039 - instrument_71_loss: 0.7093 - instrument_72_loss: 0.7083 - instrument_74_loss: 0.7054 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4918 - instrument_7_accuracy: 0.0010 - instrument_7_auc_1: 0.5659 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5233 - instrument_42_accuracy: 0.0012 - instrument_42_auc_1: 0.5637 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5550 - instrument_44_accuracy: 0.0020 - instrument_44_auc_1: 0.5860 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3889 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6318 - instrument_71_accuracy: 1.5625e-05 - instrument_71_auc_1: 0.5802 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4675 - instrument_74_accuracy: 1.5625e-05 - instrument_74_auc_1: 0.5780 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_03_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_03_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 956s 24s/step - loss: 7.8624 - instrument_1_loss: 0.7675 - instrument_7_loss: 0.7092 - instrument_41_loss: 0.7183 - instrument_42_loss: 0.7135 - instrument_43_loss: 0.7129 - instrument_44_loss: 0.7078 - instrument_61_loss: 0.7065 - instrument_69_loss: 0.7039 - instrument_71_loss: 0.7093 - instrument_72_loss: 0.7083 - instrument_74_loss: 0.7054 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4918 - instrument_7_accuracy: 0.0010 - instrument_7_auc_1: 0.5659 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5233 - instrument_42_accuracy: 0.0012 - instrument_42_auc_1: 0.5637 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5550 - instrument_44_accuracy: 0.0020 - instrument_44_auc_1: 0.5860 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3889 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6318 - instrument_71_accuracy: 1.5625e-05 - instrument_71_auc_1: 0.5802 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4675 - instrument_74_accuracy: 1.5625e-05 - instrument_74_auc_1: 0.5780 - val_loss: 7.7954 - val_instrument_1_loss: 0.7227 - val_instrument_7_loss: 0.7037 - val_instrument_41_loss: 0.7197 - val_instrument_42_loss: 0.7082 - val_instrument_43_loss: 0.7105 - val_instrument_44_loss: 0.7031 - val_instrument_61_loss: 0.7064 - val_instrument_69_loss: 0.7001 - val_instrument_71_loss: 0.7094 - val_instrument_72_loss: 0.7105 - val_instrument_74_loss: 0.7012 - val_instrument_1_accuracy: 0.0031 - val_instrument_1_auc_1: 0.4640 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0031 - val_instrument_41_auc_1: 0.5485 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.6343 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4931 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4119 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5317 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5208 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 4/10\n40/40 [==============================] - ETA: 0s - loss: 7.8032 - instrument_1_loss: 0.7629 - instrument_7_loss: 0.7022 - instrument_41_loss: 0.7141 - instrument_42_loss: 0.7073 - instrument_43_loss: 0.7084 - instrument_44_loss: 0.7014 - instrument_61_loss: 0.7015 - instrument_69_loss: 0.6996 - instrument_71_loss: 0.7028 - instrument_72_loss: 0.7026 - instrument_74_loss: 0.7005 - instrument_1_accuracy: 0.0195 - instrument_1_auc_1: 0.4969 - instrument_7_accuracy: 5.4688e-04 - instrument_7_auc_1: 0.5081 - instrument_41_accuracy: 0.0056 - instrument_41_auc_1: 0.5313 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5713 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5383 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.5499 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4050 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5505 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5238 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4662 - instrument_74_accuracy: 3.1250e-05 - instrument_74_auc_1: 0.5639 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_04_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_04_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 840s 21s/step - loss: 7.8032 - instrument_1_loss: 0.7629 - instrument_7_loss: 0.7022 - instrument_41_loss: 0.7141 - instrument_42_loss: 0.7073 - instrument_43_loss: 0.7084 - instrument_44_loss: 0.7014 - instrument_61_loss: 0.7015 - instrument_69_loss: 0.6996 - instrument_71_loss: 0.7028 - instrument_72_loss: 0.7026 - instrument_74_loss: 0.7005 - instrument_1_accuracy: 0.0195 - instrument_1_auc_1: 0.4969 - instrument_7_accuracy: 5.4688e-04 - instrument_7_auc_1: 0.5081 - instrument_41_accuracy: 0.0056 - instrument_41_auc_1: 0.5313 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5713 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5383 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.5499 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4050 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5505 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5238 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4662 - instrument_74_accuracy: 3.1250e-05 - instrument_74_auc_1: 0.5639 - val_loss: 7.7528 - val_instrument_1_loss: 0.7379 - val_instrument_7_loss: 0.6998 - val_instrument_41_loss: 0.7096 - val_instrument_42_loss: 0.7011 - val_instrument_43_loss: 0.7045 - val_instrument_44_loss: 0.6996 - val_instrument_61_loss: 0.6999 - val_instrument_69_loss: 0.6976 - val_instrument_71_loss: 0.7014 - val_instrument_72_loss: 0.7032 - val_instrument_74_loss: 0.6983 - val_instrument_1_accuracy: 0.0113 - val_instrument_1_auc_1: 0.4940 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 6.2500e-04 - val_instrument_41_auc_1: 0.5468 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5828 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5291 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.3873 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.6024 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4427 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 5/10\n40/40 [==============================] - ETA: 0s - loss: 7.7747 - instrument_1_loss: 0.7576 - instrument_7_loss: 0.6996 - instrument_41_loss: 0.7121 - instrument_42_loss: 0.7049 - instrument_43_loss: 0.7059 - instrument_44_loss: 0.6988 - instrument_61_loss: 0.6990 - instrument_69_loss: 0.6979 - instrument_71_loss: 0.7001 - instrument_72_loss: 0.7005 - instrument_74_loss: 0.6984 - instrument_1_accuracy: 0.0180 - instrument_1_auc_1: 0.4997 - instrument_7_accuracy: 9.6875e-04 - instrument_7_auc_1: 0.4890 - instrument_41_accuracy: 0.0024 - instrument_41_auc_1: 0.5324 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5521 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5446 - instrument_44_accuracy: 6.8750e-04 - instrument_44_auc_1: 0.4677 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3171 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6400 - instrument_71_accuracy: 0.0011 - instrument_71_auc_1: 0.5348 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4833 - instrument_74_accuracy: 9.3750e-05 - instrument_74_auc_1: 0.5742 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_05_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_05_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 809s 20s/step - loss: 7.7747 - instrument_1_loss: 0.7576 - instrument_7_loss: 0.6996 - instrument_41_loss: 0.7121 - instrument_42_loss: 0.7049 - instrument_43_loss: 0.7059 - instrument_44_loss: 0.6988 - instrument_61_loss: 0.6990 - instrument_69_loss: 0.6979 - instrument_71_loss: 0.7001 - instrument_72_loss: 0.7005 - instrument_74_loss: 0.6984 - instrument_1_accuracy: 0.0180 - instrument_1_auc_1: 0.4997 - instrument_7_accuracy: 9.6875e-04 - instrument_7_auc_1: 0.4890 - instrument_41_accuracy: 0.0024 - instrument_41_auc_1: 0.5324 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5521 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5446 - instrument_44_accuracy: 6.8750e-04 - instrument_44_auc_1: 0.4677 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3171 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6400 - instrument_71_accuracy: 0.0011 - instrument_71_auc_1: 0.5348 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4833 - instrument_74_accuracy: 9.3750e-05 - instrument_74_auc_1: 0.5742 - val_loss: 7.7501 - val_instrument_1_loss: 0.7305 - val_instrument_7_loss: 0.6980 - val_instrument_41_loss: 0.7178 - val_instrument_42_loss: 0.7033 - val_instrument_43_loss: 0.7052 - val_instrument_44_loss: 0.6978 - val_instrument_61_loss: 0.7014 - val_instrument_69_loss: 0.6965 - val_instrument_71_loss: 0.7022 - val_instrument_72_loss: 0.7005 - val_instrument_74_loss: 0.6969 - val_instrument_1_accuracy: 0.0094 - val_instrument_1_auc_1: 0.4634 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0000e+00 - val_instrument_41_auc_1: 0.5274 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5837 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4972 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.2460 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5497 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4621 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 6/10\n40/40 [==============================] - ETA: 0s - loss: 7.7649 - instrument_1_loss: 0.7601 - instrument_7_loss: 0.6981 - instrument_41_loss: 0.7116 - instrument_42_loss: 0.7037 - instrument_43_loss: 0.7046 - instrument_44_loss: 0.6974 - instrument_61_loss: 0.6981 - instrument_69_loss: 0.6968 - instrument_71_loss: 0.6986 - instrument_72_loss: 0.6990 - instrument_74_loss: 0.6970 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4878 - instrument_7_accuracy: 7.5000e-04 - instrument_7_auc_1: 0.4831 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5172 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5709 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5191 - instrument_44_accuracy: 2.3437e-04 - instrument_44_auc_1: 0.3375 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3742 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5869 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5242 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 5.1562e-04 - instrument_74_auc_1: 0.5644 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_06_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_06_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 823s 20s/step - loss: 7.7649 - instrument_1_loss: 0.7601 - instrument_7_loss: 0.6981 - instrument_41_loss: 0.7116 - instrument_42_loss: 0.7037 - instrument_43_loss: 0.7046 - instrument_44_loss: 0.6974 - instrument_61_loss: 0.6981 - instrument_69_loss: 0.6968 - instrument_71_loss: 0.6986 - instrument_72_loss: 0.6990 - instrument_74_loss: 0.6970 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4878 - instrument_7_accuracy: 7.5000e-04 - instrument_7_auc_1: 0.4831 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5172 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5709 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5191 - instrument_44_accuracy: 2.3437e-04 - instrument_44_auc_1: 0.3375 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3742 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5869 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5242 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 5.1562e-04 - instrument_74_auc_1: 0.5644 - val_loss: 7.7343 - val_instrument_1_loss: 0.7160 - val_instrument_7_loss: 0.6968 - val_instrument_41_loss: 0.7101 - val_instrument_42_loss: 0.7015 - val_instrument_43_loss: 0.7050 - val_instrument_44_loss: 0.6966 - val_instrument_61_loss: 0.7034 - val_instrument_69_loss: 0.6956 - val_instrument_71_loss: 0.7063 - val_instrument_72_loss: 0.7070 - val_instrument_74_loss: 0.6960 - val_instrument_1_accuracy: 0.0050 - val_instrument_1_auc_1: 0.5094 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0000e+00 - val_instrument_41_auc_1: 0.5444 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.6109 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4586 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0025 - val_instrument_61_auc_1: 0.4560 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0012 - val_instrument_71_auc_1: 0.5428 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4793 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 7/10\n40/40 [==============================] - ETA: 0s - loss: 7.7545 - instrument_1_loss: 0.7566 - instrument_7_loss: 0.6973 - instrument_41_loss: 0.7104 - instrument_42_loss: 0.7032 - instrument_43_loss: 0.7040 - instrument_44_loss: 0.6965 - instrument_61_loss: 0.6975 - instrument_69_loss: 0.6961 - instrument_71_loss: 0.6978 - instrument_72_loss: 0.6986 - instrument_74_loss: 0.6965 - instrument_1_accuracy: 0.0227 - instrument_1_auc_1: 0.5040 - instrument_7_accuracy: 9.5313e-04 - instrument_7_auc_1: 0.5582 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5480 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5572 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5104 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.4126 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4421 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5621 - instrument_71_accuracy: 1.4062e-04 - instrument_71_auc_1: 0.5336 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5092 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5229 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_07_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_07_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1015s 26s/step - loss: 7.7545 - instrument_1_loss: 0.7566 - instrument_7_loss: 0.6973 - instrument_41_loss: 0.7104 - instrument_42_loss: 0.7032 - instrument_43_loss: 0.7040 - instrument_44_loss: 0.6965 - instrument_61_loss: 0.6975 - instrument_69_loss: 0.6961 - instrument_71_loss: 0.6978 - instrument_72_loss: 0.6986 - instrument_74_loss: 0.6965 - instrument_1_accuracy: 0.0227 - instrument_1_auc_1: 0.5040 - instrument_7_accuracy: 9.5313e-04 - instrument_7_auc_1: 0.5582 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5480 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5572 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5104 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.4126 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4421 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5621 - instrument_71_accuracy: 1.4062e-04 - instrument_71_auc_1: 0.5336 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5092 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5229 - val_loss: 7.7320 - val_instrument_1_loss: 0.7148 - val_instrument_7_loss: 0.6960 - val_instrument_41_loss: 0.7158 - val_instrument_42_loss: 0.7021 - val_instrument_43_loss: 0.7050 - val_instrument_44_loss: 0.6959 - val_instrument_61_loss: 0.6996 - val_instrument_69_loss: 0.6951 - val_instrument_71_loss: 0.7057 - val_instrument_72_loss: 0.7067 - val_instrument_74_loss: 0.6953 - val_instrument_1_accuracy: 0.0056 - val_instrument_1_auc_1: 0.4874 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0081 - val_instrument_41_auc_1: 0.5351 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5699 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4698 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.5076 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5836 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5175 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 8/10\n40/40 [==============================] - ETA: 0s - loss: 7.7481 - instrument_1_loss: 0.7563 - instrument_7_loss: 0.6965 - instrument_41_loss: 0.7097 - instrument_42_loss: 0.7023 - instrument_43_loss: 0.7032 - instrument_44_loss: 0.6959 - instrument_61_loss: 0.6970 - instrument_69_loss: 0.6959 - instrument_71_loss: 0.6972 - instrument_72_loss: 0.6981 - instrument_74_loss: 0.6960 - instrument_1_accuracy: 0.0177 - instrument_1_auc_1: 0.5015 - instrument_7_accuracy: 6.2500e-04 - instrument_7_auc_1: 0.5371 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5368 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5660 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5082 - instrument_44_accuracy: 8.2812e-04 - instrument_44_auc_1: 0.4698 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4768 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5771 - instrument_71_accuracy: 1.8750e-04 - instrument_71_auc_1: 0.5660 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5181 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5266 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_08_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_08_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1063s 27s/step - loss: 7.7481 - instrument_1_loss: 0.7563 - instrument_7_loss: 0.6965 - instrument_41_loss: 0.7097 - instrument_42_loss: 0.7023 - instrument_43_loss: 0.7032 - instrument_44_loss: 0.6959 - instrument_61_loss: 0.6970 - instrument_69_loss: 0.6959 - instrument_71_loss: 0.6972 - instrument_72_loss: 0.6981 - instrument_74_loss: 0.6960 - instrument_1_accuracy: 0.0177 - instrument_1_auc_1: 0.5015 - instrument_7_accuracy: 6.2500e-04 - instrument_7_auc_1: 0.5371 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5368 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5660 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5082 - instrument_44_accuracy: 8.2812e-04 - instrument_44_auc_1: 0.4698 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4768 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5771 - instrument_71_accuracy: 1.8750e-04 - instrument_71_auc_1: 0.5660 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5181 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5266 - val_loss: 7.7214 - val_instrument_1_loss: 0.7360 - val_instrument_7_loss: 0.6954 - val_instrument_41_loss: 0.7096 - val_instrument_42_loss: 0.6991 - val_instrument_43_loss: 0.7022 - val_instrument_44_loss: 0.6953 - val_instrument_61_loss: 0.6950 - val_instrument_69_loss: 0.6947 - val_instrument_71_loss: 0.6990 - val_instrument_72_loss: 0.7001 - val_instrument_74_loss: 0.6949 - val_instrument_1_accuracy: 0.0050 - val_instrument_1_auc_1: 0.4999 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0025 - val_instrument_41_auc_1: 0.5021 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5982 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5448 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.0000e+00 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5107 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4850 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 9/10\n40/40 [==============================] - ETA: 0s - loss: 7.7462 - instrument_1_loss: 0.7569 - instrument_7_loss: 0.6962 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7022 - instrument_43_loss: 0.7034 - instrument_44_loss: 0.6954 - instrument_61_loss: 0.6962 - instrument_69_loss: 0.6956 - instrument_71_loss: 0.6970 - instrument_72_loss: 0.6976 - instrument_74_loss: 0.6956 - instrument_1_accuracy: 0.0090 - instrument_1_auc_1: 0.4916 - instrument_7_accuracy: 8.7500e-04 - instrument_7_auc_1: 0.4746 - instrument_41_accuracy: 0.0042 - instrument_41_auc_1: 0.5434 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5592 - instrument_43_accuracy: 1.5625e-05 - instrument_43_auc_1: 0.5093 - instrument_44_accuracy: 0.0019 - instrument_44_auc_1: 0.5424 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4239 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5228 - instrument_71_accuracy: 6.8750e-04 - instrument_71_auc_1: 0.5023 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5054 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.4956 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_09_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_09_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1070s 27s/step - loss: 7.7462 - instrument_1_loss: 0.7569 - instrument_7_loss: 0.6962 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7022 - instrument_43_loss: 0.7034 - instrument_44_loss: 0.6954 - instrument_61_loss: 0.6962 - instrument_69_loss: 0.6956 - instrument_71_loss: 0.6970 - instrument_72_loss: 0.6976 - instrument_74_loss: 0.6956 - instrument_1_accuracy: 0.0090 - instrument_1_auc_1: 0.4916 - instrument_7_accuracy: 8.7500e-04 - instrument_7_auc_1: 0.4746 - instrument_41_accuracy: 0.0042 - instrument_41_auc_1: 0.5434 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5592 - instrument_43_accuracy: 1.5625e-05 - instrument_43_auc_1: 0.5093 - instrument_44_accuracy: 0.0019 - instrument_44_auc_1: 0.5424 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4239 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5228 - instrument_71_accuracy: 6.8750e-04 - instrument_71_auc_1: 0.5023 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5054 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.4956 - val_loss: 7.7105 - val_instrument_1_loss: 0.7222 - val_instrument_7_loss: 0.6950 - val_instrument_41_loss: 0.7136 - val_instrument_42_loss: 0.6994 - val_instrument_43_loss: 0.7035 - val_instrument_44_loss: 0.6950 - val_instrument_61_loss: 0.6960 - val_instrument_69_loss: 0.6945 - val_instrument_71_loss: 0.6972 - val_instrument_72_loss: 0.6994 - val_instrument_74_loss: 0.6946 - val_instrument_1_accuracy: 0.0031 - val_instrument_1_auc_1: 0.4976 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0106 - val_instrument_41_auc_1: 0.5649 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5388 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5100 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4717 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.4343 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4660 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 10/10\n40/40 [==============================] - ETA: 0s - loss: 7.7444 - instrument_1_loss: 0.7565 - instrument_7_loss: 0.6958 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7021 - instrument_43_loss: 0.7035 - instrument_44_loss: 0.6952 - instrument_61_loss: 0.6963 - instrument_69_loss: 0.6953 - instrument_71_loss: 0.6967 - instrument_72_loss: 0.6973 - instrument_74_loss: 0.6954 - instrument_1_accuracy: 0.0147 - instrument_1_auc_1: 0.4936 - instrument_7_accuracy: 8.5937e-04 - instrument_7_auc_1: 0.5001 - instrument_41_accuracy: 0.0035 - instrument_41_auc_1: 0.5259 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5330 - instrument_43_accuracy: 3.1250e-05 - instrument_43_auc_1: 0.4885 - instrument_44_accuracy: 0.0027 - instrument_44_auc_1: 0.5748 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4723 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5207 - instrument_71_accuracy: 0.0010 - instrument_71_auc_1: 0.4945 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4725 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5151 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_10_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_10_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1063s 27s/step - loss: 7.7444 - instrument_1_loss: 0.7565 - instrument_7_loss: 0.6958 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7021 - instrument_43_loss: 0.7035 - instrument_44_loss: 0.6952 - instrument_61_loss: 0.6963 - instrument_69_loss: 0.6953 - instrument_71_loss: 0.6967 - instrument_72_loss: 0.6973 - instrument_74_loss: 0.6954 - instrument_1_accuracy: 0.0147 - instrument_1_auc_1: 0.4936 - instrument_7_accuracy: 8.5937e-04 - instrument_7_auc_1: 0.5001 - instrument_41_accuracy: 0.0035 - instrument_41_auc_1: 0.5259 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5330 - instrument_43_accuracy: 3.1250e-05 - instrument_43_auc_1: 0.4885 - instrument_44_accuracy: 0.0027 - instrument_44_auc_1: 0.5748 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4723 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5207 - instrument_71_accuracy: 0.0010 - instrument_71_auc_1: 0.4945 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4725 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5151 - val_loss: 7.7124 - val_instrument_1_loss: 0.7159 - val_instrument_7_loss: 0.6948 - val_instrument_41_loss: 0.7116 - val_instrument_42_loss: 0.6982 - val_instrument_43_loss: 0.7023 - val_instrument_44_loss: 0.6947 - val_instrument_61_loss: 0.6964 - val_instrument_69_loss: 0.6943 - val_instrument_71_loss: 0.7038 - val_instrument_72_loss: 0.7060 - val_instrument_74_loss: 0.6944 - val_instrument_1_accuracy: 0.0044 - val_instrument_1_auc_1: 0.5049 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0131 - val_instrument_41_auc_1: 0.5142 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5568 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4590 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.4428 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.4287 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4621 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\n\n\n\nconv_to_lstm_model.save('../models/new_classic_conv_to_lstm/')\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/new_classic_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/new_classic_conv_to_lstm/assets\n\n\n\nsave_history(conv_to_lstm_history.history, '../models/new_classic_conv_to_lstm.pkl')\n\n\nmusicnet_eval(conv_to_lstm_model, classic_eval_generator, 2)\n\n1/1 [==============================] - 0s 25ms/step\n\n\n\n\n\nAgain, same behavior is observed, we have concluded that the input data might be incompatible for our model, further investigation on the input pipeline should be taken."
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/music_transcription_2conv.html",
    "href": "posts/music/Music_trainscription_fastai/notebooks/music_transcription_2conv.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import glob\nimport os\nimport random\nfrom datetime import datetime\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.stats\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow.python.platform.build_info as build\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.python.client import device_lib\n\nfrom spec_generator_sequence import spec_generator\nfrom spec_input_generator import gen, gen_eval\n# import spectrogram\n# from spectrogram import generate_spec\n# from spectrogram import truncate_spec\n# from spectrogram import mask_spec\n# from spectrogram import add_noise\n# from spectrogram import path_to_preprocessing\nfrom spectrogram_class import spectrogram\n\n\nOrchideaSOL CNN\nThis notebooks is succesive from the baseline model notebook from music_transcription_class.ipynb.\nIn the notebook, we had demonstrate the limitation of simple CNN model on classifying instruments from raw audio files.\nIn this notebook, we will be running a similar CNN architecture, but with deeper layers, and applying regularization, batch normalization and dropout techniques.\n\nprint(build.build_info['cuda_version'])\n\n11.2\n\n\n\ndevice_lib.list_local_devices()\n\n2022-08-08 07:35:52.733084: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-08 07:35:52.777510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:52.813076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:52.813878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.814786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.815302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.815685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.816454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3085 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 13176899990423004209\n xla_global_id: -1,\n name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 3235774464\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 4598158092110336817\n physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n xla_global_id: 416903419]\n\n\n\ntf.test.is_gpu_available()\n\nWARNING:tensorflow:From /tmp/ipykernel_49617/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\n\n\n2022-08-08 07:35:53.919564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\nTrue\n\n\n2022-08-08 07:35:53.920078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.920462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.920851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.921170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.921402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3085 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n\ntf.config.list_physical_devices('GPU')\n\n2022-08-08 07:35:54.029967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:54.030428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:54.030817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\ngpu_devices\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\nWe first start by generating training and testing dataset as usual.\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\n\n\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\n\n# Splitting data for training and testing\ntrain_df, test_df = train_test_split(meta_df, stratify=meta_df['Instrument (in full)'], \n                                        train_size=0.8)\n\n\ntrain_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      4961\n      Strings/Contrabass/pizzicato_l_vib/Cb-pizz_lv-...\n      Strings\n      Violin Family\n      Cb\n      Contrabass\n      pizz_lv\n      pizzicato_l_vib\n      E1\n      28.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      2\n    \n    \n      3401\n      PluckedStrings/Guitar/ordinario/Gtr-ord-F#5-ff...\n      PluckedStrings\n      Plucked Strings\n      Gtr\n      Guitar\n      ord\n      ordinario\n      F#5\n      78.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      1.0\n      True\n      1\n    \n  \n\n\n\n\n\nsample = next(gen(train_df, return_class = True))\n\n\n# Getting the shape of input from generator\nspec_shape = sample[0].spec.shape\nspec_shape\n\n(256, 500, 1)\n\n\n\nnext(gen_eval(test_df))[0].shape\n\n(256, 500, 1)\n\n\nWe will be using the same Sequence data generator, as in previous notebook. However, to optimize our performance, we will be converting the generator in to a tf Dataset object, and optimized the performance by prefetching the dataset while fitting is in progress.\nThe process of prefecthing and training can be visualized using tf.data API. Which is in the todo list of this project.\n\nBATCH_SIZE = 32\n\ntrain_generator = (tf.data.Dataset.from_generator(lambda: spec_generator(train_df, BATCH_SIZE, \n                  add_channel = True), output_types=(tf.float32, tf.int32), \n                 output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                 (BATCH_SIZE, 16)))).prefetch((tf.data.experimental.AUTOTUNE))\n\neval_generator = (tf.data.Dataset.from_generator(lambda: spec_generator(test_df, BATCH_SIZE, \n                  add_channel = True), \n                    output_types=(tf.float32, tf.int32), \n                 output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                 (BATCH_SIZE, 16)))).prefetch((tf.data.experimental.AUTOTUNE))\n\n2022-08-08 07:35:55.370698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.371338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.371817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.372308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.372680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.372983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3085 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n\n# Sanity check for shape\ntrain_generator\n\n<PrefetchDataset element_spec=(TensorSpec(shape=(32, 256, 500, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 16), dtype=tf.int32, name=None))>\n\n\nNow we can finally start to build our model, the idea is same as before, a deeper convolutional model, including dropouts and regularization.\n\n# Starting the model\nmodel_2conv = models.Sequential()\n\n# Adding the first convoluton-pooling layer\nmodel_2conv.add(layers.InputLayer((spec_shape[0], spec_shape[1], 1),\n                     batch_size = BATCH_SIZE, dtype = tf.float32))\nmodel_2conv.add(layers.Conv2D(30, (150, 300), activation='relu', \n                    kernel_regularizer = tf.keras.regularizers.L2(l2=0.01)))\nmodel_2conv.add(layers.MaxPool2D((2, 3)))\nmodel_2conv.add(layers.BatchNormalization())\n\n# Addig the second convolutional-pooling layer\nmodel_2conv.add(layers.Conv2D(15, (15, 30), activation = 'relu',\n                                kernel_regularizer = tf.keras.regularizers.L2(l2=0.01)))\nmodel_2conv.add(layers.MaxPool2D(2, 3))\nmodel_2conv.add(layers.BatchNormalization())\n\n# Combined the filter layers into 1\nmodel_2conv.add(layers.Flatten())\nmodel_2conv.add(layers.Dropout(0.2))\nmodel_2conv.add(layers.Dense(200, activation = 'relu'))\nmodel_2conv.add(layers.Dropout(0.2))\nmodel_2conv.add(layers.Dense(50, activation = 'relu'))\nmodel_2conv.add(layers.Dropout(0.2))\n\n# Final layer to classify the 16 instruments\n# We are using softmax activation since this is a multiclass classification problem\nmodel_2conv.add(layers.Dense(16, activation = 'softmax'))\n\nmodel_2conv.build()\n\n\n# Visualizing the CNN architecture\ntf.keras.utils.plot_model(model_2conv, show_shapes = True, show_dtype= True)\n\n\n\n\nOur input shape will be the same as in the previous notebook, with 256 frequency bins and 500 timesteps\nOur output layer consists of 16 neurons, each representing one possible instrument class.\nSince we will be predicting a multiclass label (Only one true label over multiple options), we will be using categorical cross entropy as our loss function, note the this is the reason why we have chose to use softmax as the activation function of our output layer.\n\n\nmodel_2conv.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.03),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy'])\n\n\nmodel_2conv.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (32, 107, 201, 30)        1350030   \n                                                                 \n max_pooling2d (MaxPooling2D  (32, 53, 67, 30)         0         \n )                                                               \n                                                                 \n batch_normalization (BatchN  (32, 53, 67, 30)         120       \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (32, 39, 38, 15)          202515    \n                                                                 \n max_pooling2d_1 (MaxPooling  (32, 13, 13, 15)         0         \n 2D)                                                             \n                                                                 \n batch_normalization_1 (Batc  (32, 13, 13, 15)         60        \n hNormalization)                                                 \n                                                                 \n flatten (Flatten)           (32, 2535)                0         \n                                                                 \n dropout (Dropout)           (32, 2535)                0         \n                                                                 \n dense (Dense)               (32, 200)                 507200    \n                                                                 \n dropout_1 (Dropout)         (32, 200)                 0         \n                                                                 \n dense_1 (Dense)             (32, 50)                  10050     \n                                                                 \n dropout_2 (Dropout)         (32, 50)                  0         \n                                                                 \n dense_2 (Dense)             (32, 16)                  816       \n                                                                 \n=================================================================\nTotal params: 2,070,791\nTrainable params: 2,070,701\nNon-trainable params: 90\n_________________________________________________________________\n\n\n\n#  model_2conv = tf.keras.models.load_model('../models/2conv/six/')\n\n\n\nModel training\nNow that we have done the setup we can finally train our model\n\n# ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n#     f\"../models/baseline_checkpoint/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_model_2conv\", \n#                                                     monitor='val_accuracy')\n# early_callback = tf.keras.callbacks.EarlyStopping(monitor = 'accuracy', patience = 2)\n\n\n# history = model_2conv.fit(train_generator, epochs = 4, verbose=1, \n#                     validation_data = eval_generator,\n#                     validation_steps = 10, validation_freq= 2, \n#                     use_multiprocessing=True, workers = 2, callbacks=[ckpt_callback, early_callback])\n\n\n# model_2conv.save('../models/new_OrchideaSOL_2conv/')\n\n\n\nModel Evaluating\n\n# plt.plot([2, 4], history.history['val_accuracy'], label = 'Validation accuracy')\n# plt.plot([1, 2, 3, 4], history.history['accuracy'], label = 'Training accuracy')\n# plt.title('Accuracy of baseline model after early callback')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy')\n# plt.legend()\n# plt.show()\n\nThe figure above shows the accuracy of 4 epochs we have ran, combining with the our baseline model of one convolutional layers, we can conclude that basic CNN since to be performing poorly on this problem. Since\n\nBoth the validation and training accuracy starts to drop after 3 epochs\nTraining time is around 30 minutes per epoch\n\nWe can still look at the loos function\n\n# plt.plot([2, 4], history.history['val_loss'], label = 'Validation loss')\n# plt.plot([1, 2, 3, 4], history.history['loss'], label = 'Training loss')\n# plt.title('Loss of baseline model after early callback')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.show()\n\nI have mistakenly run the code without saving the history, and lost the plot\nThe general shape of the figure is a sharp drop at first spoch, and remained fairly strat with minimal decrease after that\nThe loss function hardly decrease after the first epoch, however, both the decerase of loss function and accuracy represents that the categorical cross entropy might not be the best choice for such classification problem.\nBut the model might inprove after several epoch, this notebook is yet to be run again with longer time after the more important preceeding notebooks (Sequential music transcription with LSTM) had been done.\nAlso the hyperparameter such as the number of frequency bins, optimizer and regularization coefficient still can be optimized. Due to the limited time and high training time, this notebook had been added to the to do list, and decreased in priority.\nNow lets look at the confusion matrix\n\ndef orchidea_confusion_matrix(model, generator, instrument_list):\n    '''\n    Plot confusion matrix for OrchideaSOL dataset\n    \n    Input:\n    model: Model to be used\n    generator: Sequence class, generator to generate feature and labels for \n                OrchideaSOL dataset\n    instrument_list: list of instrument in alphabetical order, used to label the plot\n    \n    Output:\n    predict: np.array, Predicted output\n    prediction_label: np.array, True label\n    '''\n    prediction_feature, prediction_label = generator.__getitem__(0)\n    predict = predict = model.predict(prediction_feature)\n    assert prediction_label.shape == predict.shape\n    from sklearn.metrics import confusion_matrix\n\n    plt.figure(figsize = (12, 10))\n    sns.heatmap(confusion_matrix(np.argmax(prediction_label, axis=1), \n                        np.argmax(predict, axis = 1)), annot = True, \n                        xticklabels=instrument_list, \n                        yticklabels=instrument_list)\n    plt.title('Confusion matrix for baseline model')\n    plt.show()\n\n    return predict, prediction_label\n\n\nmodel = tf.keras.models.load_model('../models/baseline_checkpoint/20220807_052111_02_model_2conv')\n\n\n# Defining generator to be used in our evaluation\n# Since our gpu memory space is very limited, we wil be using 1/10 of \n# testing dataset, and omitting any data augmentation on the spectrogram\nprediction_generator = spec_generator(test_df, test_df.shape[0] // 5, add_channel=True,\n                                        live_generation = True, preprocess = False, \n                                        n_mels = 256)\n\n\n\ninstrument_list = sorted(meta_df['Instrument (in full)'].unique())\ninstrument_list\n\n['Accordion',\n 'Alto Saxophone',\n 'Bass Tuba',\n 'Bassoon',\n 'Cello',\n 'Clarinet in Bb',\n 'Contrabass',\n 'Flute',\n 'French Horn',\n 'Guitar',\n 'Harp',\n 'Oboe',\n 'Trombone',\n 'Trumpet in C',\n 'Viola',\n 'Violin']\n\n\n\npredict, predict_label = orchidea_confusion_matrix(model, \n                                prediction_generator, instrument_list)\n\n2022-08-08 07:37:20.613548: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 271360000 exceeds 10% of free system memory.\n2022-08-08 07:37:20.955501: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 271360000 exceeds 10% of free system memory.\n2022-08-08 07:37:21.382134: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory.\n2022-08-08 07:37:21.382196: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory.\n2022-08-08 07:37:21.382234: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory.\n2022-08-08 07:37:22.156406: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n2022-08-08 07:37:25.464340: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-08 07:37:25.467227: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-08 07:37:25.467562: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n2022-08-08 07:37:25.469313: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-08 07:37:25.469628: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\nRelying on driver to perform ptx compilation. \nModify $PATH to customize ptxas location.\nThis message will be only logged once.\n\n\n17/17 [==============================] - 87s 3s/step\n\n\n\n\n\n\n\nERROR\nFurther inspection is needed, as the model converged to predicting the same class at the end od epoch."
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/music_transcription_RNN.html",
    "href": "posts/music/Music_trainscription_fastai/notebooks/music_transcription_RNN.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import Input\nfrom keras import backend as K\nfrom keras import layers, models\nfrom keras.layers import LSTM, Dense\nfrom keras.models import Model, Sequential\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n\nfrom spec_generator_sequence import spec_generator\nfrom spec_generator_sequence_multilabel import (spec_generator_multi,\n                                                spec_generator_multioutput)\nfrom spec_input_generator import gen, gen_eval\nfrom spectrogram_class import spectrogram"
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/music_transcription_RNN.html#understanding-unrolled-lstm-under-10-seconds",
    "href": "posts/music/Music_trainscription_fastai/notebooks/music_transcription_RNN.html#understanding-unrolled-lstm-under-10-seconds",
    "title": "Most interesting blog on Earth!",
    "section": "Understanding unrolled LSTM, under 10 seconds!",
    "text": "Understanding unrolled LSTM, under 10 seconds!\n\n\n\nImage from this fantastic blog.\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nIf you are still here"
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/music_transcription_RNN.html#understanding-lstm-under-1-minute",
    "href": "posts/music/Music_trainscription_fastai/notebooks/music_transcription_RNN.html#understanding-lstm-under-1-minute",
    "title": "Most interesting blog on Earth!",
    "section": "Understanding LSTM, under 1 minute!",
    "text": "Understanding LSTM, under 1 minute!\nLSTM, or more generally RNN, is useful to find the relation between sequential data. Instead of having a fixed length of inputs, and trying to learn the weights for each input in a model, RNN uses previous inputs as a predictor for current prediction!\nEach RNN cells can have 2 outputs (or 3 for LSMT, more on that later), and the first output will be feed in to the next LSTM cell, undergoes matrix multiplication, and combined with the input of the next sequential timestep. The cycle goes on. The final output will then be fed into the next layer\nIf the parameter return_sequence of LSTM is set to true, then each LSTM cell will have a second output, to be fed to the next layer, hence we will have a output with the same length along the sequence. This architecture is called an unrolled LSTM.\nIn this notebook, we will be using the single output version of LSTM, then feeding the output to layers of Dense layers, to classify the instrument and notes of the audio files.\nSummary: * LSTM works will flexible length of input data * LSTM uses the previous data as input, transformed using kernel to be learned * LSTM can produce output of same sequence length.\n\n\ntf.test.is_gpu_available()\n\nWARNING:tensorflow:From /tmp/ipykernel_4020/4084812110.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\n\n\n2022-08-07 18:41:28.718001: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-07 18:41:28.791105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:28.828697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:28.829898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.878813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.879839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.880616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.881097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3370 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\nTrue\n\n\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\n\n\nmeta_df.shape\n\n(13162, 17)\n\n\n\nmeta_df = meta_df[~meta_df['Pitch ID (if applicable)'].isna()]\n\n\nmeta_df.isna().sum()\n\nPath                              0\nFamily (abbr.)                    0\nFamily (in full)                  0\nInstrument (abbr.)                0\nInstrument (in full)              0\nTechnique (abbr.)                 0\nTechnique (in full)               0\nPitch                             0\nPitch ID (if applicable)          0\nDynamics                          0\nDynamics ID (if applicable)     568\nInstance ID                       0\nMute (abbr.)                      0\nMute (in full)                    0\nString ID (if applicable)      5666\nNeeded digital retuning           0\nFold                              0\ndtype: int64\n\n\n\nmeta_df.shape\n\n(13162, 17)\n\n\n\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\nAs of before, the columns we are interested in (instruments and pitch id) doesnt have any null vales, we can just perform the train test split and the rest of the preprocessing is taken by our spectrogram class module.\n\nfrom random import random\n\n\ntrain_df, test_df = train_test_split(meta_df, stratify=meta_df['Instrument (in full)'], \n                                            train_size=0.7, random_state= 42)\n\n\nmulti_generator = spec_generator_multi(train_df, 32)\n\n\n_, num_target = multi_generator.__getitem__(2)[1].shape\nprint(num_target)\n\n107\n\n\n\n_, num_row, num_col= multi_generator.__getitem__(2)[0].shape\nprint(num_row)\nprint(num_col)\n\n500\n256"
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/music_transcription_class.html",
    "href": "posts/music/Music_trainscription_fastai/notebooks/music_transcription_class.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "!which python\n\n/home/shiya/anaconda3/envs/music/bin/python\n\n\n\nimport glob\nimport os\nimport random\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.stats\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import datasets, layers, models\n\nfrom spec_generator_sequence import spec_generator\nfrom spectrogram_class import spectrogram\n\n\nPreprocessing\nThis is a successive notebook after the EDA notebooks.\nAfter the EDA analysis, we can proceed to perform the preprocessing for audio. We will be working will spectrogram, as a spectrogram contains the spatial and temporal information of a audio files.\nWe will be utilizing Librosa package for spectrogram generation and augmentation. For easier implementation, we have defined a spectrogram class to perform all the procedures. Please refer to spectrogram_class.py in the same folder for more information.\n\nspectrogram\n\nspectrogram_class.spectrogram\n\n\nLets have a look at what a spectrogram looks like\n\nprint(tf.config.list_physical_devices('GPU'))\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n2022-08-08 01:38:04.723562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:04.752704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:04.753369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\nhop_length = 4096\nwin_length = 1024\nn_fft = 1024\n\nWe have randomly picked a audio file in OrchideaSOL to showcase the spectrogram.\n\n%%time\ntest_spec = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\n\n# To demonstrate that the test_spec is an instance on our custom defined class\n# let's check its type!\ntype(test_spec)\n\nCPU times: user 47.1 ms, sys: 76.8 ms, total: 124 ms\nWall time: 302 ms\n\n\nspectrogram_class.spectrogram\n\n\nWe have a support function in the class to help us visualized the spectrogram, now lets have a look at what a harps looks like in spectrogram\n\ntest_spec.plot_spec() \n\n\n\n\nNow I owe you some explanation, why am I showing you this? There are some important points to observe:\n\nYou probably heard about the relation between a note and its soundwave frequency, for example, in piano the middle G3 note corresponds to 196 HZ. But we are not exactly seeing one horizontal line on 196 HZ. Why is that? The main reason is that the magnitude of sound wave/magnitude is additive! Lets have a closer look:\n\n\n\n\n(screenshot captured from our lord and savior, 3Blue1Brown.\n\nAlthought the resulting soundwave is of frequency of 196 HZ, it is actually a combination of soundwave of different frequencies. This is exactly the same note for piano and violin sounds different! Imagine a world where a tuba, harp, tuna and your cat sound the same, what a horrible world\n\n\nThis is actually why throughout the project, we have decided to use neural network to perform the music transcription, a convolutional network or RNN are able to capture the feature between spatial and temporal information across the spectrogram.\n\n\n\n(A demonstration of how a CNN is able to capture different features on a face in different layers.)\nBut before we can use the dataset into directly, we have to consider about the data augmentation. Our model should be able to perform tasks include:\n\nTranscribe music in a noisy environment\nTranscribe music is moderate data loss\nStill be able to tell both audio files contain violins, although the violins have different sound signature\n\nBut how can our model learn these difference if our data is perfectly clean, and only contain audio files from the same instruments. That is where data augmentation comes in.\n\n\nData augmentation\nWe have to augmentate our data so that our training dataset match the scenario above. To achieve this, we have decided to add noise, masking and shifting to the spectrogram.\nAll of these function had been defined under the spectrogram class. Now lets try to visualize this.\n\nAdding noise\nRemeber this is how our signal and spectrogram for harps looks like:\n\ndef plt_signal_spec(spec, xlim = None):\n    fig = plt.figure(figsize = (14, 6))\n    ax_1 = fig.add_subplot(121)\n    ax_1.plot(spec.signal)\n    if xlim:\n        ax_1.set_xlim((0, xlim))\n    \n    ax_2 = fig.add_subplot(122)\n    spec.plot_spec(ax = ax_2, db_off = True)\n    if xlim:\n        ax_2.set_xlim((0, xlim))\n    plt.show()\n\n\nplt_signal_spec(test_spec, xlim = 2000)\n\n\n\n\nThe figure on the left is the signal directly converted from raw audio file, which represents the magnitude/pressure of the recorded audio. Whereas the figure on the right is the corresponding converted spectrogram.\nWe will then add random noise to the signal, before converting to the spectrogram, the signal is added based on the normal distribution of the maximum value of the signal.\n\ntest_spec_noise = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\ntest_spec_noise.add_noise(noise_factor = 0.2)\n\n\nplt_signal_spec(test_spec_noise, xlim = 2000)\n\n\n\n\nNow there is some fuzziness going on in the signal! However, since the magnitude of the mel spectrogram is log scaled, it isnt obvious in the right figure.\n\nNote that the in practice, we dont add so much fuzziness in our training, the increased noise factor is for demonstration purpose only.\n\n\n\nSpectrogram masking\nAnother technique we used is the spectrogram masking, the masking function will randomly set the magnitude of spectrogram into 0, across the time and frequency of the spectrogram.\n\ntest_spec_mask = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\ntest_spec_mask.mask_spec()\n\n\nplt_signal_spec(test_spec_mask, xlim = 2000)\n\n\n\n\nNote how we have remove chunks of the frequency and time dimension of the spectrogram. If its not, run a few more time, its random anyway.\n\n\nSpectrogram shifting\nSince in our audio files, the recording always starts at the beginning, and there is always a few seconds of silence before the recording ends, we will need to shift the spectrogram, so that the machine learning models doesnt depends too much on the beginning of the time slices.\n\ntest_spec_shift = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\ntest_spec_shift.shift_spec(max_sec = 5)\n\n\nplt_signal_spec(test_spec_shift, xlim = 2000)\n\n\n\n\nNote the now the spectrogram is now shift to the right of time, we have set the maximum time it can shift for 5 seconds for demonstration purpose.\n\n\nSpectrogram truncation\nSince we have audio files of different length, we will need to preprocess it in a way that all of the sample have the same dimension as a numpy array. To do this we will simply be padding the numpy array as zeros, or trimming the last few seconds of the recording. The truncation is applied by default when the spectrogram class initialization is called.\nNow to get the size of our input (to be fed into the model), we wil simply run a sample, and acquire the shape from the numpy array.\n\n%%time\nsample = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', \n                    hop_length = hop_length, n_mels = 512,\n                    n_fft = n_fft)\n\nCPU times: user 143 ms, sys: 80 ms, total: 223 ms\nWall time: 325 ms\n\n\n\nspec_shape = sample.spec.shape\nspec_shape\n\n(512, 500)\n\n\nNow that we have the spectrogram ready for out training, we can use the instrument and pitch data in the metadata dataframe.\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\nmeta_df.head()\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n    \n      2\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#1\n      34.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      1\n    \n    \n      3\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#1\n      34.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      2\n    \n    \n      4\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#2\n      46.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      1\n    \n  \n\n\n\n\n\nmeta_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13265 entries, 0 to 13264\nData columns (total 17 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   Path                         13265 non-null  object \n 1   Family (abbr.)               13265 non-null  object \n 2   Family (in full)             13265 non-null  object \n 3   Instrument (abbr.)           13265 non-null  object \n 4   Instrument (in full)         13265 non-null  object \n 5   Technique (abbr.)            13265 non-null  object \n 6   Technique (in full)          13265 non-null  object \n 7   Pitch                        13265 non-null  object \n 8   Pitch ID (if applicable)     13162 non-null  float64\n 9   Dynamics                     13265 non-null  object \n 10  Dynamics ID (if applicable)  12646 non-null  float64\n 11  Instance ID                  13262 non-null  float64\n 12  Mute (abbr.)                 13265 non-null  object \n 13  Mute (in full)               13265 non-null  object \n 14  String ID (if applicable)    7516 non-null   float64\n 15  Needed digital retuning      13265 non-null  bool   \n 16  Fold                         13265 non-null  int64  \ndtypes: bool(1), float64(4), int64(1), object(11)\nmemory usage: 1.6+ MB\n\n\n\nmeta_df.describe()\n\n\n\n\n\n  \n    \n      \n      Pitch ID (if applicable)\n      Dynamics ID (if applicable)\n      Instance ID\n      String ID (if applicable)\n      Fold\n    \n  \n  \n    \n      count\n      13162.000000\n      12646.000000\n      13262.000000\n      7516.000000\n      13265.000000\n    \n    \n      mean\n      63.842653\n      2.073857\n      0.848138\n      2.360298\n      2.000000\n    \n    \n      std\n      16.512067\n      1.329919\n      1.177874\n      1.196041\n      1.414267\n    \n    \n      min\n      20.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n    \n    \n      25%\n      52.000000\n      2.000000\n      0.000000\n      1.000000\n      1.000000\n    \n    \n      50%\n      64.000000\n      2.000000\n      0.000000\n      2.000000\n      2.000000\n    \n    \n      75%\n      76.000000\n      3.000000\n      2.000000\n      3.000000\n      3.000000\n    \n    \n      max\n      109.000000\n      4.000000\n      12.000000\n      6.000000\n      4.000000\n    \n  \n\n\n\n\n\nmeta_df.isnull().sum()\n\nPath                              0\nFamily (abbr.)                    0\nFamily (in full)                  0\nInstrument (abbr.)                0\nInstrument (in full)              0\nTechnique (abbr.)                 0\nTechnique (in full)               0\nPitch                             0\nPitch ID (if applicable)        103\nDynamics                          0\nDynamics ID (if applicable)     619\nInstance ID                       3\nMute (abbr.)                      0\nMute (in full)                    0\nString ID (if applicable)      5749\nNeeded digital retuning           0\nFold                              0\ndtype: int64\n\n\n\nmeta_df['Instrument (in full)'].value_counts()\n\nViolin            1987\nViola             1952\nContrabass        1636\nCello             1593\nAccordion          872\nTrombone           670\nTrumpet in C       590\nFrench Horn        589\nFlute              529\nHarp               507\nBass Tuba          500\nClarinet in Bb     406\nAlto Saxophone     377\nBassoon            358\nGuitar             353\nOboe               346\nName: Instrument (in full), dtype: int64\n\n\nWe donreally care about the pitchID, Dynamics Id and String ID.\nWe can see some degree of bias in the instrument classes.\n\n\n\nModel fitting\n\ntrain_df, test_df = train_test_split(meta_df, train_size = 0.7, random_state = 42)\n\n\nprint('The number of rows for the training data is ', train_df.shape[0])\nprint('The number of rows for the test data is ', test_df.shape[0])\n\nThe number of rows for the training data is  9285\nThe number of rows for the test data is  3980\n\n\n\nBATCH_SIZE = 32\n\ntrain_generator = tf.data.Dataset.from_generator(lambda: spec_generator(train_df, BATCH_SIZE, \n                    add_channel = True, live_generation = True), \n                    output_types=(tf.float32, tf.int32),\n                    output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                    (BATCH_SIZE, 16))).prefetch(5)\neval_generator = tf.data.Dataset.from_generator(lambda: spec_generator(test_df, BATCH_SIZE, \n                    add_channel = True, live_generation = True,), \n                    output_types=(tf.float32, tf.int32), \n                    output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                    (BATCH_SIZE, 16))).prefetch(5)\n\n2022-08-08 01:38:09.491735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-08 01:38:09.493556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:09.494346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:09.495138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.299483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.299828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.300078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.300294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3370 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n\ntrain_generator\n\n<PrefetchDataset element_spec=(TensorSpec(shape=(32, 512, 500, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 16), dtype=tf.int32, name=None))>\n\n\n\nmodel = models.Sequential()\nmodel.add(layers.InputLayer((spec_shape[0], spec_shape[1], 1), batch_size = BATCH_SIZE, \n                                    dtype = tf.float32))\nmodel.add(layers.Conv2D(15, (15, 200), strides=(10, 10), activation='relu'))\n# , input_shape = (spec_shape[0], spec_shape[1], 1)))\nmodel.add(layers.MaxPool2D((5, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(16, activation = 'sigmoid'))\nmodel.build()\n\n\ntf.keras.utils.plot_model(model, show_shapes = True, show_dtype= True)\n\n\n\n\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\nprint(model.metrics)\n\n[]\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (32, 50, 31, 15)          45015     \n                                                                 \n max_pooling2d (MaxPooling2D  (32, 10, 15, 15)         0         \n )                                                               \n                                                                 \n flatten (Flatten)           (32, 2250)                0         \n                                                                 \n dense (Dense)               (32, 16)                  36016     \n                                                                 \n=================================================================\nTotal params: 81,031\nTrainable params: 81,031\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom datetime import datetime\ndatetime.now().strftime('%Y%m%d_%H%M%S')\n\n'20220808_013811'\n\n\n\ntrain_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      4851\n      Strings/Contrabass/pizzicato_bartok/Cb-pizz_ba...\n      Strings\n      Violin Family\n      Cb\n      Contrabass\n      pizz_bartok\n      pizzicato_bartok\n      F3\n      53.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      1.0\n      False\n      1\n    \n    \n      12565\n      Winds/Oboe+sordina/ordinario/Ob+S-ord-E6-mf-N-...\n      Winds\n      Woodwinds\n      Ob\n      Oboe\n      ord\n      ordinario\n      E6\n      88.0\n      mf\n      2.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      4\n    \n  \n\n\n\n\n\n%load_ext tensorboard\n\n\n# ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n#     f\"../models/baseline_checkpoint/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_model\", \n#                                                     monitor='val_accuracy')\n# early_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 2, \n#                                                         restore_best_weights = True)\n# log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n\n# history = model.fit(train_generator, epochs = 10, verbose=1, \n#                     validation_data = eval_generator, \n#                     validation_freq= 1, \n#                     # use_multiprocessing=True, workers = 2, \n#                     callbacks=[ckpt_callback, early_callback, tensorboard_callback])\n\n\n# model.save('../models/new_baseline/')\n\nNow that we have an working model, we need to save the model and the history object, we have defined a short checkpoint callback to save the model automatically, now lets define a function to save the history.\n\nimport pickle\n\ndef save_history(history, path):\n    with open(path, 'wb+') as f:\n        pickle.dump(history, f)\n        \ndef load_history(path):\n    with open(path, 'rb') as f:\n        return pickle.load(f)\n\n\n# save_history(history.history, '../models/new_baseline/history.pkl')\n\n\nhistory = load_history('../models/new_baseline/history.pkl')\n\n\nmodel = tf.keras.models.load_model('../models/new_baseline/')\n\n\nhistory.keys()\n\ndict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n\n\n\n\nModel evaluation\nIts time to check if our model is good! drum rolling\n\nplt.plot(history['val_accuracy'], label = 'Validation accuracy')\nplt.plot(history['accuracy'], label = 'Training accuracy')\nplt.title('Accuracy of baseline model after early callback')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nThe figure above shows the both the training and testing accuracy for our instrument classification. We can see that the testing accuracy starts to decrease after 2 epoch, and only peaks at 26% accuracy.\nOur current options are to apply dropout layer or regularizer for out current layers. However, due to the time constrant, we have decided to apply these technique on more complicated models, as this baseline model should only be serving the purpose of quick gauge of how well CNN can perform on out dataset.\nThis indicate the limitation of our fairly simple baseline model. To get an idea of how the classification if performed over class, lets have a look at the prediction across all classes.\nNow we are interested in the confusion matrix to represents the prediction for each class, due to gpu memory limitation, we will only be taking the snapshot of the test data as the classification report.\n\nprediction_generator = spec_generator(test_df, test_df.shape[0] // 5, add_channel=True,\n                                        live_generation = True, preprocess = False)\n\n\ninstrument_list = sorted(meta_df['Instrument (in full)'].unique())\ninstrument_list\n\n['Accordion',\n 'Alto Saxophone',\n 'Bass Tuba',\n 'Bassoon',\n 'Cello',\n 'Clarinet in Bb',\n 'Contrabass',\n 'Flute',\n 'French Horn',\n 'Guitar',\n 'Harp',\n 'Oboe',\n 'Trombone',\n 'Trumpet in C',\n 'Viola',\n 'Violin']\n\n\n\ndef orchidea_confusion_matrix(model, generator, instrument_list):\n    '''\n    Plot confusion matrix for OrchideaSOL dataset\n    \n    Input:\n    model: Model to be used\n    generator: Sequence class, generator to generate feature and labels for \n                OrchideaSOL dataset\n    instrument_list: list of instrument in alphabetical order, used to label the plot\n    \n    Output:\n    predict: np.array, Predicted output\n    prediction_label: np.array, True label\n    '''\n    prediction_feature, prediction_label = generator.__getitem__(0)\n    predict = predict = model.predict(prediction_feature)\n    assert prediction_label.shape == predict.shape\n    from sklearn.metrics import confusion_matrix\n\n    plt.figure(figsize = (12, 10))\n    sns.heatmap(confusion_matrix(np.argmax(prediction_label, axis=1), \n                        np.argmax(predict, axis = 1)), annot = True, \n                        xticklabels=instrument_list, \n                        yticklabels=instrument_list)\n    plt.title('Confusion matrix for baseline model')\n    plt.show()\n\n    return predict, prediction_label\n\n\npredict, prediction_label = orchidea_confusion_matrix(model, \n                                prediction_generator, instrument_list)\n\n25/25 [==============================] - 1s 23ms/step\n\n\n\n\n\nAccording to our baseline model, everything is a violin or viola, with the exception of good prediction on controbass. We will need to adjsut our loss function by assinging weighted entropy, or by creating more data for other classes using data augmentation.\nHowever, the last option is unfavourable, since it will increase the training time per epochs.\n\nprint(classification_report(np.argmax(prediction_label, axis=1), \n                        np.argmax(predict, axis = 1)))\n\n              precision    recall  f1-score   support\n\n           0       0.50      0.02      0.03        65\n           1       0.00      0.00      0.00        25\n           2       0.00      0.00      0.00        32\n           3       0.00      0.00      0.00        20\n           4       0.15      0.06      0.08        88\n           5       0.17      0.04      0.07        23\n           6       0.36      0.45      0.40        84\n           7       0.14      0.06      0.08        36\n           8       0.67      0.06      0.10        36\n           9       0.33      0.05      0.08        21\n          10       0.33      0.11      0.17        27\n          11       0.00      0.00      0.00        18\n          12       0.33      0.23      0.27        35\n          13       0.40      0.06      0.10        34\n          14       0.20      0.67      0.31       119\n          15       0.21      0.29      0.24       133\n\n    accuracy                           0.23       796\n   macro avg       0.24      0.13      0.12       796\nweighted avg       0.25      0.23      0.17       796\n\n\n\nThe precisio nnad recall is bad all across the board, with the exception of 67% recall on instrument 14 (viola)"
  },
  {
    "objectID": "posts/music/Music_trainscription_fastai/notebooks/test.html",
    "href": "posts/music/Music_trainscription_fastai/notebooks/test.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nfrom sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n\nfrom spec_generator_sequence import _get_spec, spec_generator\nfrom spec_generator_sequence_multilabel import (spec_generator_multi,\n                                                spec_generator_multioutput)\nfrom spec_input_generator import gen, gen_eval\nfrom spectrogram_class import spectrogram\nfrom classic_generator import classic_generator\n\n\nfrom classic_generator import _instrument_label_generator, get_full_path, classic_train_generator, classic_generator\n\n\nget_full_path('/train_data/2335.wav', mode='train')\n\n'/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav'\n\n\n\ny, sr = librosa.load('/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav', \n                        sr = 44100)\n\n\nplt.plot(y)\n\n\n\n\n\nmel_spec = librosa.feature.melspectrogram(y, n_mels = 128, sr = 44100)\n\n\nsns.heatmap(mel_spec)\n\n<AxesSubplot:>\n\n\n\n\n\n\nlibrosa.display.specshow(mel_spec)\n\n<matplotlib.collections.QuadMesh at 0x7fa87c058e20>\n\n\n\n\n\n\nclassic_generator_test = classic_generator(batch_size = 1)\n\n\nx_list = [path.rsplit('/', 1)[-1].rsplit('.')[0] for path in classic_generator_test.x]\ny_list = [path.rsplit('/', 1)[-1].rsplit('.')[0] for path in classic_generator_test.y]\n\n\nx_list == y_list\n\nTrue\n\n\n\nclassic_generator_test.y[0]\n\n'/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_labels/1727.csv'\n\n\n\nclassic_generator_test.x[122]\n\n'/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2366.wav'\n\n\n\n# classic_train_generator('/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav')\n\n\ntest_classic_gen = classic_generator_test.__getitem__(3)\n\n\ntest_classic_gen[0][0].shape\n\n(128, 200, 1)\n\n\n\ntest_classic_gen[1]['instrument_1'].shape\n\n(1, 200, 83)\n\n\n\nsns.heatmap(np.squeeze(test_classic_gen[0][0], -1))\n\n<AxesSubplot:>\n\n\n\n\n\n\ntest_classic_gen[0][0].shape\n\n(3769, 128, 1)\n\n\n\nsns.heatmap(test_classic_gen[1]['1'][1])\n\n\n%run classic_generator\n\n\ndef ger_instrument_frame(file, ins, num_freq, num_time):\n    _df = pd.read_csv(file)\n    _df = _df[_df['instrument'] == ins]\n    tmp_arr = np.zeros((num_freq, num_time))\n    for i in _df.iterrows():\n        start_time = i['start_time']\n\n/bin/bash: /home/shiya/anaconda3/envs/music/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/home/shiya/Documents/music_transcription/notebooks\n\n\n\ncwd = os.getcwd()\n\n\ncwd\n\n'/home/shiya/Documents/music_transcription/notebooks'\n\n\n\nos.path.join(os.getcwd(), '/../data/classic/musicnet/train_labels/1727.csv')\n\n'/../data/classic/musicnet/train_labels/1727.csv'\n\n\n\nlibrosa.get_duration(filename = '../data/classic/musicnet/train_data/1727.wav')\n\n447.0595918367347\n\n\n\n'../data/classic/musicnet/train_labels/1727.csv'.rsplit('/', maxsplit=1)\n\n['../data/classic/musicnet/train_labels', '1727.csv']\n\n\n\nos.path.join('../data/classic/musicnet/train_labels/', \n                '../train_data/', \n                '1727.wav')\n\n'../data/classic/musicnet/train_labels/../train_data/1727.wav'\n\n\n\ntest_inst_generator = _instrument_label_generator('../data/classic/musicnet/train_labels/1727.csv', 1, 9000, mode = 'train')\n\n\nsns.heatmap(test_inst_generator)\n\n<AxesSubplot:>\n\n\n\n\n\n\nclassic_label = pd.read_csv('../data/classic/musicnet/train_labels/1727.csv')\nclassic_label.head()\n\n\n\n\n\n  \n    \n      \n      start_time\n      end_time\n      instrument\n      note\n      start_beat\n      end_beat\n      note_value\n    \n  \n  \n    \n      0\n      9182\n      90078\n      43\n      53\n      4.0\n      1.5\n      Dotted Quarter\n    \n    \n      1\n      9182\n      33758\n      42\n      65\n      4.0\n      0.5\n      Eighth\n    \n    \n      2\n      9182\n      62430\n      1\n      69\n      4.0\n      1.0\n      Quarter\n    \n    \n      3\n      9182\n      202206\n      44\n      41\n      4.0\n      3.5\n      Whole\n    \n    \n      4\n      9182\n      62430\n      1\n      81\n      4.0\n      1.0\n      Quarter\n    \n  \n\n\n\n\n\nclassic_1_inst = classic_label[classic_label['instrument'] == 1]\nclassic_1_inst.head()\n\n\n\n\n\n  \n    \n      \n      start_time\n      end_time\n      instrument\n      note\n      start_beat\n      end_beat\n      note_value\n    \n  \n  \n    \n      2\n      9182\n      62430\n      1\n      69\n      4.0\n      1.0\n      Quarter\n    \n    \n      4\n      9182\n      62430\n      1\n      81\n      4.0\n      1.0\n      Quarter\n    \n    \n      7\n      62430\n      119774\n      1\n      84\n      5.0\n      1.0\n      Quarter\n    \n    \n      8\n      62430\n      119774\n      1\n      72\n      5.0\n      1.0\n      Quarter\n    \n    \n      11\n      119774\n      145886\n      1\n      74\n      6.0\n      0.5\n      Eighth\n    \n  \n\n\n\n\n\nfor i in classic_label.head(2).iterrows():\n    print(i)\n    print(type(i[1]))\n\n(0, start_time              9182\nend_time               90078\ninstrument                43\nnote                      53\nstart_beat               4.0\nend_beat                 1.5\nnote_value    Dotted Quarter\nName: 0, dtype: object)\n<class 'pandas.core.series.Series'>\n(1, start_time      9182\nend_time       33758\ninstrument        42\nnote              65\nstart_beat       4.0\nend_beat         0.5\nnote_value    Eighth\nName: 1, dtype: object)\n<class 'pandas.core.series.Series'>\n\n\n\nclassic_spec = spectrogram('../data/classic/musicnet/train_data/1727.wav', \n                            trunc_off=True)\n\n\nlabel_list = os.listdir('../data/classic/musicnet/train_labels/')\nlabel_list[:3]\n\n['2422.csv', '2114.csv', '2335.csv']\n\n\n\ndf_list = []\nfor i in label_list:\n    df_list.append(pd.read_csv('../data/classic/musicnet/train_labels/' + i))\nlabel_df = pd.concat(df_list)\nlabel_df.head(2)\n\n\n\n\n\n  \n    \n      \n      start_time\n      end_time\n      instrument\n      note\n      start_beat\n      end_beat\n      note_value\n    \n  \n  \n    \n      0\n      90078\n      124382\n      1\n      60\n      0.5\n      0.489583\n      Quarter\n    \n    \n      1\n      124382\n      138718\n      1\n      65\n      1.0\n      0.489583\n      Quarter\n    \n  \n\n\n\n\n\nlabel_df['instrument'].unique()\n\narray([ 1, 43, 41, 61, 71, 72, 74, 69, 42, 44,  7])\n\n\n\nlen(label_df['note'].unique())\n\n83\n\n\n\n19233758/ len(classic_spec.signal) * 9627\n\n9391.849238622863\n\n\n\n19421150/ len(classic_spec.signal) * 9627\n\n9483.352802956157\n\n\n\nclassic_spec.sr\n\n44100\n\n\n\nclassic_spec.spec.shape\n\n(256, 9627)\n\n\n\ny = np.array([['a', 'r'], ['b', 'q'], ['c', 'z']])\n\nnb = MultiLabelBinarizer()\nnb.fit(y)\n\nnb.transform(np.array([['a', np.nan], ['d']]))\n\narray([[1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0]])\n\n\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\n\n\nmultioutput_generator = spec_generator_multioutput(meta_df, 32)\n\n\nmultioutput_generator.__getitem__(2)[1][1].shape\n\nKeyError: 1\n\n\n\nmeta_df['Pitch ID (if applicable)'][meta_df['Pitch ID (if applicable)'].isna()]\n\n138     NaN\n142     NaN\n233     NaN\n234     NaN\n235     NaN\n         ..\n13027   NaN\n13028   NaN\n13029   NaN\n13030   NaN\n13043   NaN\nName: Pitch ID (if applicable), Length: 103, dtype: float64\n\n\n\ngenerator = spec_generator(meta_df, 32)\n\n\nprint(generator.indices)\n\n[9012 3451 6714 ... 9387 4120 7367]\n\n\n\n\n%run spec_generator_sequence_multilabel.py\n\n\ngenerator\n\n<spec_generator_sequence.spec_generator at 0x7f58509d07c0>\n\n\n\ngenerate_multi = spec_generator_multi(meta_df, 32)\n\n\ngenerate_multi.__getitem__(2)[1].shape\n\n(32, 107)\n\n\n\ngenerate_multi.\n\nSyntaxError: invalid syntax (446800042.py, line 1)\n\n\n\nimport random\n\n\n\n%%timeit\nrandom_num = random.randint(1, 50)\ngenerator.__getitem__(random_num)[1][1]\n\n40.6 ms  8.01 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n\n\n\ntest = pd.DataFrame({'test':['d', 'z', 'r', 'e', 'y']})\n\n\ntest\n\n\n\n\n\n  \n    \n      \n      test\n    \n  \n  \n    \n      0\n      d\n    \n    \n      1\n      z\n    \n    \n      2\n      r\n    \n    \n      3\n      e\n    \n    \n      4\n      y\n    \n  \n\n\n\n\n\nhot = OneHotEncoder(sparse=False)\nhot.fit_transform(test)\n\narray([[1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       [0., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0.]])\n\n\n\nhot.categories_\n\n[array(['d', 'e', 'r', 'y', 'z'], dtype=object)]\n\n\n\n(np.random.randint(0, 2, size=10000) == np.random.randint(0, 2, size=10000)).mean()\n\n0.4947\n\n\n\nhop_length = 2048\nwin_length = 512\nn_fft = 1024\n\n\nmeta_df['Path'].sample(1).values[0]\n\n'Brass/Trumpet_C+sordina_wah/flatterzunge_open/TpC+SW-flatt_open-G#3-mf-N-N.wav'\n\n\n\n%%timeit\npath = meta_df['Path'].sample(1).values[0]\ntest = _get_spec('Winds/Flute/ordinario/Fl-ord-D6-ff-N-T20d.wav', test_verbose=False)\n\n962 s  37.6 s per loop (mean  std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%run spectrogram_class.py\n\nAttributeError: 'spectrogram' object has no attribute 'spec'\n\n\n\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\n\nmeta_test = meta_df[['Instrument (in full)']]\n\n\none_hot = OneHotEncoder(sparse= False)\n\none_hot.fit_transform(meta_test)\n\narray([[0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.]])\n\n\n\nmeta_df['Path'][2:6].values\n\narray(['Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#1-f-N-T20u.wav',\n       'Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#1-p-N-T22u.wav',\n       'Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#2-f-N-T29u.wav',\n       'Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#2-p-N-T31u.wav'],\n      dtype=object)\n\n\n\none_hot.categories_\n\n[array(['Accordion', 'Alto Saxophone', 'Bass Tuba', 'Bassoon', 'Cello',\n        'Clarinet in Bb', 'Contrabass', 'Flute', 'French Horn', 'Guitar',\n        'Harp', 'Oboe', 'Trombone', 'Trumpet in C', 'Viola', 'Violin'],\n       dtype=object)]\n\n\n\nmeta_freq = 1/meta_df.groupby('Instrument (in full)')['Instrument (in full)'].transform('count')\n\n\nmeta_freq\n\n0        0.002000\n1        0.002000\n2        0.002000\n3        0.002000\n4        0.002000\n           ...   \n13260    0.002653\n13261    0.002653\n13262    0.002653\n13263    0.002653\n13264    0.002653\nName: Instrument (in full), Length: 13265, dtype: float64\n\n\n\n%%time\nmeta_df.sample(32, )\n            # replace = True, \n            # weights=meta_freq)[['Instrument (in full)']].value_counts(normalize=True)\n\nCPU times: user 1.75 ms, sys: 0 ns, total: 1.75 ms\nWall time: 1.54 ms\n\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      978\n      Brass/Horn/sforzato/Hn-sfz-F#2-f-N-N.wav\n      Brass\n      Brass\n      Hn\n      French Horn\n      sfz\n      sforzato\n      F#2\n      42.0\n      f\n      3.0\n      0.0\n      N\n      None\n      NaN\n      False\n      0\n    \n    \n      748\n      Brass/Horn/flatterzunge_stopped/Hn-flatt_stopp...\n      Brass\n      Brass\n      Hn\n      French Horn\n      flatt_stopped\n      flatterzunge_stopped\n      B3\n      59.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      4\n    \n    \n      7327\n      Strings/Viola/sul_tasto_tremolo/Va-tasto_trem-...\n      Strings\n      Violin Family\n      Va\n      Viola\n      tasto_trem\n      sul_tasto_tremolo\n      E5\n      76.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      True\n      0\n    \n    \n      2757\n      Keyboards/Accordion/ordinario/Acc-ord-D#4-mf-a...\n      Keyboards\n      Keyboards\n      Acc\n      Accordion\n      ord\n      ordinario\n      D#4\n      63.0\n      mf\n      2.0\n      4.0\n      N\n      None\n      NaN\n      False\n      4\n    \n    \n      3972\n      PluckedStrings/Harp/ordinario/Hp-ord-G#4-mf-N-...\n      PluckedStrings\n      Plucked Strings\n      Hp\n      Harp\n      ord\n      ordinario\n      G#4\n      68.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      4\n    \n    \n      7147\n      Strings/Viola/sul_ponticello/Va-pont-F#5-mf-3c...\n      Strings\n      Violin Family\n      Va\n      Viola\n      pont\n      sul_ponticello\n      F#5\n      78.0\n      mf\n      2.0\n      2.0\n      N\n      None\n      3.0\n      False\n      0\n    \n    \n      10638\n      Strings/Violoncello/pizzicato_secco/Vc-pizz_se...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      pizz_sec\n      pizzicato_secco\n      A3\n      57.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      1.0\n      False\n      0\n    \n    \n      7542\n      Strings/Viola/tremolo/Va-trem-D6-mf-1c-T12u.wav\n      Strings\n      Violin Family\n      Va\n      Viola\n      trem\n      tremolo\n      D6\n      86.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      True\n      3\n    \n    \n      7457\n      Strings/Viola/tremolo/Va-trem-C#5-pp-2c-N.wav\n      Strings\n      Violin Family\n      Va\n      Viola\n      trem\n      tremolo\n      C#5\n      73.0\n      pp\n      0.0\n      1.0\n      N\n      None\n      2.0\n      False\n      3\n    \n    \n      8171\n      Strings/Violin/artificial_harmonic/Vn-art_harm...\n      Strings\n      Violin Family\n      Vn\n      Violin\n      art_harm\n      artificial_harmonic\n      B7\n      107.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      False\n      3\n    \n    \n      12890\n      Winds/Sax_Alto/aeolian/ASax-aeol-A3-p-N-R100u.wav\n      Winds\n      Woodwinds\n      ASax\n      Alto Saxophone\n      aeol\n      aeolian\n      A3\n      57.0\n      p\n      1.0\n      0.0\n      N\n      None\n      NaN\n      True\n      2\n    \n    \n      482\n      Brass/Bass_Tuba/slap_pitched/BTb-slap-F#1-f-N-...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      slap\n      slap_pitched\n      F#1\n      30.0\n      f\n      3.0\n      0.0\n      N\n      None\n      NaN\n      False\n      0\n    \n    \n      9260\n      Strings/Violin/sul_ponticello/Vn-pont-A3-mf-4c...\n      Strings\n      Violin Family\n      Vn\n      Violin\n      pont\n      sul_ponticello\n      A3\n      57.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      0\n    \n    \n      2346\n      Brass/Trumpet_C/slap_pitched/TpC-slap-G#4-p-N-...\n      Brass\n      Brass\n      TpC\n      Trumpet in C\n      slap\n      slap_pitched\n      G#4\n      68.0\n      p\n      1.0\n      0.0\n      N\n      None\n      NaN\n      False\n      1\n    \n    \n      3475\n      PluckedStrings/Guitar/ordinario_high_register/...\n      PluckedStrings\n      Plucked Strings\n      Gtr\n      Guitar\n      ord_hi_reg\n      ordinario_high_register\n      D6\n      86.0\n      mf\n      2.0\n      1.0\n      N\n      None\n      2.0\n      False\n      2\n    \n    \n      3089\n      Keyboards/Accordion/ordinario/Acc-ord-G1-pp-N-...\n      Keyboards\n      Keyboards\n      Acc\n      Accordion\n      ord\n      ordinario\n      G1\n      31.0\n      pp\n      0.0\n      0.0\n      N\n      None\n      NaN\n      False\n      0\n    \n    \n      8444\n      Strings/Violin/ordinario/Vn-ord-B4-mf-4c-N.wav\n      Strings\n      Violin Family\n      Vn\n      Violin\n      ord\n      ordinario\n      B4\n      71.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      1\n    \n    \n      5873\n      Strings/Viola+sordina/ordinario/Va+S-ord-G3-mf...\n      Strings\n      Violin Family\n      Va\n      Viola\n      ord\n      ordinario\n      G3\n      55.0\n      mf\n      2.0\n      3.0\n      S\n      Sordina\n      4.0\n      True\n      1\n    \n    \n      6683\n      Strings/Viola/pizzicato_l_vib/Va-pizz_lv-C#4-m...\n      Strings\n      Violin Family\n      Va\n      Viola\n      pizz_lv\n      pizzicato_l_vib\n      C#4\n      61.0\n      mf\n      2.0\n      2.0\n      N\n      None\n      3.0\n      False\n      2\n    \n    \n      9961\n      Strings/Violoncello+sordina_piombo/tremolo/Vc+...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      trem\n      tremolo\n      G#4\n      68.0\n      mf\n      2.0\n      0.0\n      SP\n      Piombo\n      1.0\n      False\n      0\n    \n    \n      1317\n      Brass/Trombone+sordina_wah/flatterzunge_open/T...\n      Brass\n      Brass\n      Tbn\n      Trombone\n      flatt_open\n      flatterzunge_open\n      B3\n      59.0\n      mf\n      2.0\n      0.0\n      SW\n      Wah\n      NaN\n      False\n      1\n    \n    \n      10597\n      Strings/Violoncello/pizzicato_l_vib/Vc-pizz_lv...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      pizz_lv\n      pizzicato_l_vib\n      F2\n      41.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      3\n    \n    \n      715\n      Brass/Horn/flatterzunge/Hn-flatt-F3-mf-N-N.wav\n      Brass\n      Brass\n      Hn\n      French Horn\n      flatt\n      flatterzunge\n      F3\n      53.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      2\n    \n    \n      6528\n      Strings/Viola/ordinario/Va-ord-G#3-pp-4c-T18u.wav\n      Strings\n      Violin Family\n      Va\n      Viola\n      ord\n      ordinario\n      G#3\n      56.0\n      pp\n      0.0\n      3.0\n      N\n      None\n      4.0\n      True\n      2\n    \n    \n      5542\n      Strings/Contrabass/tremolo/Cb-trem-B3-pp-1c-T1...\n      Strings\n      Violin Family\n      Cb\n      Contrabass\n      trem\n      tremolo\n      B3\n      59.0\n      pp\n      0.0\n      0.0\n      N\n      None\n      1.0\n      True\n      1\n    \n    \n      11158\n      Strings/Violoncello/tremolo/Vc-trem-D4-pp-2c-N...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      trem\n      tremolo\n      D4\n      62.0\n      pp\n      0.0\n      1.0\n      N\n      None\n      2.0\n      False\n      4\n    \n    \n      4033\n      PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_b...\n      PluckedStrings\n      Plucked Strings\n      Hp\n      Harp\n      pizz_bartok\n      pizzicato_bartok\n      D#2\n      39.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      NaN\n      True\n      0\n    \n    \n      10073\n      Strings/Violoncello/col_legno_battuto/Vc-legno...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      legno_batt\n      col_legno_battuto\n      C#2\n      37.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      2\n    \n    \n      12233\n      Winds/Flute/flatterzunge/Fl-flatt-D#6-ff-N-N.wav\n      Winds\n      Woodwinds\n      Fl\n      Flute\n      flatt\n      flatterzunge\n      D#6\n      87.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      NaN\n      False\n      1\n    \n    \n      11602\n      Winds/Bassoon/vibrato/Bn-vib-G#3-mf-N-N.wav\n      Winds\n      Woodwinds\n      Bn\n      Bassoon\n      vib\n      vibrato\n      G#3\n      56.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      2\n    \n    \n      10654\n      Strings/Violoncello/pizzicato_secco/Vc-pizz_se...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      pizz_sec\n      pizzicato_secco\n      B4\n      71.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      False\n      0\n    \n    \n      3546\n      PluckedStrings/Guitar/sul_ponticello/Gtr-pont-...\n      PluckedStrings\n      Plucked Strings\n      Gtr\n      Guitar\n      pont\n      sul_ponticello\n      C4\n      60.0\n      mf\n      2.0\n      1.0\n      N\n      None\n      2.0\n      False\n      3\n    \n  \n\n\n\n\n\nmeta_df.sample(1)['Path'].values[0]\n\n'Strings/Viola/ordinario/Va-ord-A4-ff-3c-R100d.wav'\n\n\n\ntest_spec = spectrogram(meta_df.sample(1)['Path'].values[0])\n\n\ntest_spec.plot_spec()\n\n\n\n\n\ntest, _ = librosa.load('../data/_OrchideaSOL2020_release/OrchideaSOL2020/PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', \n                    sr = None)\n\n\ntest.shape\n\n(826215,)\n\n\n\ndef mask_spec(arr, inplace = False):\n    loop = random.randint(1, 2)\n    tmp = arr.copy()\n    for i in range(loop):\n        start = random.randint(0, arr.shape[1])\n        duration = random.randint(25, 60)\n        if inplace == True:\n            arr[:, start:start + duration] = 0\n        else:\n            tmp[:, start:start+duration] = 0\n    freq_loop = random.randint(1, 3)\n    for freq in range(freq_loop):\n        start = random.randint(0, arr.shape[0])\n        duration = random.randint(25, 60)\n        if inplace == True:\n            arr[start:start + duration, :] = 0\n        else:\n            tmp[start:start + duration, :] = 0\n\n    return None if inplace == True else tmp\n\n\n# librosa.display.specshow(librosa.amplitude_to_db(mask_spec(spec_sample)), y_axis='log', x_axis = 's')\n\n\nimport random\nprint(random.randint(0, 9))\n\n9\n\n\n\nmeta_df['_ins'] = meta_df['Instrument (in full)']\n\n\nprint(random.randint.__doc__)\n\nReturn random integer in range [a, b], including both end points.\n        \n\n\n\ntest, _ = next(gen(meta_df, test_verbose = True))\n\nHIT\nSUCCESS\n\n\n\nprint(test)\n\n[[[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n ...\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]]\n\n\n\nnp.load('/home/shiya/Documents/Music_transcription_fastai/data/_OrchideaSOL2020_release/OrchideaSOL2020/Strings/Violin/ordinario/Vn-ord-A3-ff-4c-N.npy', \n            allow_pickle=True)\n\nFileNotFoundError: [Errno 2] No such file or directory: '/home/shiya/Documents/Music_transcription_fastai/data/_OrchideaSOL2020_release/OrchideaSOL2020/Strings/Violin/ordinario/Vn-ord-A3-ff-4c-N.npy'\n\n\n\n0 in test\n\nTrue\n\n\n\nprint(test.shape)\n\n(256, 500, 1)\n\n\n\nlibrosa.display.specshow(librosa.amplitude_to_db(np.reshape(test, newshape = test.shape[:2])), x_axis = 's', \n                                                y_axis = 'mel', sr=44100, hop_length=2048, \n                                                n_fft=2048)\n\n<matplotlib.collections.QuadMesh at 0x7f7700c99760>\n\n\n\n\n\n\nsample = meta_df.sample(1)\n\n\nsample['Path'].values\n\narray(['Strings/Violoncello+sordina_piombo/ordinario/Vc+SP-ord-D3-mf-2c-N.wav'],\n      dtype=object)\n\n\n\nspec = spectrogram(sample['Path'].values[0])\n\nAttributeError: 'spectrogram' object has no attribute 'spec'\n\n\n\nsample['Path']\n\n8462    Strings/Violin/ordinario/Vn-ord-C#4-pp-4c-N.wav\nName: Path, dtype: object\n\n\n\nnp.save('testnig.npy', spec.spec)\n\n\n!ls\n\n/bin/bash: /home/shiya/anaconda3/envs/music/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n__init__.py          spectrogram_class.py\nmodel.png            spectrogram.py\nmusic_transcription_2conv.ipynb  test.ipynb\nmusic_transcription_class.ipynb  testnig.npy\nmusic_transcription.ipynb    wav_converter_class.py\n__pycache__          wav_converter.py\nspec_input_generator.py\n\n\n\nload_test = np.load('testnig.npy', allow_pickle = True)\n\n\nload_test\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nload_test.shape\n\n()\n\n\n\nsample_path = meta_df.sample(1)['Path'].values[0]\nsample_path\n\n'Strings/Violin/ordinario/Vn-ord-A4-ff-2c-N.wav'\n\n\n\ntest_sample = spectrogram(sample_path, \n                preprocess = False, trunc_off = True)\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntest_sample.add_noise()\n\n\ntest_sample.generate_spec()\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntest_sample.mask_spec()\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntest_sample.shift_spec()\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntesttest.plot_spec()\n\nNameError: name 'testtest' is not defined"
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/EDA.html",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/EDA.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\n\nfrom spectrogram_class import spectrogram"
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/EDA.html#meta-data-orchideasol",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/EDA.html#meta-data-orchideasol",
    "title": "Most interesting blog on Earth!",
    "section": "Meta data (OrchideaSOL)",
    "text": "Meta data (OrchideaSOL)\nThe main dataset, other than the raw audio files, we are going to explore is the metadata dataframe. Lets first explore the Orchetral dataset.\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\n\nmeta_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13265 entries, 0 to 13264\nData columns (total 17 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   Path                         13265 non-null  object \n 1   Family (abbr.)               13265 non-null  object \n 2   Family (in full)             13265 non-null  object \n 3   Instrument (abbr.)           13265 non-null  object \n 4   Instrument (in full)         13265 non-null  object \n 5   Technique (abbr.)            13265 non-null  object \n 6   Technique (in full)          13265 non-null  object \n 7   Pitch                        13265 non-null  object \n 8   Pitch ID (if applicable)     13162 non-null  float64\n 9   Dynamics                     13265 non-null  object \n 10  Dynamics ID (if applicable)  12646 non-null  float64\n 11  Instance ID                  13262 non-null  float64\n 12  Mute (abbr.)                 13265 non-null  object \n 13  Mute (in full)               13265 non-null  object \n 14  String ID (if applicable)    7516 non-null   float64\n 15  Needed digital retuning      13265 non-null  bool   \n 16  Fold                         13265 non-null  int64  \ndtypes: bool(1), float64(4), int64(1), object(11)\nmemory usage: 1.6+ MB\n\n\n\nmeta_df.nunique().sort_values()\n\nNeeded digital retuning            2\nFold                               5\nFamily (abbr.)                     5\nFamily (in full)                   5\nDynamics ID (if applicable)        5\nString ID (if applicable)          6\nDynamics                           7\nMute (abbr.)                       7\nMute (in full)                     7\nInstance ID                       13\nInstrument (abbr.)                16\nInstrument (in full)              16\nTechnique (in full)               52\nTechnique (abbr.)                 56\nPitch ID (if applicable)          90\nPitch                            105\nPath                           13265\ndtype: int64\n\n\nThe Series above represents the number of unique values across all features. From the object types, unique values and intuition, we can conclude that all bu the path to the audio files are categorical features.\n\n# Check for null values\nmeta_df.isna().sum()\n\nPath                              0\nFamily (abbr.)                    0\nFamily (in full)                  0\nInstrument (abbr.)                0\nInstrument (in full)              0\nTechnique (abbr.)                 0\nTechnique (in full)               0\nPitch                             0\nPitch ID (if applicable)        103\nDynamics                          0\nDynamics ID (if applicable)     619\nInstance ID                       3\nMute (abbr.)                      0\nMute (in full)                    0\nString ID (if applicable)      5749\nNeeded digital retuning           0\nFold                              0\ndtype: int64\n\n\nThe only columns we are interested in are the Path, Instrument and Pitch. We can see that out of all three, only Pitch ID contains numm values. Now lets take a look at what the null values are, represented in pitch.\n\nmeta_df['Pitch'].unique()\n\narray(['A#0', 'A#1', 'A#2', 'A#3', 'A#4', 'A0', 'A1', 'A2', 'A3', 'A4',\n       'B0', 'B1', 'B2', 'B3', 'C#1', 'C#2', 'C#3', 'C#4', 'C1', 'C2',\n       'C3', 'C4', 'D#1', 'D#2', 'D#3', 'D#4', 'D1', 'D2', 'D3', 'D4',\n       'E1', 'E2', 'E3', 'E4', 'F#1', 'F#2', 'F#3', 'F#4', 'F1', 'F2',\n       'F3', 'F4', 'G#0', 'G#1', 'G#2', 'G#3', 'G#4', 'G1', 'G2', 'G3',\n       'G4', 'N', 'C#1_D#1', 'C1_C#1', 'C1_G1', 'D1_D#1', 'D1_F1', 'B4',\n       'C#5', 'C5', 'D#5', 'D5', 'E5', 'F5', 'A#5', 'A5', 'B5', 'F#5',\n       'G#5', 'G5', 'C#6', 'C6', 'D#6', 'D6', 'E6', 'F#6', 'F6', 'G6',\n       'A#6', 'A#7', 'A6', 'A7', 'B6', 'B7', 'C#7', 'C#8', 'C7', 'C8',\n       'D#7', 'D7', 'E7', 'F#7', 'F7', 'G#6', 'G#7', 'G7', 'C#3_B5',\n       'C#3_C#4', 'C#3_C#5', 'C#3_C#6', 'C#3_D#6', 'C#3_F5', 'C#3_F6',\n       'C#3_G#4', 'C#3_G#5'], dtype=object)\n\n\n\n# Getting the unique values of Pitch where the Pitch ID are missing.\nmeta_df[['Pitch', 'Pitch ID (if applicable)']] \\\n            [meta_df['Pitch ID (if applicable)'].isnull()]['Pitch'].unique()\n\narray(['N', 'C#1_D#1', 'C1_C#1', 'C1_G1', 'D1_D#1', 'D1_F1', 'C#3_B5',\n       'C#3_C#4', 'C#3_C#5', 'C#3_C#6', 'C#3_D#6', 'C#3_F5', 'C#3_F6',\n       'C#3_G#4', 'C#3_G#5'], dtype=object)\n\n\nIf you are not familiar with music, let me give you some insight (I am either grade 5 or 6 on piano ABRSM, cant remember which since I lost my certificate):\n\n\nNone of the terms make sense to me!\n\n\nThere is no note N in music, nor can you be C1 and C# at the same time! The documentation on OrchideaSOL doesnt include any useful information on the subject either. The only logical way to deal with the null values is to simple drop them.\nTo justify the decision, lets look at the precentage of the missing values.\n\n# Getting the fractiong of missing values in the Pitch ID feature.\nmeta_df['Pitch ID (if applicable)'].isnull().sum()/meta_df.shape[0]\n\n0.007764794572182435\n\n\nIts not even 1% of our data, we dont lose much training information from losing these unexplanable samples notes."
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/EDA.html#orchideasol-instrument-analysis",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/EDA.html#orchideasol-instrument-analysis",
    "title": "Most interesting blog on Earth!",
    "section": "OrchideaSOL instrument analysis",
    "text": "OrchideaSOL instrument analysis\nNow that we have an understading of the note missing values, lets have a look at the instruments distribution.\n\norchidea_instrument = meta_df['Instrument (in full)'].value_counts(normalize=True)\norchidea_instrument\n\nViolin            0.149793\nViola             0.147154\nContrabass        0.123332\nCello             0.120090\nAccordion         0.065737\nTrombone          0.050509\nTrumpet in C      0.044478\nFrench Horn       0.044403\nFlute             0.039879\nHarp              0.038221\nBass Tuba         0.037693\nClarinet in Bb    0.030607\nAlto Saxophone    0.028421\nBassoon           0.026988\nGuitar            0.026611\nOboe              0.026084\nName: Instrument (in full), dtype: float64\n\n\n\nplt.figure(figsize = (18, 8))\nplt.bar(orchidea_instrument.index, orchidea_instrument.values)\nplt.title('Instrument distrubution of OrchideaSOL')\nplt.xlabel('Instruments')\nplt.ylabel('Fractions')\nplt.show()\n\n\n\n\nWe can see that violin, viola, contrabass and cello has more entries comparing to the rest of the instruments.\nSince we have some unbalanced data, we can either comtemplate the bias through adjusting the loss function (increasing the penalty for misclassifying minor classes), or creating more data for the minorities through audio file augmentation, more on that in the other notebooks!"
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/EDA.html#orchideasol-instrumentnotes-analysis",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/EDA.html#orchideasol-instrumentnotes-analysis",
    "title": "Most interesting blog on Earth!",
    "section": "OrchideaSOL instrument/notes analysis",
    "text": "OrchideaSOL instrument/notes analysis\nNow that we have a sense of how the instruments distribution looks like, it is also useful to look at the distribution of the instruments corresponding to the notes.\n\n# Creating crosstab for instrument and Pitch ID, \n# which represents the value counts for all instrument/Pitch ID combinations\n\nplt.figure(figsize = (12, 10))\nsns.heatmap(pd.crosstab(meta_df['Instrument (in full)'], \n                                meta_df['Pitch ID (if applicable)']))\n\n<AxesSubplot:xlabel='Pitch ID (if applicable)', ylabel='Instrument (in full)'>\n\n\n\n\n\nWe can see that the majority of notes for viola and violin distributed in the middle of the note range. Wheareas Cello and Contrabass has distribution shift towards the lower ends of the range.\nIt is also useful to note that: * Accoridian, guitar and harp has lower number of note counts, * However, their notes are more evenly distributed across the note range."
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/classic_transcription.html",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/classic_transcription.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import librosa\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom keras.layers import LSTM, ConvLSTM1D, Input, MaxPool1D\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\nfrom keras.layers.core import Dense, Dropout, Flatten\nfrom keras.models import Model, Sequential\nfrom librosa import display\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom classic_generator import classic_generator\nfrom spectrogram_class import spectrogram\n\nModuleNotFoundError: No module named 'classic_generator'\n\n\n\n!pwd\n\n/Users/ylee/Downloads\n\n\n\nimport pickle\n\ndef save_history(history, path):\n    with open(path, 'wb+') as f:\n        pickle.dump(history, f)\n        \ndef load_history(path):\n    with open(path, 'rb') as f:\n        return pickle.load(f)\n\n\nMusic transcription - MusicNet LSTM\nThis is a successive notebook from RNN models for OrchideaSOL dataset. In that notebook, we have demonstrate that the RNN model, in particularly LSTM, is able to capture the features of instrument and pitch in a raw audio file.\nThe purpose of this notebook is to apply the same principle on the more complicated MusicNet dataset. On our previous notebook, we have had mild success of predicting more than 50% in both insturment and notes, for short single instrumental audio files.\nAlthough it is the same principle, this time we are going to need to convert the whole sequence of audio file into the transcription in to the same music file length. To be more clear:\n\nFor all timesteps in the spectrogram, we are going to produce the classification of instruments and note.\n\nThe end goal of this project is to convert the corresponding output into a MIDI file.\nThe model architecture will be to be fine tuned since we are facing the challenges below:\n\nWe are making classifying prediction for every timestep,\n\nExcluding the expected dimension of time and notes as output, we also need the same classification for every instruments, which is 11 of them.\nThe audio files are not neccessarily made of single instrument, which means that our RNN model will need to find the relation of the sound signature for each instruments, in the sea of spectrograms magnitude.\nThe audio files has different length, range from 3 minutes to 20 minutes. Padding will be required, however, zeros padding will cause problem of exploding/vanishing gradient in RNN model.\n\n\n# Testing if GPU is enabled, if it's not, I am not running this notebook!\ntf.config.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\nimport tensorflow as tf\ntf.test.is_built_with_cuda()\ntf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n\nWARNING:tensorflow:From /var/folders/n9/gg0_718d4db65yyqhxlk09vh0000gp/T/ipykernel_38191/1822807733.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\nMetal device set to: Apple M1\n\nsystemMemory: 8.00 GB\nmaxCacheSize: 2.67 GB\n\n\n\n2022-09-16 16:43:02.967559: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-16 16:43:02.968025: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\nTrue\n\n\n\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n\n[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 3715140090454835554\nxla_global_id: -1\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nlocality {\n  bus_id: 1\n}\nincarnation: 11023430109333998903\nphysical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\nxla_global_id: -1\n]\n\n\n2022-09-16 16:43:03.107869: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-16 16:43:03.107888: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\nThroughout this notebook, we will be using integer code number for instrument and notes classification. The instrument and note lists below are generated by concatenating the training labels, which are seperated csv files.\n\ninstrument_list = [1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\nnote_list = [21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62,\n             63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]\n\n\n\nRNN-LSTM\nTo begin our construction of RNN model, we introduce the architect idea for our model.\nSince our model has to output the MIDI file equivalence for each instruments, we will be forking our output into 11 Dense layers, with each layer reprenting one instruments.\nEach output will have a dimension of : \\[\n\\text{\\# of instruments} \\times (\\text{\\# of timesteps} \\times \\text{\\# of notes})\n\\]\nThe architect idea for the model is as follow: 1. Passing input spectrogrom into convolutional layers, in order to extract features 1. Passing features in to multiple layers of Dense layers, to embed the features 1. Passing embedded features into LSTM model, to predict labels for each timestep depending on the sequnce of embedded features.\n\nUPDATE:\nAfter countless time wasted on differnet architect and changing parameter, we have decided to change the input shape, instead of feeding the batch of spectrogram which are padded with the longest time dimension across the batch, we will be taken random snipshot of fixed time length across all batch. This workaround is to prevent:\n* Long padding of zeros for shorter audio files, to prevent vanishing gradient * Long training time and * Reduce GPU memory usage (VsCode tends to close unexpectedly and doesnt release allocated GPU space before closing, making a full system restart required.)\nWe have decided to take 200 timeslices for the model input, with our default hop length (number of timestep to skip when computing fast-fourier transform for spectrogram), and using the sample rate of audio, which is 44100HZ, we can calculate that:\n\\[\n\\text{Length in second of inputs} = \\frac{300 \\times 200}{44100} \\\\\n\\approx 16 \\, \\text{seconds}\n\\]\n\ndef instrument_layer_simple(input, out_name):\n    '''\n    Support function from building multiple output models. Returns an output layer\n    of TimeDistributed Dense layer, corresponds to the number of notes in labels\n    \n    Input:\n    input: Preceeding input layer to be fed into.\n    out_name: str, name of output layer names to be assigned\n    '''\n    out = layers.TimeDistributed(Dense(len(note_list), activation='sigmoid'),\n                                 name=out_name)(input)\n    return out\n\n\ninstrument_list\n\n[1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\nNow we can start to build our (hopefully will be working this time) model.\n\nBATCH_SIZE = 8\n\n# We have choose to work with 128 frequency bins,\n# and the None input shape indicates the dynamic length of time dimension\ninp = Input((None, 128, 1), batch_size=BATCH_SIZE)\nnormalizer = layers.BatchNormalization()(inp)\n\n# Extracting features from first convolutional layers.\n# Padding set to same to maintain same time dimension\nconv_1 = Conv2D(10, (50, 40), padding = 'same')(normalizer)\n\nnormalizer_2 = layers.BatchNormalization()(conv_1)\n# pool_1 = layers.TimeDistributed(MaxPool1D(2))(normalizer_2)\n# flatten_1 = flatten = layers.TimeDistributed(layers.Flatten())(pool_1)\n\n# conv_2 = layers.Conv2D(5, (1000, 30), padding = 'same')(pool_1)\n# pool_2 = layers.TimeDistributed(layers.MaxPool1D(2))(conv_2)\n\n# Flatten each filter layer for each timesteps\nflatten = layers.TimeDistributed(Flatten())(normalizer_2)\n\n# Feed in the flattened layers to multiple layers of Dense layer,\n# Performing embedding\nDense_1 = layers.TimeDistributed(Dense(300, activation = 'relu'))(flatten)\nnormalizer_3 = layers.BatchNormalization()(Dense_1)\ndrop_1 = layers.Dropout(0.2)(normalizer_3)\n\nDense_2 = layers.TimeDistributed(Dense(150, activation = 'relu'))(drop_1)\nnormalizer_4 = layers.BatchNormalization()(Dense_2)\ndrop_2 = layers.Dropout(0.2)(normalizer_4)\n\nDense_3 = layers.TimeDistributed(Dense(50, activation = 'relu'))(drop_2)\nnormalizer_5 = layers.BatchNormalization()(Dense_3)\ndrop_3 = layers.Dropout(0.2)(normalizer_5)\n\nDense_3 = layers.TimeDistributed(Dense(200, activation = 'relu'))(drop_2)\nnormalizer_5 = layers.BatchNormalization()(Dense_3)\ndrop_3 = layers.Dropout(0.2)(normalizer_5)\n\n# Lastly, put the decoded features in to LSTM layer\nlast_lstm = LSTM(500, return_sequences=True, dropout = 0.3)(drop_3)\n\n# Outputing to final Dense layer with sigmoid activation,\n# To predict the note labels for each timesteps\nsimple_lstm_model = Model(inp, [instrument_layer_simple(last_lstm,\n                                f\"instrument_{ins}\") for ins in instrument_list])\n\n\n2022-09-16 16:43:04.148904: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-16 16:43:04.148928: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\n\n\ntf.keras.utils.plot_model(simple_lstm_model, show_shapes=True, show_dtype=True)\n\n\n\n\n\nclassic_train_generator =  classic_generator(mode='train', batch_size=BATCH_SIZE)\n                        # (tf.dtypes.float32, tf.dtypes.bool))\n\nclassic_eval_generator = classic_generator(mode='test', batch_size=BATCH_SIZE)\n\n\n# Getting the path to the latest trained model\nnewest_ckpt = !ls -dt $PWD/../models/classic_truc_conv_to_lstm/* | head -1\nnewest_ckpt = newest_ckpt[0]\nnewest_ckpt\n\n'/Users/ylee/Documents/Music_transcription_fastai/notebooks/../models/classic_truc_conv_to_lstm/20220916_135022_02_classic_truc_conv_to_lstm'\n\n\n\n# Run the following code to load the newest model\n\nsimple_lstm_model = tf.keras.models.load_model(newest_ckpt, compile=False)\n\nSince our prediction will be a really sparse metrics, with a few 1s, we will need to define our custom loss function such that the false negatives are hugely reduced. We will be using the weighted_cross_entropy_with_logits in tensorflow, and setting positive weight which is larger than 1.\n\ndef weighted_cross_entropy_with_logits(labels, logits):\n    loss = tf.nn.weighted_cross_entropy_with_logits(\n        labels, logits, pos_weight = 150\n        )\n    return loss\n\ndef my_loss():\n    return weighted_cross_entropy_with_logits\n\n# As mentioned before, since our prediction is a sparse multilabel problem, \n# the accuracy might not makes muc of sense, in addition, we will be adding\n# AUC for each instrument to gauge how well the model is performing\nsimple_lstm_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = 0.0005),\n                          loss={f\"instrument_{ins}\": my_loss()\n                                for ins in instrument_list},\n                          metrics=['accuracy', tf.keras.metrics.AUC()])\n\nNameError: name 'simple_lstm_model' is not defined\n\n\n\nfrom datetime import datetime\n\n# Define checkpoint to save model for every epoch\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    f\"../models/classic_truc_conv_to_lstm/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_classic_truc_conv_to_lstm\",\n    monitor='val_accuracy',\n    save_freq='epoch')\n    \n# Define checkpoint to save model if validation loss is decreasing\nearly_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)\n\n\nsimple_lstm_history = simple_lstm_model.fit(classic_train_generator, \n                                                epochs=5,\n                                                validation_data=classic_eval_generator,\n                                                validation_freq=1,\n                                                # use_multiprocessing= True,\n                                                # workers= 3,\n                                                verbose=1,\n                                                callbacks=[ckpt_callback])\n\nNameError: name 'tf' is not defined\n\n\n\n# simple_lstm_model.save('../models/classic_full_convlstm/')\n\nSince we have multiple output with 2 dimensional output, it is easier to use visualization to gauge how well the model is performing.\n\ndef musicnet_eval(model, generator, ins):\n    '''\n    Supporting function to  plot the predicted label and true labels\n    of for specified instrument.\n    \n    Input:\n    model: model to be used\n    generator: testing Sequence generator to be used\n    ins: int, instrument number, ranged from 0 to 10\n    '''\n    _feature, _label = generator.__getitem__(0)\n    _prediction = model.predict(_feature)\n    fig = plt.figure(figsize= (18, 6))\n\n    ax_1 = fig.add_subplot(121)\n    ax_1.set_title('Prediction')\n    sns.heatmap(_prediction[ins][0], ax = ax_1)\n\n    ax_2 = fig.add_subplot(122)\n    sns.heatmap(_label[f\"instrument_{instrument_list[ins]}\"][0], ax = ax_2)\n    ax_2.set_title('True label')\n\n    plt.xlabel('Frequency bins')\n    plt.ylabel('Timesteps')\n\n    plt.tight_layout()\n    plt.show()\n\n\ntest_generator = classic_generator(mode='test', batch_size=1)\n\n\nmusicnet_eval(simple_lstm_model, test_generator, 4)\n\n1/1 [==============================] - 2s 2s/step\n\n\n\n\n\n\ntest_set = test_generator.__getitem__(2)\n\n\ntest_set[0].shape\n\n(1, 200, 128, 1)\n\n\n\npredict_test = simple_lstm_model.predict(test_set[0])\n\n1/1 [==============================] - 0s 25ms/step\n\n\n\ninstrument_list\n\n[1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\n\nsns.heatmap(test_set[1]['instrument_41'][0])\n\n<AxesSubplot:>\n\n\n\n\n\n\ninstrument_list\n\n[1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n\n\nThe problem with this model is that the model is predicting contant value along timesteps. The model focused and retained the sequential relation across timesteps too much. And eventually learned to predict zeros for all scenarios. Look at figure below for a better visualization.\n\n\n\n2 lstm\nOur first model produced unsatisfactory results, which implies we might have to start from a simpler model, we will be starting again at a model with 2 LSTM layers.\n\nBATCH_SIZE = 16\n\ninp = Input((None, 128), batch_size=BATCH_SIZE)\n\n# 2 LSTM layers\nx = LSTM(500, return_sequences=True, dropout = 0.3, stateful = True)(inp)\nx = LSTM(500, return_sequences=True, dropout = 0.3, stateful = True)(x)\n\nlstm_2 = Model(inp, [instrument_layer_simple(x,\n                                f\"instrument_{ins}\") for ins in instrument_list])\n\n\ntf.keras.utils.plot_model(lstm_2, show_shapes=True, show_dtype=True)\n\n\n\n\n\nlstm2_train_generator =  classic_generator(mode='train', batch_size=BATCH_SIZE, expand_dim = False)\n                        # (tf.dtypes.float32, tf.dtypes.bool))\n\nlstm2_eval_generator = classic_generator(mode='test', batch_size=BATCH_SIZE, expand_dim = False, \n                                            preprocess = False)\n\n\ndef weighted_cross_entropy_with_logits(labels, logits):\n    loss = tf.nn.weighted_cross_entropy_with_logits(\n        labels, logits, pos_weight = 2\n        )\n    return loss\n\ndef my_loss():\n    return weighted_cross_entropy_with_logits\n\n\nlstm_2.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = 0.001),\n                          loss={f\"instrument_{ins}\": my_loss()\n                                for ins in instrument_list},\n                          metrics=['accuracy', tf.keras.metrics.AUC()])\n\n\nfrom datetime import datetime\n\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    f\"../models/classic_lstm2/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_classic_lstm2\",\n    monitor='val_accuracy',\n    save_freq='epoch')\n    \nearly_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)\n\n\nlstm2_history = lstm_2.fit(lstm2_train_generator, \n                            epochs=10,\n                            validation_data=lstm2_eval_generator,\n                            validation_freq=1,\n                            use_multiprocessing= True,\n                            workers= 3,\n                            verbose=1,\n                            callbacks=[ckpt_callback])\n\nNameError: name 'lstm_2' is not defined\n\n\n\nmusicnet_eval(lstm_2, test_generator, 3)\n\n1/1 [==============================] - 0s 37ms/step\n\n\n\n\n\nWe still observe the same pattern, that constant values are passed along the timesteps.\nThe x-axis above represents the 81 notes, and y-axis represetns the timesteps.\nThe figure on the left represents the prediction of our model, whereas the right figure represents the true labels.\n\n\nConv to lstm\nAttempts last architecture of padding filters of convolutional layers to LSTM layers. This is the most basic model that extract features by convolutional layers, and passing the feature through the sequnce to make a prediction.\n\nBATCH_SIZE = 8\n\n# We have choose to work with 128 frequency bins,\n# and the None input shape indicates the dynamic length of time dimension\ninp = Input((None, 128, 1), batch_size=BATCH_SIZE)\nnormalizer = layers.BatchNormalization()(inp)\n\n# Extracting features from first convolutional layers.\n# Padding set to same to maintain same time dimension\nconv_1 = Conv2D(30, (50, 40), padding = 'same')(normalizer)\n\nnormalizer_2 = layers.BatchNormalization()(conv_1)\n# pool_1 = layers.TimeDistributed(MaxPool1D(2))(normalizer_2)\n# flatten_1 = flatten = layers.TimeDistributed(layers.Flatten())(pool_1)\n\n# conv_2 = layers.Conv2D(5, (1000, 30), padding = 'same')(pool_1)\n# pool_2 = layers.TimeDistributed(layers.MaxPool1D(2))(conv_2)\n\n# Flatten each filter layer for each timesteps\nflatten = layers.TimeDistributed(Flatten())(normalizer_2)\n\ndrop_1 = layers.Dropout(0.3)(flatten)\n# # Feed in the flattened layers to multiple layers of Dense layer,\n# # Performing embedding\n# Dense_1 = layers.TimeDistributed(Dense(300, activation = 'relu'))(flatten)\n# normalizer_3 = layers.BatchNormalization()(Dense_1)\n# drop_1 = layers.Dropout(0.2)(normalizer_3)\n\n# Dense_2 = layers.TimeDistributed(Dense(150, activation = 'relu'))(drop_1)\n# normalizer_4 = layers.BatchNormalization()(Dense_2)\n# drop_2 = layers.Dropout(0.2)(normalizer_4)\n\n# Dense_3 = layers.TimeDistributed(Dense(50, activation = 'relu'))(drop_2)\n# normalizer_5 = layers.BatchNormalization()(Dense_3)\n# drop_3 = layers.Dropout(0.2)(normalizer_5)\n\n# Dense_3 = layers.TimeDistributed(Dense(200, activation = 'relu'))(drop_2)\n# normalizer_5 = layers.BatchNormalization()(Dense_3)\n# drop_3 = layers.Dropout(0.2)(normalizer_5)\n\n# Lastly, put the decoded features in to LSTM layer\nlast_lstm = LSTM(500, return_sequences=True, dropout = 0.3)(drop_1)\n\n# Outputing to final Dense layer with sigmoid activation,\n# To predict the note labels for each timesteps\nconv_to_lstm_model = Model(inp, [instrument_layer_simple(last_lstm,\n                                f\"instrument_{ins}\") for ins in instrument_list])\n\n\ntf.keras.utils.plot_model(conv_to_lstm_model, show_shapes=True, show_dtype=True)\n\n\n\n\n\nclassic_train_generator =  classic_generator(mode='train', batch_size=BATCH_SIZE)\n                        # (tf.dtypes.float32, tf.dtypes.bool))\n\nclassic_eval_generator = classic_generator(mode='test', batch_size=BATCH_SIZE, preprocess = False)\n\n\ndef weighted_cross_entropy_with_logits(labels, logits):\n    loss = tf.nn.weighted_cross_entropy_with_logits(\n        labels, logits, pos_weight = 5\n        )\n    return loss\n\ndef my_loss():\n    return weighted_cross_entropy_with_logits\n\n# As mentioned before, since our prediction is a sparse multilabel problem, \n# the accuracy might not makes muc of sense, in addition, we will be adding\n# AUC for each instrument to gauge how well the model is performing\nconv_to_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001),\n                          loss={f\"instrument_{ins}\": my_loss() \n                                for ins in instrument_list},\n                          metrics=['accuracy', tf.keras.metrics.AUC()])\n\n\nfrom datetime import datetime\n\nckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    f\"../models/classic_direct_conv_to_lstm/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_classic_direct_conv_to_lstm\",\n    monitor='val_accuracy',\n    save_freq='epoch')\n    \nearly_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)\n\n\nconv_to_lstm_history = conv_to_lstm_model.fit(classic_train_generator, \n                                                epochs=10,\n                                                validation_data=classic_eval_generator,\n                                                validation_freq=1,\n                                                # use_multiprocessing= True,\n                                                # workers= 3,\n                                                verbose=1,\n                                                # class_weight= {0: 0.11, 1: 0.89}, \n                                                callbacks=[ckpt_callback])\n\nEpoch 1/10\n\n\n2022-08-07 18:44:58.981777: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n2022-08-07 18:44:59.811714: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-07 18:44:59.813228: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-07 18:44:59.813276: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n2022-08-07 18:44:59.814714: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-07 18:44:59.814829: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\nRelying on driver to perform ptx compilation. \nModify $PATH to customize ptxas location.\nThis message will be only logged once.\n\n\n40/40 [==============================] - ETA: 0s - loss: 9.4632 - instrument_1_loss: 0.9001 - instrument_7_loss: 0.8716 - instrument_41_loss: 0.8497 - instrument_42_loss: 0.8685 - instrument_43_loss: 0.8520 - instrument_44_loss: 0.8687 - instrument_61_loss: 0.8508 - instrument_69_loss: 0.8303 - instrument_71_loss: 0.8733 - instrument_72_loss: 0.8566 - instrument_74_loss: 0.8417 - instrument_1_accuracy: 0.0041 - instrument_1_auc_1: 0.4734 - instrument_7_accuracy: 0.0077 - instrument_7_auc_1: 0.3513 - instrument_41_accuracy: 0.0077 - instrument_41_auc_1: 0.5359 - instrument_42_accuracy: 0.0058 - instrument_42_auc_1: 0.5681 - instrument_43_accuracy: 0.0010 - instrument_43_auc_1: 0.5255 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.5647 - instrument_61_accuracy: 0.0016 - instrument_61_auc_1: 0.4531 - instrument_69_accuracy: 0.0015 - instrument_69_auc_1: 0.6652 - instrument_71_accuracy: 0.0340 - instrument_71_auc_1: 0.6068 - instrument_72_accuracy: 6.7188e-04 - instrument_72_auc_1: 0.5844 - instrument_74_accuracy: 0.0421 - instrument_74_auc_1: 0.5877 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n2022-08-07 19:00:44.902483: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-08-07 19:00:44.963806: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-08-07 19:00:44.996746: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_01_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_01_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 977s 24s/step - loss: 9.4632 - instrument_1_loss: 0.9001 - instrument_7_loss: 0.8716 - instrument_41_loss: 0.8497 - instrument_42_loss: 0.8685 - instrument_43_loss: 0.8520 - instrument_44_loss: 0.8687 - instrument_61_loss: 0.8508 - instrument_69_loss: 0.8303 - instrument_71_loss: 0.8733 - instrument_72_loss: 0.8566 - instrument_74_loss: 0.8417 - instrument_1_accuracy: 0.0041 - instrument_1_auc_1: 0.4734 - instrument_7_accuracy: 0.0077 - instrument_7_auc_1: 0.3513 - instrument_41_accuracy: 0.0077 - instrument_41_auc_1: 0.5359 - instrument_42_accuracy: 0.0058 - instrument_42_auc_1: 0.5681 - instrument_43_accuracy: 0.0010 - instrument_43_auc_1: 0.5255 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.5647 - instrument_61_accuracy: 0.0016 - instrument_61_auc_1: 0.4531 - instrument_69_accuracy: 0.0015 - instrument_69_auc_1: 0.6652 - instrument_71_accuracy: 0.0340 - instrument_71_auc_1: 0.6068 - instrument_72_accuracy: 6.7188e-04 - instrument_72_auc_1: 0.5844 - instrument_74_accuracy: 0.0421 - instrument_74_auc_1: 0.5877 - val_loss: 8.4378 - val_instrument_1_loss: 0.8021 - val_instrument_7_loss: 0.7764 - val_instrument_41_loss: 0.7603 - val_instrument_42_loss: 0.7667 - val_instrument_43_loss: 0.7576 - val_instrument_44_loss: 0.7714 - val_instrument_61_loss: 0.7615 - val_instrument_69_loss: 0.7426 - val_instrument_71_loss: 0.7806 - val_instrument_72_loss: 0.7687 - val_instrument_74_loss: 0.7498 - val_instrument_1_accuracy: 0.0000e+00 - val_instrument_1_auc_1: 0.4493 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0069 - val_instrument_41_auc_1: 0.5197 - val_instrument_42_accuracy: 0.0031 - val_instrument_42_auc_1: 0.5051 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5542 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4583 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5306 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5087 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 2/10\n40/40 [==============================] - ETA: 0s - loss: 8.1047 - instrument_1_loss: 0.7867 - instrument_7_loss: 0.7376 - instrument_41_loss: 0.7359 - instrument_42_loss: 0.7357 - instrument_43_loss: 0.7339 - instrument_44_loss: 0.7341 - instrument_61_loss: 0.7287 - instrument_69_loss: 0.7197 - instrument_71_loss: 0.7377 - instrument_72_loss: 0.7307 - instrument_74_loss: 0.7239 - instrument_1_accuracy: 0.0014 - instrument_1_auc_1: 0.4879 - instrument_7_accuracy: 0.0012 - instrument_7_auc_1: 0.4978 - instrument_41_accuracy: 0.0033 - instrument_41_auc_1: 0.5315 - instrument_42_accuracy: 0.0027 - instrument_42_auc_1: 0.5510 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5280 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.4818 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3864 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6101 - instrument_71_accuracy: 0.0000e+00 - instrument_71_auc_1: 0.5646 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5410 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5612     \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n2022-08-07 19:17:20.517681: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n2022-08-07 19:17:20.554600: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_02_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_02_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 995s 25s/step - loss: 8.1047 - instrument_1_loss: 0.7867 - instrument_7_loss: 0.7376 - instrument_41_loss: 0.7359 - instrument_42_loss: 0.7357 - instrument_43_loss: 0.7339 - instrument_44_loss: 0.7341 - instrument_61_loss: 0.7287 - instrument_69_loss: 0.7197 - instrument_71_loss: 0.7377 - instrument_72_loss: 0.7307 - instrument_74_loss: 0.7239 - instrument_1_accuracy: 0.0014 - instrument_1_auc_1: 0.4879 - instrument_7_accuracy: 0.0012 - instrument_7_auc_1: 0.4978 - instrument_41_accuracy: 0.0033 - instrument_41_auc_1: 0.5315 - instrument_42_accuracy: 0.0027 - instrument_42_auc_1: 0.5510 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5280 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.4818 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3864 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6101 - instrument_71_accuracy: 0.0000e+00 - instrument_71_auc_1: 0.5646 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5410 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5612 - val_loss: 7.8978 - val_instrument_1_loss: 0.7504 - val_instrument_7_loss: 0.7148 - val_instrument_41_loss: 0.7218 - val_instrument_42_loss: 0.7141 - val_instrument_43_loss: 0.7118 - val_instrument_44_loss: 0.7135 - val_instrument_61_loss: 0.7121 - val_instrument_69_loss: 0.7065 - val_instrument_71_loss: 0.7230 - val_instrument_72_loss: 0.7210 - val_instrument_74_loss: 0.7088 - val_instrument_1_accuracy: 0.0037 - val_instrument_1_auc_1: 0.4846 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0100 - val_instrument_41_auc_1: 0.5245 - val_instrument_42_accuracy: 0.0031 - val_instrument_42_auc_1: 0.5151 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5917 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.2997 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5111 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4152 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 3/10\n40/40 [==============================] - ETA: 0s - loss: 7.8624 - instrument_1_loss: 0.7675 - instrument_7_loss: 0.7092 - instrument_41_loss: 0.7183 - instrument_42_loss: 0.7135 - instrument_43_loss: 0.7129 - instrument_44_loss: 0.7078 - instrument_61_loss: 0.7065 - instrument_69_loss: 0.7039 - instrument_71_loss: 0.7093 - instrument_72_loss: 0.7083 - instrument_74_loss: 0.7054 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4918 - instrument_7_accuracy: 0.0010 - instrument_7_auc_1: 0.5659 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5233 - instrument_42_accuracy: 0.0012 - instrument_42_auc_1: 0.5637 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5550 - instrument_44_accuracy: 0.0020 - instrument_44_auc_1: 0.5860 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3889 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6318 - instrument_71_accuracy: 1.5625e-05 - instrument_71_auc_1: 0.5802 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4675 - instrument_74_accuracy: 1.5625e-05 - instrument_74_auc_1: 0.5780 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_03_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_03_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 956s 24s/step - loss: 7.8624 - instrument_1_loss: 0.7675 - instrument_7_loss: 0.7092 - instrument_41_loss: 0.7183 - instrument_42_loss: 0.7135 - instrument_43_loss: 0.7129 - instrument_44_loss: 0.7078 - instrument_61_loss: 0.7065 - instrument_69_loss: 0.7039 - instrument_71_loss: 0.7093 - instrument_72_loss: 0.7083 - instrument_74_loss: 0.7054 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4918 - instrument_7_accuracy: 0.0010 - instrument_7_auc_1: 0.5659 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5233 - instrument_42_accuracy: 0.0012 - instrument_42_auc_1: 0.5637 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5550 - instrument_44_accuracy: 0.0020 - instrument_44_auc_1: 0.5860 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3889 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6318 - instrument_71_accuracy: 1.5625e-05 - instrument_71_auc_1: 0.5802 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4675 - instrument_74_accuracy: 1.5625e-05 - instrument_74_auc_1: 0.5780 - val_loss: 7.7954 - val_instrument_1_loss: 0.7227 - val_instrument_7_loss: 0.7037 - val_instrument_41_loss: 0.7197 - val_instrument_42_loss: 0.7082 - val_instrument_43_loss: 0.7105 - val_instrument_44_loss: 0.7031 - val_instrument_61_loss: 0.7064 - val_instrument_69_loss: 0.7001 - val_instrument_71_loss: 0.7094 - val_instrument_72_loss: 0.7105 - val_instrument_74_loss: 0.7012 - val_instrument_1_accuracy: 0.0031 - val_instrument_1_auc_1: 0.4640 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0031 - val_instrument_41_auc_1: 0.5485 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.6343 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4931 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4119 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5317 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5208 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 4/10\n40/40 [==============================] - ETA: 0s - loss: 7.8032 - instrument_1_loss: 0.7629 - instrument_7_loss: 0.7022 - instrument_41_loss: 0.7141 - instrument_42_loss: 0.7073 - instrument_43_loss: 0.7084 - instrument_44_loss: 0.7014 - instrument_61_loss: 0.7015 - instrument_69_loss: 0.6996 - instrument_71_loss: 0.7028 - instrument_72_loss: 0.7026 - instrument_74_loss: 0.7005 - instrument_1_accuracy: 0.0195 - instrument_1_auc_1: 0.4969 - instrument_7_accuracy: 5.4688e-04 - instrument_7_auc_1: 0.5081 - instrument_41_accuracy: 0.0056 - instrument_41_auc_1: 0.5313 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5713 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5383 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.5499 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4050 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5505 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5238 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4662 - instrument_74_accuracy: 3.1250e-05 - instrument_74_auc_1: 0.5639 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_04_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_04_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 840s 21s/step - loss: 7.8032 - instrument_1_loss: 0.7629 - instrument_7_loss: 0.7022 - instrument_41_loss: 0.7141 - instrument_42_loss: 0.7073 - instrument_43_loss: 0.7084 - instrument_44_loss: 0.7014 - instrument_61_loss: 0.7015 - instrument_69_loss: 0.6996 - instrument_71_loss: 0.7028 - instrument_72_loss: 0.7026 - instrument_74_loss: 0.7005 - instrument_1_accuracy: 0.0195 - instrument_1_auc_1: 0.4969 - instrument_7_accuracy: 5.4688e-04 - instrument_7_auc_1: 0.5081 - instrument_41_accuracy: 0.0056 - instrument_41_auc_1: 0.5313 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5713 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5383 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.5499 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4050 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5505 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5238 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4662 - instrument_74_accuracy: 3.1250e-05 - instrument_74_auc_1: 0.5639 - val_loss: 7.7528 - val_instrument_1_loss: 0.7379 - val_instrument_7_loss: 0.6998 - val_instrument_41_loss: 0.7096 - val_instrument_42_loss: 0.7011 - val_instrument_43_loss: 0.7045 - val_instrument_44_loss: 0.6996 - val_instrument_61_loss: 0.6999 - val_instrument_69_loss: 0.6976 - val_instrument_71_loss: 0.7014 - val_instrument_72_loss: 0.7032 - val_instrument_74_loss: 0.6983 - val_instrument_1_accuracy: 0.0113 - val_instrument_1_auc_1: 0.4940 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 6.2500e-04 - val_instrument_41_auc_1: 0.5468 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5828 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5291 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.3873 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.6024 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4427 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 5/10\n40/40 [==============================] - ETA: 0s - loss: 7.7747 - instrument_1_loss: 0.7576 - instrument_7_loss: 0.6996 - instrument_41_loss: 0.7121 - instrument_42_loss: 0.7049 - instrument_43_loss: 0.7059 - instrument_44_loss: 0.6988 - instrument_61_loss: 0.6990 - instrument_69_loss: 0.6979 - instrument_71_loss: 0.7001 - instrument_72_loss: 0.7005 - instrument_74_loss: 0.6984 - instrument_1_accuracy: 0.0180 - instrument_1_auc_1: 0.4997 - instrument_7_accuracy: 9.6875e-04 - instrument_7_auc_1: 0.4890 - instrument_41_accuracy: 0.0024 - instrument_41_auc_1: 0.5324 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5521 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5446 - instrument_44_accuracy: 6.8750e-04 - instrument_44_auc_1: 0.4677 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3171 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6400 - instrument_71_accuracy: 0.0011 - instrument_71_auc_1: 0.5348 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4833 - instrument_74_accuracy: 9.3750e-05 - instrument_74_auc_1: 0.5742 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_05_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_05_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 809s 20s/step - loss: 7.7747 - instrument_1_loss: 0.7576 - instrument_7_loss: 0.6996 - instrument_41_loss: 0.7121 - instrument_42_loss: 0.7049 - instrument_43_loss: 0.7059 - instrument_44_loss: 0.6988 - instrument_61_loss: 0.6990 - instrument_69_loss: 0.6979 - instrument_71_loss: 0.7001 - instrument_72_loss: 0.7005 - instrument_74_loss: 0.6984 - instrument_1_accuracy: 0.0180 - instrument_1_auc_1: 0.4997 - instrument_7_accuracy: 9.6875e-04 - instrument_7_auc_1: 0.4890 - instrument_41_accuracy: 0.0024 - instrument_41_auc_1: 0.5324 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5521 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5446 - instrument_44_accuracy: 6.8750e-04 - instrument_44_auc_1: 0.4677 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3171 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6400 - instrument_71_accuracy: 0.0011 - instrument_71_auc_1: 0.5348 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4833 - instrument_74_accuracy: 9.3750e-05 - instrument_74_auc_1: 0.5742 - val_loss: 7.7501 - val_instrument_1_loss: 0.7305 - val_instrument_7_loss: 0.6980 - val_instrument_41_loss: 0.7178 - val_instrument_42_loss: 0.7033 - val_instrument_43_loss: 0.7052 - val_instrument_44_loss: 0.6978 - val_instrument_61_loss: 0.7014 - val_instrument_69_loss: 0.6965 - val_instrument_71_loss: 0.7022 - val_instrument_72_loss: 0.7005 - val_instrument_74_loss: 0.6969 - val_instrument_1_accuracy: 0.0094 - val_instrument_1_auc_1: 0.4634 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0000e+00 - val_instrument_41_auc_1: 0.5274 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5837 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4972 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.2460 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5497 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4621 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 6/10\n40/40 [==============================] - ETA: 0s - loss: 7.7649 - instrument_1_loss: 0.7601 - instrument_7_loss: 0.6981 - instrument_41_loss: 0.7116 - instrument_42_loss: 0.7037 - instrument_43_loss: 0.7046 - instrument_44_loss: 0.6974 - instrument_61_loss: 0.6981 - instrument_69_loss: 0.6968 - instrument_71_loss: 0.6986 - instrument_72_loss: 0.6990 - instrument_74_loss: 0.6970 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4878 - instrument_7_accuracy: 7.5000e-04 - instrument_7_auc_1: 0.4831 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5172 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5709 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5191 - instrument_44_accuracy: 2.3437e-04 - instrument_44_auc_1: 0.3375 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3742 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5869 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5242 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 5.1562e-04 - instrument_74_auc_1: 0.5644 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_06_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_06_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 823s 20s/step - loss: 7.7649 - instrument_1_loss: 0.7601 - instrument_7_loss: 0.6981 - instrument_41_loss: 0.7116 - instrument_42_loss: 0.7037 - instrument_43_loss: 0.7046 - instrument_44_loss: 0.6974 - instrument_61_loss: 0.6981 - instrument_69_loss: 0.6968 - instrument_71_loss: 0.6986 - instrument_72_loss: 0.6990 - instrument_74_loss: 0.6970 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4878 - instrument_7_accuracy: 7.5000e-04 - instrument_7_auc_1: 0.4831 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5172 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5709 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5191 - instrument_44_accuracy: 2.3437e-04 - instrument_44_auc_1: 0.3375 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3742 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5869 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5242 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 5.1562e-04 - instrument_74_auc_1: 0.5644 - val_loss: 7.7343 - val_instrument_1_loss: 0.7160 - val_instrument_7_loss: 0.6968 - val_instrument_41_loss: 0.7101 - val_instrument_42_loss: 0.7015 - val_instrument_43_loss: 0.7050 - val_instrument_44_loss: 0.6966 - val_instrument_61_loss: 0.7034 - val_instrument_69_loss: 0.6956 - val_instrument_71_loss: 0.7063 - val_instrument_72_loss: 0.7070 - val_instrument_74_loss: 0.6960 - val_instrument_1_accuracy: 0.0050 - val_instrument_1_auc_1: 0.5094 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0000e+00 - val_instrument_41_auc_1: 0.5444 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.6109 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4586 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0025 - val_instrument_61_auc_1: 0.4560 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0012 - val_instrument_71_auc_1: 0.5428 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4793 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 7/10\n40/40 [==============================] - ETA: 0s - loss: 7.7545 - instrument_1_loss: 0.7566 - instrument_7_loss: 0.6973 - instrument_41_loss: 0.7104 - instrument_42_loss: 0.7032 - instrument_43_loss: 0.7040 - instrument_44_loss: 0.6965 - instrument_61_loss: 0.6975 - instrument_69_loss: 0.6961 - instrument_71_loss: 0.6978 - instrument_72_loss: 0.6986 - instrument_74_loss: 0.6965 - instrument_1_accuracy: 0.0227 - instrument_1_auc_1: 0.5040 - instrument_7_accuracy: 9.5313e-04 - instrument_7_auc_1: 0.5582 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5480 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5572 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5104 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.4126 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4421 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5621 - instrument_71_accuracy: 1.4062e-04 - instrument_71_auc_1: 0.5336 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5092 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5229 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_07_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_07_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1015s 26s/step - loss: 7.7545 - instrument_1_loss: 0.7566 - instrument_7_loss: 0.6973 - instrument_41_loss: 0.7104 - instrument_42_loss: 0.7032 - instrument_43_loss: 0.7040 - instrument_44_loss: 0.6965 - instrument_61_loss: 0.6975 - instrument_69_loss: 0.6961 - instrument_71_loss: 0.6978 - instrument_72_loss: 0.6986 - instrument_74_loss: 0.6965 - instrument_1_accuracy: 0.0227 - instrument_1_auc_1: 0.5040 - instrument_7_accuracy: 9.5313e-04 - instrument_7_auc_1: 0.5582 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5480 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5572 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5104 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.4126 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4421 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5621 - instrument_71_accuracy: 1.4062e-04 - instrument_71_auc_1: 0.5336 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5092 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5229 - val_loss: 7.7320 - val_instrument_1_loss: 0.7148 - val_instrument_7_loss: 0.6960 - val_instrument_41_loss: 0.7158 - val_instrument_42_loss: 0.7021 - val_instrument_43_loss: 0.7050 - val_instrument_44_loss: 0.6959 - val_instrument_61_loss: 0.6996 - val_instrument_69_loss: 0.6951 - val_instrument_71_loss: 0.7057 - val_instrument_72_loss: 0.7067 - val_instrument_74_loss: 0.6953 - val_instrument_1_accuracy: 0.0056 - val_instrument_1_auc_1: 0.4874 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0081 - val_instrument_41_auc_1: 0.5351 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5699 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4698 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.5076 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5836 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5175 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 8/10\n40/40 [==============================] - ETA: 0s - loss: 7.7481 - instrument_1_loss: 0.7563 - instrument_7_loss: 0.6965 - instrument_41_loss: 0.7097 - instrument_42_loss: 0.7023 - instrument_43_loss: 0.7032 - instrument_44_loss: 0.6959 - instrument_61_loss: 0.6970 - instrument_69_loss: 0.6959 - instrument_71_loss: 0.6972 - instrument_72_loss: 0.6981 - instrument_74_loss: 0.6960 - instrument_1_accuracy: 0.0177 - instrument_1_auc_1: 0.5015 - instrument_7_accuracy: 6.2500e-04 - instrument_7_auc_1: 0.5371 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5368 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5660 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5082 - instrument_44_accuracy: 8.2812e-04 - instrument_44_auc_1: 0.4698 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4768 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5771 - instrument_71_accuracy: 1.8750e-04 - instrument_71_auc_1: 0.5660 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5181 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5266 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_08_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_08_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1063s 27s/step - loss: 7.7481 - instrument_1_loss: 0.7563 - instrument_7_loss: 0.6965 - instrument_41_loss: 0.7097 - instrument_42_loss: 0.7023 - instrument_43_loss: 0.7032 - instrument_44_loss: 0.6959 - instrument_61_loss: 0.6970 - instrument_69_loss: 0.6959 - instrument_71_loss: 0.6972 - instrument_72_loss: 0.6981 - instrument_74_loss: 0.6960 - instrument_1_accuracy: 0.0177 - instrument_1_auc_1: 0.5015 - instrument_7_accuracy: 6.2500e-04 - instrument_7_auc_1: 0.5371 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5368 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5660 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5082 - instrument_44_accuracy: 8.2812e-04 - instrument_44_auc_1: 0.4698 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4768 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5771 - instrument_71_accuracy: 1.8750e-04 - instrument_71_auc_1: 0.5660 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5181 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5266 - val_loss: 7.7214 - val_instrument_1_loss: 0.7360 - val_instrument_7_loss: 0.6954 - val_instrument_41_loss: 0.7096 - val_instrument_42_loss: 0.6991 - val_instrument_43_loss: 0.7022 - val_instrument_44_loss: 0.6953 - val_instrument_61_loss: 0.6950 - val_instrument_69_loss: 0.6947 - val_instrument_71_loss: 0.6990 - val_instrument_72_loss: 0.7001 - val_instrument_74_loss: 0.6949 - val_instrument_1_accuracy: 0.0050 - val_instrument_1_auc_1: 0.4999 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0025 - val_instrument_41_auc_1: 0.5021 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5982 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5448 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.0000e+00 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5107 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4850 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 9/10\n40/40 [==============================] - ETA: 0s - loss: 7.7462 - instrument_1_loss: 0.7569 - instrument_7_loss: 0.6962 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7022 - instrument_43_loss: 0.7034 - instrument_44_loss: 0.6954 - instrument_61_loss: 0.6962 - instrument_69_loss: 0.6956 - instrument_71_loss: 0.6970 - instrument_72_loss: 0.6976 - instrument_74_loss: 0.6956 - instrument_1_accuracy: 0.0090 - instrument_1_auc_1: 0.4916 - instrument_7_accuracy: 8.7500e-04 - instrument_7_auc_1: 0.4746 - instrument_41_accuracy: 0.0042 - instrument_41_auc_1: 0.5434 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5592 - instrument_43_accuracy: 1.5625e-05 - instrument_43_auc_1: 0.5093 - instrument_44_accuracy: 0.0019 - instrument_44_auc_1: 0.5424 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4239 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5228 - instrument_71_accuracy: 6.8750e-04 - instrument_71_auc_1: 0.5023 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5054 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.4956 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_09_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_09_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1070s 27s/step - loss: 7.7462 - instrument_1_loss: 0.7569 - instrument_7_loss: 0.6962 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7022 - instrument_43_loss: 0.7034 - instrument_44_loss: 0.6954 - instrument_61_loss: 0.6962 - instrument_69_loss: 0.6956 - instrument_71_loss: 0.6970 - instrument_72_loss: 0.6976 - instrument_74_loss: 0.6956 - instrument_1_accuracy: 0.0090 - instrument_1_auc_1: 0.4916 - instrument_7_accuracy: 8.7500e-04 - instrument_7_auc_1: 0.4746 - instrument_41_accuracy: 0.0042 - instrument_41_auc_1: 0.5434 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5592 - instrument_43_accuracy: 1.5625e-05 - instrument_43_auc_1: 0.5093 - instrument_44_accuracy: 0.0019 - instrument_44_auc_1: 0.5424 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4239 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5228 - instrument_71_accuracy: 6.8750e-04 - instrument_71_auc_1: 0.5023 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5054 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.4956 - val_loss: 7.7105 - val_instrument_1_loss: 0.7222 - val_instrument_7_loss: 0.6950 - val_instrument_41_loss: 0.7136 - val_instrument_42_loss: 0.6994 - val_instrument_43_loss: 0.7035 - val_instrument_44_loss: 0.6950 - val_instrument_61_loss: 0.6960 - val_instrument_69_loss: 0.6945 - val_instrument_71_loss: 0.6972 - val_instrument_72_loss: 0.6994 - val_instrument_74_loss: 0.6946 - val_instrument_1_accuracy: 0.0031 - val_instrument_1_auc_1: 0.4976 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0106 - val_instrument_41_auc_1: 0.5649 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5388 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5100 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4717 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.4343 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4660 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\nEpoch 10/10\n40/40 [==============================] - ETA: 0s - loss: 7.7444 - instrument_1_loss: 0.7565 - instrument_7_loss: 0.6958 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7021 - instrument_43_loss: 0.7035 - instrument_44_loss: 0.6952 - instrument_61_loss: 0.6963 - instrument_69_loss: 0.6953 - instrument_71_loss: 0.6967 - instrument_72_loss: 0.6973 - instrument_74_loss: 0.6954 - instrument_1_accuracy: 0.0147 - instrument_1_auc_1: 0.4936 - instrument_7_accuracy: 8.5937e-04 - instrument_7_auc_1: 0.5001 - instrument_41_accuracy: 0.0035 - instrument_41_auc_1: 0.5259 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5330 - instrument_43_accuracy: 3.1250e-05 - instrument_43_auc_1: 0.4885 - instrument_44_accuracy: 0.0027 - instrument_44_auc_1: 0.5748 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4723 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5207 - instrument_71_accuracy: 0.0010 - instrument_71_auc_1: 0.4945 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4725 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5151 \n\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_10_classic_direct_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_10_classic_direct_conv_to_lstm/assets\n\n\n40/40 [==============================] - 1063s 27s/step - loss: 7.7444 - instrument_1_loss: 0.7565 - instrument_7_loss: 0.6958 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7021 - instrument_43_loss: 0.7035 - instrument_44_loss: 0.6952 - instrument_61_loss: 0.6963 - instrument_69_loss: 0.6953 - instrument_71_loss: 0.6967 - instrument_72_loss: 0.6973 - instrument_74_loss: 0.6954 - instrument_1_accuracy: 0.0147 - instrument_1_auc_1: 0.4936 - instrument_7_accuracy: 8.5937e-04 - instrument_7_auc_1: 0.5001 - instrument_41_accuracy: 0.0035 - instrument_41_auc_1: 0.5259 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5330 - instrument_43_accuracy: 3.1250e-05 - instrument_43_auc_1: 0.4885 - instrument_44_accuracy: 0.0027 - instrument_44_auc_1: 0.5748 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4723 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5207 - instrument_71_accuracy: 0.0010 - instrument_71_auc_1: 0.4945 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4725 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5151 - val_loss: 7.7124 - val_instrument_1_loss: 0.7159 - val_instrument_7_loss: 0.6948 - val_instrument_41_loss: 0.7116 - val_instrument_42_loss: 0.6982 - val_instrument_43_loss: 0.7023 - val_instrument_44_loss: 0.6947 - val_instrument_61_loss: 0.6964 - val_instrument_69_loss: 0.6943 - val_instrument_71_loss: 0.7038 - val_instrument_72_loss: 0.7060 - val_instrument_74_loss: 0.6944 - val_instrument_1_accuracy: 0.0044 - val_instrument_1_auc_1: 0.5049 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0131 - val_instrument_41_auc_1: 0.5142 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5568 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4590 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.4428 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.4287 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4621 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00\n\n\n\nconv_to_lstm_model.save('../models/new_classic_conv_to_lstm/')\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: ../models/new_classic_conv_to_lstm/assets\n\n\nINFO:tensorflow:Assets written to: ../models/new_classic_conv_to_lstm/assets\n\n\n\nsave_history(conv_to_lstm_history.history, '../models/new_classic_conv_to_lstm.pkl')\n\n\nmusicnet_eval(conv_to_lstm_model, classic_eval_generator, 2)\n\n1/1 [==============================] - 0s 25ms/step\n\n\n\n\n\nAgain, same behavior is observed, we have concluded that the input data might be incompatible for our model, further investigation on the input pipeline should be taken."
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/music_transcription_2conv.html",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/music_transcription_2conv.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import glob\nimport os\nimport random\nfrom datetime import datetime\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.stats\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow.python.platform.build_info as build\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.python.client import device_lib\n\nfrom spec_generator_sequence import spec_generator\nfrom spec_input_generator import gen, gen_eval\n# import spectrogram\n# from spectrogram import generate_spec\n# from spectrogram import truncate_spec\n# from spectrogram import mask_spec\n# from spectrogram import add_noise\n# from spectrogram import path_to_preprocessing\nfrom spectrogram_class import spectrogram\n\n\nOrchideaSOL CNN\nThis notebooks is succesive from the baseline model notebook from music_transcription_class.ipynb.\nIn the notebook, we had demonstrate the limitation of simple CNN model on classifying instruments from raw audio files.\nIn this notebook, we will be running a similar CNN architecture, but with deeper layers, and applying regularization, batch normalization and dropout techniques.\n\nprint(build.build_info['cuda_version'])\n\n11.2\n\n\n\ndevice_lib.list_local_devices()\n\n2022-08-08 07:35:52.733084: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-08 07:35:52.777510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:52.813076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:52.813878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.814786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.815302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.815685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.816454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3085 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 13176899990423004209\n xla_global_id: -1,\n name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 3235774464\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 4598158092110336817\n physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n xla_global_id: 416903419]\n\n\n\ntf.test.is_gpu_available()\n\nWARNING:tensorflow:From /tmp/ipykernel_49617/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\n\n\n2022-08-08 07:35:53.919564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\nTrue\n\n\n2022-08-08 07:35:53.920078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.920462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.920851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.921170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:53.921402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3085 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n\ntf.config.list_physical_devices('GPU')\n\n2022-08-08 07:35:54.029967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:54.030428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:54.030817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\ngpu_devices\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\nWe first start by generating training and testing dataset as usual.\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\n\n\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\n\n# Splitting data for training and testing\ntrain_df, test_df = train_test_split(meta_df, stratify=meta_df['Instrument (in full)'], \n                                        train_size=0.8)\n\n\ntrain_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      4961\n      Strings/Contrabass/pizzicato_l_vib/Cb-pizz_lv-...\n      Strings\n      Violin Family\n      Cb\n      Contrabass\n      pizz_lv\n      pizzicato_l_vib\n      E1\n      28.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      2\n    \n    \n      3401\n      PluckedStrings/Guitar/ordinario/Gtr-ord-F#5-ff...\n      PluckedStrings\n      Plucked Strings\n      Gtr\n      Guitar\n      ord\n      ordinario\n      F#5\n      78.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      1.0\n      True\n      1\n    \n  \n\n\n\n\n\nsample = next(gen(train_df, return_class = True))\n\n\n# Getting the shape of input from generator\nspec_shape = sample[0].spec.shape\nspec_shape\n\n(256, 500, 1)\n\n\n\nnext(gen_eval(test_df))[0].shape\n\n(256, 500, 1)\n\n\nWe will be using the same Sequence data generator, as in previous notebook. However, to optimize our performance, we will be converting the generator in to a tf Dataset object, and optimized the performance by prefetching the dataset while fitting is in progress.\nThe process of prefecthing and training can be visualized using tf.data API. Which is in the todo list of this project.\n\nBATCH_SIZE = 32\n\ntrain_generator = (tf.data.Dataset.from_generator(lambda: spec_generator(train_df, BATCH_SIZE, \n                  add_channel = True), output_types=(tf.float32, tf.int32), \n                 output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                 (BATCH_SIZE, 16)))).prefetch((tf.data.experimental.AUTOTUNE))\n\neval_generator = (tf.data.Dataset.from_generator(lambda: spec_generator(test_df, BATCH_SIZE, \n                  add_channel = True), \n                    output_types=(tf.float32, tf.int32), \n                 output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                 (BATCH_SIZE, 16)))).prefetch((tf.data.experimental.AUTOTUNE))\n\n2022-08-08 07:35:55.370698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.371338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.371817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.372308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.372680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 07:35:55.372983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3085 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n\n# Sanity check for shape\ntrain_generator\n\n<PrefetchDataset element_spec=(TensorSpec(shape=(32, 256, 500, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 16), dtype=tf.int32, name=None))>\n\n\nNow we can finally start to build our model, the idea is same as before, a deeper convolutional model, including dropouts and regularization.\n\n# Starting the model\nmodel_2conv = models.Sequential()\n\n# Adding the first convoluton-pooling layer\nmodel_2conv.add(layers.InputLayer((spec_shape[0], spec_shape[1], 1),\n                     batch_size = BATCH_SIZE, dtype = tf.float32))\nmodel_2conv.add(layers.Conv2D(30, (150, 300), activation='relu', \n                    kernel_regularizer = tf.keras.regularizers.L2(l2=0.01)))\nmodel_2conv.add(layers.MaxPool2D((2, 3)))\nmodel_2conv.add(layers.BatchNormalization())\n\n# Addig the second convolutional-pooling layer\nmodel_2conv.add(layers.Conv2D(15, (15, 30), activation = 'relu',\n                                kernel_regularizer = tf.keras.regularizers.L2(l2=0.01)))\nmodel_2conv.add(layers.MaxPool2D(2, 3))\nmodel_2conv.add(layers.BatchNormalization())\n\n# Combined the filter layers into 1\nmodel_2conv.add(layers.Flatten())\nmodel_2conv.add(layers.Dropout(0.2))\nmodel_2conv.add(layers.Dense(200, activation = 'relu'))\nmodel_2conv.add(layers.Dropout(0.2))\nmodel_2conv.add(layers.Dense(50, activation = 'relu'))\nmodel_2conv.add(layers.Dropout(0.2))\n\n# Final layer to classify the 16 instruments\n# We are using softmax activation since this is a multiclass classification problem\nmodel_2conv.add(layers.Dense(16, activation = 'softmax'))\n\nmodel_2conv.build()\n\n\n# Visualizing the CNN architecture\ntf.keras.utils.plot_model(model_2conv, show_shapes = True, show_dtype= True)\n\n\n\n\nOur input shape will be the same as in the previous notebook, with 256 frequency bins and 500 timesteps\nOur output layer consists of 16 neurons, each representing one possible instrument class.\nSince we will be predicting a multiclass label (Only one true label over multiple options), we will be using categorical cross entropy as our loss function, note the this is the reason why we have chose to use softmax as the activation function of our output layer.\n\n\nmodel_2conv.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.03),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy'])\n\n\nmodel_2conv.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (32, 107, 201, 30)        1350030   \n                                                                 \n max_pooling2d (MaxPooling2D  (32, 53, 67, 30)         0         \n )                                                               \n                                                                 \n batch_normalization (BatchN  (32, 53, 67, 30)         120       \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (32, 39, 38, 15)          202515    \n                                                                 \n max_pooling2d_1 (MaxPooling  (32, 13, 13, 15)         0         \n 2D)                                                             \n                                                                 \n batch_normalization_1 (Batc  (32, 13, 13, 15)         60        \n hNormalization)                                                 \n                                                                 \n flatten (Flatten)           (32, 2535)                0         \n                                                                 \n dropout (Dropout)           (32, 2535)                0         \n                                                                 \n dense (Dense)               (32, 200)                 507200    \n                                                                 \n dropout_1 (Dropout)         (32, 200)                 0         \n                                                                 \n dense_1 (Dense)             (32, 50)                  10050     \n                                                                 \n dropout_2 (Dropout)         (32, 50)                  0         \n                                                                 \n dense_2 (Dense)             (32, 16)                  816       \n                                                                 \n=================================================================\nTotal params: 2,070,791\nTrainable params: 2,070,701\nNon-trainable params: 90\n_________________________________________________________________\n\n\n\n#  model_2conv = tf.keras.models.load_model('../models/2conv/six/')\n\n\n\nModel training\nNow that we have done the setup we can finally train our model\n\n# ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n#     f\"../models/baseline_checkpoint/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_model_2conv\", \n#                                                     monitor='val_accuracy')\n# early_callback = tf.keras.callbacks.EarlyStopping(monitor = 'accuracy', patience = 2)\n\n\n# history = model_2conv.fit(train_generator, epochs = 4, verbose=1, \n#                     validation_data = eval_generator,\n#                     validation_steps = 10, validation_freq= 2, \n#                     use_multiprocessing=True, workers = 2, callbacks=[ckpt_callback, early_callback])\n\n\n# model_2conv.save('../models/new_OrchideaSOL_2conv/')\n\n\n\nModel Evaluating\n\n# plt.plot([2, 4], history.history['val_accuracy'], label = 'Validation accuracy')\n# plt.plot([1, 2, 3, 4], history.history['accuracy'], label = 'Training accuracy')\n# plt.title('Accuracy of baseline model after early callback')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy')\n# plt.legend()\n# plt.show()\n\nThe figure above shows the accuracy of 4 epochs we have ran, combining with the our baseline model of one convolutional layers, we can conclude that basic CNN since to be performing poorly on this problem. Since\n\nBoth the validation and training accuracy starts to drop after 3 epochs\nTraining time is around 30 minutes per epoch\n\nWe can still look at the loos function\n\n# plt.plot([2, 4], history.history['val_loss'], label = 'Validation loss')\n# plt.plot([1, 2, 3, 4], history.history['loss'], label = 'Training loss')\n# plt.title('Loss of baseline model after early callback')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.show()\n\nI have mistakenly run the code without saving the history, and lost the plot\nThe general shape of the figure is a sharp drop at first spoch, and remained fairly strat with minimal decrease after that\nThe loss function hardly decrease after the first epoch, however, both the decerase of loss function and accuracy represents that the categorical cross entropy might not be the best choice for such classification problem.\nBut the model might inprove after several epoch, this notebook is yet to be run again with longer time after the more important preceeding notebooks (Sequential music transcription with LSTM) had been done.\nAlso the hyperparameter such as the number of frequency bins, optimizer and regularization coefficient still can be optimized. Due to the limited time and high training time, this notebook had been added to the to do list, and decreased in priority.\nNow lets look at the confusion matrix\n\ndef orchidea_confusion_matrix(model, generator, instrument_list):\n    '''\n    Plot confusion matrix for OrchideaSOL dataset\n    \n    Input:\n    model: Model to be used\n    generator: Sequence class, generator to generate feature and labels for \n                OrchideaSOL dataset\n    instrument_list: list of instrument in alphabetical order, used to label the plot\n    \n    Output:\n    predict: np.array, Predicted output\n    prediction_label: np.array, True label\n    '''\n    prediction_feature, prediction_label = generator.__getitem__(0)\n    predict = predict = model.predict(prediction_feature)\n    assert prediction_label.shape == predict.shape\n    from sklearn.metrics import confusion_matrix\n\n    plt.figure(figsize = (12, 10))\n    sns.heatmap(confusion_matrix(np.argmax(prediction_label, axis=1), \n                        np.argmax(predict, axis = 1)), annot = True, \n                        xticklabels=instrument_list, \n                        yticklabels=instrument_list)\n    plt.title('Confusion matrix for baseline model')\n    plt.show()\n\n    return predict, prediction_label\n\n\nmodel = tf.keras.models.load_model('../models/baseline_checkpoint/20220807_052111_02_model_2conv')\n\n\n# Defining generator to be used in our evaluation\n# Since our gpu memory space is very limited, we wil be using 1/10 of \n# testing dataset, and omitting any data augmentation on the spectrogram\nprediction_generator = spec_generator(test_df, test_df.shape[0] // 5, add_channel=True,\n                                        live_generation = True, preprocess = False, \n                                        n_mels = 256)\n\n\n\ninstrument_list = sorted(meta_df['Instrument (in full)'].unique())\ninstrument_list\n\n['Accordion',\n 'Alto Saxophone',\n 'Bass Tuba',\n 'Bassoon',\n 'Cello',\n 'Clarinet in Bb',\n 'Contrabass',\n 'Flute',\n 'French Horn',\n 'Guitar',\n 'Harp',\n 'Oboe',\n 'Trombone',\n 'Trumpet in C',\n 'Viola',\n 'Violin']\n\n\n\npredict, predict_label = orchidea_confusion_matrix(model, \n                                prediction_generator, instrument_list)\n\n2022-08-08 07:37:20.613548: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 271360000 exceeds 10% of free system memory.\n2022-08-08 07:37:20.955501: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 271360000 exceeds 10% of free system memory.\n2022-08-08 07:37:21.382134: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory.\n2022-08-08 07:37:21.382196: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory.\n2022-08-08 07:37:21.382234: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory.\n2022-08-08 07:37:22.156406: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n2022-08-08 07:37:25.464340: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-08 07:37:25.467227: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-08 07:37:25.467562: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n2022-08-08 07:37:25.469313: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2022-08-08 07:37:25.469628: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\nRelying on driver to perform ptx compilation. \nModify $PATH to customize ptxas location.\nThis message will be only logged once.\n\n\n17/17 [==============================] - 87s 3s/step\n\n\n\n\n\n\n\nERROR\nFurther inspection is needed, as the model converged to predicting the same class at the end od epoch."
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/music_transcription_RNN.html",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/music_transcription_RNN.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import Input\nfrom keras import backend as K\nfrom keras import layers, models\nfrom keras.layers import LSTM, Dense\nfrom keras.models import Model, Sequential\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n\nfrom spec_generator_sequence import spec_generator\nfrom spec_generator_sequence_multilabel import (spec_generator_multi,\n                                                spec_generator_multioutput)\nfrom spec_input_generator import gen, gen_eval\nfrom spectrogram_class import spectrogram"
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/music_transcription_RNN.html#understanding-unrolled-lstm-under-10-seconds",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/music_transcription_RNN.html#understanding-unrolled-lstm-under-10-seconds",
    "title": "Most interesting blog on Earth!",
    "section": "Understanding unrolled LSTM, under 10 seconds!",
    "text": "Understanding unrolled LSTM, under 10 seconds!\n\n\n\nImage from this fantastic blog.\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nIf you are still here"
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/music_transcription_RNN.html#understanding-lstm-under-1-minute",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/music_transcription_RNN.html#understanding-lstm-under-1-minute",
    "title": "Most interesting blog on Earth!",
    "section": "Understanding LSTM, under 1 minute!",
    "text": "Understanding LSTM, under 1 minute!\nLSTM, or more generally RNN, is useful to find the relation between sequential data. Instead of having a fixed length of inputs, and trying to learn the weights for each input in a model, RNN uses previous inputs as a predictor for current prediction!\nEach RNN cells can have 2 outputs (or 3 for LSMT, more on that later), and the first output will be feed in to the next LSTM cell, undergoes matrix multiplication, and combined with the input of the next sequential timestep. The cycle goes on. The final output will then be fed into the next layer\nIf the parameter return_sequence of LSTM is set to true, then each LSTM cell will have a second output, to be fed to the next layer, hence we will have a output with the same length along the sequence. This architecture is called an unrolled LSTM.\nIn this notebook, we will be using the single output version of LSTM, then feeding the output to layers of Dense layers, to classify the instrument and notes of the audio files.\nSummary: * LSTM works will flexible length of input data * LSTM uses the previous data as input, transformed using kernel to be learned * LSTM can produce output of same sequence length.\n\n\ntf.test.is_gpu_available()\n\nWARNING:tensorflow:From /tmp/ipykernel_4020/4084812110.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\n\n\n2022-08-07 18:41:28.718001: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-07 18:41:28.791105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:28.828697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:28.829898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.878813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.879839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.880616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-07 18:41:29.881097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3370 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\nTrue\n\n\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\n\n\nmeta_df.shape\n\n(13162, 17)\n\n\n\nmeta_df = meta_df[~meta_df['Pitch ID (if applicable)'].isna()]\n\n\nmeta_df.isna().sum()\n\nPath                              0\nFamily (abbr.)                    0\nFamily (in full)                  0\nInstrument (abbr.)                0\nInstrument (in full)              0\nTechnique (abbr.)                 0\nTechnique (in full)               0\nPitch                             0\nPitch ID (if applicable)          0\nDynamics                          0\nDynamics ID (if applicable)     568\nInstance ID                       0\nMute (abbr.)                      0\nMute (in full)                    0\nString ID (if applicable)      5666\nNeeded digital retuning           0\nFold                              0\ndtype: int64\n\n\n\nmeta_df.shape\n\n(13162, 17)\n\n\n\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\nAs of before, the columns we are interested in (instruments and pitch id) doesnt have any null vales, we can just perform the train test split and the rest of the preprocessing is taken by our spectrogram class module.\n\nfrom random import random\n\n\ntrain_df, test_df = train_test_split(meta_df, stratify=meta_df['Instrument (in full)'], \n                                            train_size=0.7, random_state= 42)\n\n\nmulti_generator = spec_generator_multi(train_df, 32)\n\n\n_, num_target = multi_generator.__getitem__(2)[1].shape\nprint(num_target)\n\n107\n\n\n\n_, num_row, num_col= multi_generator.__getitem__(2)[0].shape\nprint(num_row)\nprint(num_col)\n\n500\n256"
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/music_transcription_class.html",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/music_transcription_class.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "!which python\n\n/home/shiya/anaconda3/envs/music/bin/python\n\n\n\nimport glob\nimport os\nimport random\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.stats\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import datasets, layers, models\n\nfrom spec_generator_sequence import spec_generator\nfrom spectrogram_class import spectrogram\n\n\nPreprocessing\nThis is a successive notebook after the EDA notebooks.\nAfter the EDA analysis, we can proceed to perform the preprocessing for audio. We will be working will spectrogram, as a spectrogram contains the spatial and temporal information of a audio files.\nWe will be utilizing Librosa package for spectrogram generation and augmentation. For easier implementation, we have defined a spectrogram class to perform all the procedures. Please refer to spectrogram_class.py in the same folder for more information.\n\nspectrogram\n\nspectrogram_class.spectrogram\n\n\nLets have a look at what a spectrogram looks like\n\nprint(tf.config.list_physical_devices('GPU'))\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n2022-08-08 01:38:04.723562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:04.752704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:04.753369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\nhop_length = 4096\nwin_length = 1024\nn_fft = 1024\n\nWe have randomly picked a audio file in OrchideaSOL to showcase the spectrogram.\n\n%%time\ntest_spec = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\n\n# To demonstrate that the test_spec is an instance on our custom defined class\n# let's check its type!\ntype(test_spec)\n\nCPU times: user 47.1 ms, sys: 76.8 ms, total: 124 ms\nWall time: 302 ms\n\n\nspectrogram_class.spectrogram\n\n\nWe have a support function in the class to help us visualized the spectrogram, now lets have a look at what a harps looks like in spectrogram\n\ntest_spec.plot_spec() \n\n\n\n\nNow I owe you some explanation, why am I showing you this? There are some important points to observe:\n\nYou probably heard about the relation between a note and its soundwave frequency, for example, in piano the middle G3 note corresponds to 196 HZ. But we are not exactly seeing one horizontal line on 196 HZ. Why is that? The main reason is that the magnitude of sound wave/magnitude is additive! Lets have a closer look:\n\n\n\n\n(screenshot captured from our lord and savior, 3Blue1Brown.\n\nAlthought the resulting soundwave is of frequency of 196 HZ, it is actually a combination of soundwave of different frequencies. This is exactly the same note for piano and violin sounds different! Imagine a world where a tuba, harp, tuna and your cat sound the same, what a horrible world\n\n\nThis is actually why throughout the project, we have decided to use neural network to perform the music transcription, a convolutional network or RNN are able to capture the feature between spatial and temporal information across the spectrogram.\n\n\n\n(A demonstration of how a CNN is able to capture different features on a face in different layers.)\nBut before we can use the dataset into directly, we have to consider about the data augmentation. Our model should be able to perform tasks include:\n\nTranscribe music in a noisy environment\nTranscribe music is moderate data loss\nStill be able to tell both audio files contain violins, although the violins have different sound signature\n\nBut how can our model learn these difference if our data is perfectly clean, and only contain audio files from the same instruments. That is where data augmentation comes in.\n\n\nData augmentation\nWe have to augmentate our data so that our training dataset match the scenario above. To achieve this, we have decided to add noise, masking and shifting to the spectrogram.\nAll of these function had been defined under the spectrogram class. Now lets try to visualize this.\n\nAdding noise\nRemeber this is how our signal and spectrogram for harps looks like:\n\ndef plt_signal_spec(spec, xlim = None):\n    fig = plt.figure(figsize = (14, 6))\n    ax_1 = fig.add_subplot(121)\n    ax_1.plot(spec.signal)\n    if xlim:\n        ax_1.set_xlim((0, xlim))\n    \n    ax_2 = fig.add_subplot(122)\n    spec.plot_spec(ax = ax_2, db_off = True)\n    if xlim:\n        ax_2.set_xlim((0, xlim))\n    plt.show()\n\n\nplt_signal_spec(test_spec, xlim = 2000)\n\n\n\n\nThe figure on the left is the signal directly converted from raw audio file, which represents the magnitude/pressure of the recorded audio. Whereas the figure on the right is the corresponding converted spectrogram.\nWe will then add random noise to the signal, before converting to the spectrogram, the signal is added based on the normal distribution of the maximum value of the signal.\n\ntest_spec_noise = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\ntest_spec_noise.add_noise(noise_factor = 0.2)\n\n\nplt_signal_spec(test_spec_noise, xlim = 2000)\n\n\n\n\nNow there is some fuzziness going on in the signal! However, since the magnitude of the mel spectrogram is log scaled, it isnt obvious in the right figure.\n\nNote that the in practice, we dont add so much fuzziness in our training, the increased noise factor is for demonstration purpose only.\n\n\n\nSpectrogram masking\nAnother technique we used is the spectrogram masking, the masking function will randomly set the magnitude of spectrogram into 0, across the time and frequency of the spectrogram.\n\ntest_spec_mask = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\ntest_spec_mask.mask_spec()\n\n\nplt_signal_spec(test_spec_mask, xlim = 2000)\n\n\n\n\nNote how we have remove chunks of the frequency and time dimension of the spectrogram. If its not, run a few more time, its random anyway.\n\n\nSpectrogram shifting\nSince in our audio files, the recording always starts at the beginning, and there is always a few seconds of silence before the recording ends, we will need to shift the spectrogram, so that the machine learning models doesnt depends too much on the beginning of the time slices.\n\ntest_spec_shift = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', preprocess = False, \n                           trunc_off = True)\ntest_spec_shift.shift_spec(max_sec = 5)\n\n\nplt_signal_spec(test_spec_shift, xlim = 2000)\n\n\n\n\nNote the now the spectrogram is now shift to the right of time, we have set the maximum time it can shift for 5 seconds for demonstration purpose.\n\n\nSpectrogram truncation\nSince we have audio files of different length, we will need to preprocess it in a way that all of the sample have the same dimension as a numpy array. To do this we will simply be padding the numpy array as zeros, or trimming the last few seconds of the recording. The truncation is applied by default when the spectrogram class initialization is called.\nNow to get the size of our input (to be fed into the model), we wil simply run a sample, and acquire the shape from the numpy array.\n\n%%time\nsample = spectrogram('PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', \n                    hop_length = hop_length, n_mels = 512,\n                    n_fft = n_fft)\n\nCPU times: user 143 ms, sys: 80 ms, total: 223 ms\nWall time: 325 ms\n\n\n\nspec_shape = sample.spec.shape\nspec_shape\n\n(512, 500)\n\n\nNow that we have the spectrogram ready for out training, we can use the instrument and pitch data in the metadata dataframe.\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\nmeta_df.head()\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n    \n      2\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#1\n      34.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      1\n    \n    \n      3\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#1\n      34.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      2\n    \n    \n      4\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#2\n      46.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      1\n    \n  \n\n\n\n\n\nmeta_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13265 entries, 0 to 13264\nData columns (total 17 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   Path                         13265 non-null  object \n 1   Family (abbr.)               13265 non-null  object \n 2   Family (in full)             13265 non-null  object \n 3   Instrument (abbr.)           13265 non-null  object \n 4   Instrument (in full)         13265 non-null  object \n 5   Technique (abbr.)            13265 non-null  object \n 6   Technique (in full)          13265 non-null  object \n 7   Pitch                        13265 non-null  object \n 8   Pitch ID (if applicable)     13162 non-null  float64\n 9   Dynamics                     13265 non-null  object \n 10  Dynamics ID (if applicable)  12646 non-null  float64\n 11  Instance ID                  13262 non-null  float64\n 12  Mute (abbr.)                 13265 non-null  object \n 13  Mute (in full)               13265 non-null  object \n 14  String ID (if applicable)    7516 non-null   float64\n 15  Needed digital retuning      13265 non-null  bool   \n 16  Fold                         13265 non-null  int64  \ndtypes: bool(1), float64(4), int64(1), object(11)\nmemory usage: 1.6+ MB\n\n\n\nmeta_df.describe()\n\n\n\n\n\n  \n    \n      \n      Pitch ID (if applicable)\n      Dynamics ID (if applicable)\n      Instance ID\n      String ID (if applicable)\n      Fold\n    \n  \n  \n    \n      count\n      13162.000000\n      12646.000000\n      13262.000000\n      7516.000000\n      13265.000000\n    \n    \n      mean\n      63.842653\n      2.073857\n      0.848138\n      2.360298\n      2.000000\n    \n    \n      std\n      16.512067\n      1.329919\n      1.177874\n      1.196041\n      1.414267\n    \n    \n      min\n      20.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n    \n    \n      25%\n      52.000000\n      2.000000\n      0.000000\n      1.000000\n      1.000000\n    \n    \n      50%\n      64.000000\n      2.000000\n      0.000000\n      2.000000\n      2.000000\n    \n    \n      75%\n      76.000000\n      3.000000\n      2.000000\n      3.000000\n      3.000000\n    \n    \n      max\n      109.000000\n      4.000000\n      12.000000\n      6.000000\n      4.000000\n    \n  \n\n\n\n\n\nmeta_df.isnull().sum()\n\nPath                              0\nFamily (abbr.)                    0\nFamily (in full)                  0\nInstrument (abbr.)                0\nInstrument (in full)              0\nTechnique (abbr.)                 0\nTechnique (in full)               0\nPitch                             0\nPitch ID (if applicable)        103\nDynamics                          0\nDynamics ID (if applicable)     619\nInstance ID                       3\nMute (abbr.)                      0\nMute (in full)                    0\nString ID (if applicable)      5749\nNeeded digital retuning           0\nFold                              0\ndtype: int64\n\n\n\nmeta_df['Instrument (in full)'].value_counts()\n\nViolin            1987\nViola             1952\nContrabass        1636\nCello             1593\nAccordion          872\nTrombone           670\nTrumpet in C       590\nFrench Horn        589\nFlute              529\nHarp               507\nBass Tuba          500\nClarinet in Bb     406\nAlto Saxophone     377\nBassoon            358\nGuitar             353\nOboe               346\nName: Instrument (in full), dtype: int64\n\n\nWe donreally care about the pitchID, Dynamics Id and String ID.\nWe can see some degree of bias in the instrument classes.\n\n\n\nModel fitting\n\ntrain_df, test_df = train_test_split(meta_df, train_size = 0.7, random_state = 42)\n\n\nprint('The number of rows for the training data is ', train_df.shape[0])\nprint('The number of rows for the test data is ', test_df.shape[0])\n\nThe number of rows for the training data is  9285\nThe number of rows for the test data is  3980\n\n\n\nBATCH_SIZE = 32\n\ntrain_generator = tf.data.Dataset.from_generator(lambda: spec_generator(train_df, BATCH_SIZE, \n                    add_channel = True, live_generation = True), \n                    output_types=(tf.float32, tf.int32),\n                    output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                    (BATCH_SIZE, 16))).prefetch(5)\neval_generator = tf.data.Dataset.from_generator(lambda: spec_generator(test_df, BATCH_SIZE, \n                    add_channel = True, live_generation = True,), \n                    output_types=(tf.float32, tf.int32), \n                    output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), \n                    (BATCH_SIZE, 16))).prefetch(5)\n\n2022-08-08 01:38:09.491735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-08 01:38:09.493556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:09.494346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:09.495138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.299483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.299828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.300078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-08 01:38:10.300294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3370 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n\ntrain_generator\n\n<PrefetchDataset element_spec=(TensorSpec(shape=(32, 512, 500, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 16), dtype=tf.int32, name=None))>\n\n\n\nmodel = models.Sequential()\nmodel.add(layers.InputLayer((spec_shape[0], spec_shape[1], 1), batch_size = BATCH_SIZE, \n                                    dtype = tf.float32))\nmodel.add(layers.Conv2D(15, (15, 200), strides=(10, 10), activation='relu'))\n# , input_shape = (spec_shape[0], spec_shape[1], 1)))\nmodel.add(layers.MaxPool2D((5, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(16, activation = 'sigmoid'))\nmodel.build()\n\n\ntf.keras.utils.plot_model(model, show_shapes = True, show_dtype= True)\n\n\n\n\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\nprint(model.metrics)\n\n[]\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (32, 50, 31, 15)          45015     \n                                                                 \n max_pooling2d (MaxPooling2D  (32, 10, 15, 15)         0         \n )                                                               \n                                                                 \n flatten (Flatten)           (32, 2250)                0         \n                                                                 \n dense (Dense)               (32, 16)                  36016     \n                                                                 \n=================================================================\nTotal params: 81,031\nTrainable params: 81,031\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom datetime import datetime\ndatetime.now().strftime('%Y%m%d_%H%M%S')\n\n'20220808_013811'\n\n\n\ntrain_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      4851\n      Strings/Contrabass/pizzicato_bartok/Cb-pizz_ba...\n      Strings\n      Violin Family\n      Cb\n      Contrabass\n      pizz_bartok\n      pizzicato_bartok\n      F3\n      53.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      1.0\n      False\n      1\n    \n    \n      12565\n      Winds/Oboe+sordina/ordinario/Ob+S-ord-E6-mf-N-...\n      Winds\n      Woodwinds\n      Ob\n      Oboe\n      ord\n      ordinario\n      E6\n      88.0\n      mf\n      2.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      4\n    \n  \n\n\n\n\n\n%load_ext tensorboard\n\n\n# ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n#     f\"../models/baseline_checkpoint/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{{epoch:02d}}_model\", \n#                                                     monitor='val_accuracy')\n# early_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 2, \n#                                                         restore_best_weights = True)\n# log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n\n# history = model.fit(train_generator, epochs = 10, verbose=1, \n#                     validation_data = eval_generator, \n#                     validation_freq= 1, \n#                     # use_multiprocessing=True, workers = 2, \n#                     callbacks=[ckpt_callback, early_callback, tensorboard_callback])\n\n\n# model.save('../models/new_baseline/')\n\nNow that we have an working model, we need to save the model and the history object, we have defined a short checkpoint callback to save the model automatically, now lets define a function to save the history.\n\nimport pickle\n\ndef save_history(history, path):\n    with open(path, 'wb+') as f:\n        pickle.dump(history, f)\n        \ndef load_history(path):\n    with open(path, 'rb') as f:\n        return pickle.load(f)\n\n\n# save_history(history.history, '../models/new_baseline/history.pkl')\n\n\nhistory = load_history('../models/new_baseline/history.pkl')\n\n\nmodel = tf.keras.models.load_model('../models/new_baseline/')\n\n\nhistory.keys()\n\ndict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n\n\n\n\nModel evaluation\nIts time to check if our model is good! drum rolling\n\nplt.plot(history['val_accuracy'], label = 'Validation accuracy')\nplt.plot(history['accuracy'], label = 'Training accuracy')\nplt.title('Accuracy of baseline model after early callback')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nThe figure above shows the both the training and testing accuracy for our instrument classification. We can see that the testing accuracy starts to decrease after 2 epoch, and only peaks at 26% accuracy.\nOur current options are to apply dropout layer or regularizer for out current layers. However, due to the time constrant, we have decided to apply these technique on more complicated models, as this baseline model should only be serving the purpose of quick gauge of how well CNN can perform on out dataset.\nThis indicate the limitation of our fairly simple baseline model. To get an idea of how the classification if performed over class, lets have a look at the prediction across all classes.\nNow we are interested in the confusion matrix to represents the prediction for each class, due to gpu memory limitation, we will only be taking the snapshot of the test data as the classification report.\n\nprediction_generator = spec_generator(test_df, test_df.shape[0] // 5, add_channel=True,\n                                        live_generation = True, preprocess = False)\n\n\ninstrument_list = sorted(meta_df['Instrument (in full)'].unique())\ninstrument_list\n\n['Accordion',\n 'Alto Saxophone',\n 'Bass Tuba',\n 'Bassoon',\n 'Cello',\n 'Clarinet in Bb',\n 'Contrabass',\n 'Flute',\n 'French Horn',\n 'Guitar',\n 'Harp',\n 'Oboe',\n 'Trombone',\n 'Trumpet in C',\n 'Viola',\n 'Violin']\n\n\n\ndef orchidea_confusion_matrix(model, generator, instrument_list):\n    '''\n    Plot confusion matrix for OrchideaSOL dataset\n    \n    Input:\n    model: Model to be used\n    generator: Sequence class, generator to generate feature and labels for \n                OrchideaSOL dataset\n    instrument_list: list of instrument in alphabetical order, used to label the plot\n    \n    Output:\n    predict: np.array, Predicted output\n    prediction_label: np.array, True label\n    '''\n    prediction_feature, prediction_label = generator.__getitem__(0)\n    predict = predict = model.predict(prediction_feature)\n    assert prediction_label.shape == predict.shape\n    from sklearn.metrics import confusion_matrix\n\n    plt.figure(figsize = (12, 10))\n    sns.heatmap(confusion_matrix(np.argmax(prediction_label, axis=1), \n                        np.argmax(predict, axis = 1)), annot = True, \n                        xticklabels=instrument_list, \n                        yticklabels=instrument_list)\n    plt.title('Confusion matrix for baseline model')\n    plt.show()\n\n    return predict, prediction_label\n\n\npredict, prediction_label = orchidea_confusion_matrix(model, \n                                prediction_generator, instrument_list)\n\n25/25 [==============================] - 1s 23ms/step\n\n\n\n\n\nAccording to our baseline model, everything is a violin or viola, with the exception of good prediction on controbass. We will need to adjsut our loss function by assinging weighted entropy, or by creating more data for other classes using data augmentation.\nHowever, the last option is unfavourable, since it will increase the training time per epochs.\n\nprint(classification_report(np.argmax(prediction_label, axis=1), \n                        np.argmax(predict, axis = 1)))\n\n              precision    recall  f1-score   support\n\n           0       0.50      0.02      0.03        65\n           1       0.00      0.00      0.00        25\n           2       0.00      0.00      0.00        32\n           3       0.00      0.00      0.00        20\n           4       0.15      0.06      0.08        88\n           5       0.17      0.04      0.07        23\n           6       0.36      0.45      0.40        84\n           7       0.14      0.06      0.08        36\n           8       0.67      0.06      0.10        36\n           9       0.33      0.05      0.08        21\n          10       0.33      0.11      0.17        27\n          11       0.00      0.00      0.00        18\n          12       0.33      0.23      0.27        35\n          13       0.40      0.06      0.10        34\n          14       0.20      0.67      0.31       119\n          15       0.21      0.29      0.24       133\n\n    accuracy                           0.23       796\n   macro avg       0.24      0.13      0.12       796\nweighted avg       0.25      0.23      0.17       796\n\n\n\nThe precisio nnad recall is bad all across the board, with the exception of 67% recall on instrument 14 (viola)"
  },
  {
    "objectID": "posts/music_backup/Music_transcription_fastai/notebooks/test.html",
    "href": "posts/music_backup/Music_transcription_fastai/notebooks/test.html",
    "title": "Most interesting blog on Earth!",
    "section": "",
    "text": "import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nfrom sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n\nfrom spec_generator_sequence import _get_spec, spec_generator\nfrom spec_generator_sequence_multilabel import (spec_generator_multi,\n                                                spec_generator_multioutput)\nfrom spec_input_generator import gen, gen_eval\nfrom spectrogram_class import spectrogram\nfrom classic_generator import classic_generator\n\n\nfrom classic_generator import _instrument_label_generator, get_full_path, classic_train_generator, classic_generator\n\n\nget_full_path('/train_data/2335.wav', mode='train')\n\n'/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav'\n\n\n\ny, sr = librosa.load('/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav', \n                        sr = 44100)\n\n\nplt.plot(y)\n\n\n\n\n\nmel_spec = librosa.feature.melspectrogram(y, n_mels = 128, sr = 44100)\n\n\nsns.heatmap(mel_spec)\n\n<AxesSubplot:>\n\n\n\n\n\n\nlibrosa.display.specshow(mel_spec)\n\n<matplotlib.collections.QuadMesh at 0x7fa87c058e20>\n\n\n\n\n\n\nclassic_generator_test = classic_generator(batch_size = 1)\n\n\nx_list = [path.rsplit('/', 1)[-1].rsplit('.')[0] for path in classic_generator_test.x]\ny_list = [path.rsplit('/', 1)[-1].rsplit('.')[0] for path in classic_generator_test.y]\n\n\nx_list == y_list\n\nTrue\n\n\n\nclassic_generator_test.y[0]\n\n'/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_labels/1727.csv'\n\n\n\nclassic_generator_test.x[122]\n\n'/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2366.wav'\n\n\n\n# classic_train_generator('/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav')\n\n\ntest_classic_gen = classic_generator_test.__getitem__(3)\n\n\ntest_classic_gen[0][0].shape\n\n(128, 200, 1)\n\n\n\ntest_classic_gen[1]['instrument_1'].shape\n\n(1, 200, 83)\n\n\n\nsns.heatmap(np.squeeze(test_classic_gen[0][0], -1))\n\n<AxesSubplot:>\n\n\n\n\n\n\ntest_classic_gen[0][0].shape\n\n(3769, 128, 1)\n\n\n\nsns.heatmap(test_classic_gen[1]['1'][1])\n\n\n%run classic_generator\n\n\ndef ger_instrument_frame(file, ins, num_freq, num_time):\n    _df = pd.read_csv(file)\n    _df = _df[_df['instrument'] == ins]\n    tmp_arr = np.zeros((num_freq, num_time))\n    for i in _df.iterrows():\n        start_time = i['start_time']\n\n/bin/bash: /home/shiya/anaconda3/envs/music/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/home/shiya/Documents/music_transcription/notebooks\n\n\n\ncwd = os.getcwd()\n\n\ncwd\n\n'/home/shiya/Documents/music_transcription/notebooks'\n\n\n\nos.path.join(os.getcwd(), '/../data/classic/musicnet/train_labels/1727.csv')\n\n'/../data/classic/musicnet/train_labels/1727.csv'\n\n\n\nlibrosa.get_duration(filename = '../data/classic/musicnet/train_data/1727.wav')\n\n447.0595918367347\n\n\n\n'../data/classic/musicnet/train_labels/1727.csv'.rsplit('/', maxsplit=1)\n\n['../data/classic/musicnet/train_labels', '1727.csv']\n\n\n\nos.path.join('../data/classic/musicnet/train_labels/', \n                '../train_data/', \n                '1727.wav')\n\n'../data/classic/musicnet/train_labels/../train_data/1727.wav'\n\n\n\ntest_inst_generator = _instrument_label_generator('../data/classic/musicnet/train_labels/1727.csv', 1, 9000, mode = 'train')\n\n\nsns.heatmap(test_inst_generator)\n\n<AxesSubplot:>\n\n\n\n\n\n\nclassic_label = pd.read_csv('../data/classic/musicnet/train_labels/1727.csv')\nclassic_label.head()\n\n\n\n\n\n  \n    \n      \n      start_time\n      end_time\n      instrument\n      note\n      start_beat\n      end_beat\n      note_value\n    \n  \n  \n    \n      0\n      9182\n      90078\n      43\n      53\n      4.0\n      1.5\n      Dotted Quarter\n    \n    \n      1\n      9182\n      33758\n      42\n      65\n      4.0\n      0.5\n      Eighth\n    \n    \n      2\n      9182\n      62430\n      1\n      69\n      4.0\n      1.0\n      Quarter\n    \n    \n      3\n      9182\n      202206\n      44\n      41\n      4.0\n      3.5\n      Whole\n    \n    \n      4\n      9182\n      62430\n      1\n      81\n      4.0\n      1.0\n      Quarter\n    \n  \n\n\n\n\n\nclassic_1_inst = classic_label[classic_label['instrument'] == 1]\nclassic_1_inst.head()\n\n\n\n\n\n  \n    \n      \n      start_time\n      end_time\n      instrument\n      note\n      start_beat\n      end_beat\n      note_value\n    \n  \n  \n    \n      2\n      9182\n      62430\n      1\n      69\n      4.0\n      1.0\n      Quarter\n    \n    \n      4\n      9182\n      62430\n      1\n      81\n      4.0\n      1.0\n      Quarter\n    \n    \n      7\n      62430\n      119774\n      1\n      84\n      5.0\n      1.0\n      Quarter\n    \n    \n      8\n      62430\n      119774\n      1\n      72\n      5.0\n      1.0\n      Quarter\n    \n    \n      11\n      119774\n      145886\n      1\n      74\n      6.0\n      0.5\n      Eighth\n    \n  \n\n\n\n\n\nfor i in classic_label.head(2).iterrows():\n    print(i)\n    print(type(i[1]))\n\n(0, start_time              9182\nend_time               90078\ninstrument                43\nnote                      53\nstart_beat               4.0\nend_beat                 1.5\nnote_value    Dotted Quarter\nName: 0, dtype: object)\n<class 'pandas.core.series.Series'>\n(1, start_time      9182\nend_time       33758\ninstrument        42\nnote              65\nstart_beat       4.0\nend_beat         0.5\nnote_value    Eighth\nName: 1, dtype: object)\n<class 'pandas.core.series.Series'>\n\n\n\nclassic_spec = spectrogram('../data/classic/musicnet/train_data/1727.wav', \n                            trunc_off=True)\n\n\nlabel_list = os.listdir('../data/classic/musicnet/train_labels/')\nlabel_list[:3]\n\n['2422.csv', '2114.csv', '2335.csv']\n\n\n\ndf_list = []\nfor i in label_list:\n    df_list.append(pd.read_csv('../data/classic/musicnet/train_labels/' + i))\nlabel_df = pd.concat(df_list)\nlabel_df.head(2)\n\n\n\n\n\n  \n    \n      \n      start_time\n      end_time\n      instrument\n      note\n      start_beat\n      end_beat\n      note_value\n    \n  \n  \n    \n      0\n      90078\n      124382\n      1\n      60\n      0.5\n      0.489583\n      Quarter\n    \n    \n      1\n      124382\n      138718\n      1\n      65\n      1.0\n      0.489583\n      Quarter\n    \n  \n\n\n\n\n\nlabel_df['instrument'].unique()\n\narray([ 1, 43, 41, 61, 71, 72, 74, 69, 42, 44,  7])\n\n\n\nlen(label_df['note'].unique())\n\n83\n\n\n\n19233758/ len(classic_spec.signal) * 9627\n\n9391.849238622863\n\n\n\n19421150/ len(classic_spec.signal) * 9627\n\n9483.352802956157\n\n\n\nclassic_spec.sr\n\n44100\n\n\n\nclassic_spec.spec.shape\n\n(256, 9627)\n\n\n\ny = np.array([['a', 'r'], ['b', 'q'], ['c', 'z']])\n\nnb = MultiLabelBinarizer()\nnb.fit(y)\n\nnb.transform(np.array([['a', np.nan], ['d']]))\n\narray([[1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0]])\n\n\n\nmeta_df = pd.read_csv('../data/OrchideaSOL_metadata.csv')\n\n\nmultioutput_generator = spec_generator_multioutput(meta_df, 32)\n\n\nmultioutput_generator.__getitem__(2)[1][1].shape\n\nKeyError: 1\n\n\n\nmeta_df['Pitch ID (if applicable)'][meta_df['Pitch ID (if applicable)'].isna()]\n\n138     NaN\n142     NaN\n233     NaN\n234     NaN\n235     NaN\n         ..\n13027   NaN\n13028   NaN\n13029   NaN\n13030   NaN\n13043   NaN\nName: Pitch ID (if applicable), Length: 103, dtype: float64\n\n\n\ngenerator = spec_generator(meta_df, 32)\n\n\nprint(generator.indices)\n\n[9012 3451 6714 ... 9387 4120 7367]\n\n\n\n\n%run spec_generator_sequence_multilabel.py\n\n\ngenerator\n\n<spec_generator_sequence.spec_generator at 0x7f58509d07c0>\n\n\n\ngenerate_multi = spec_generator_multi(meta_df, 32)\n\n\ngenerate_multi.__getitem__(2)[1].shape\n\n(32, 107)\n\n\n\ngenerate_multi.\n\nSyntaxError: invalid syntax (446800042.py, line 1)\n\n\n\nimport random\n\n\n\n%%timeit\nrandom_num = random.randint(1, 50)\ngenerator.__getitem__(random_num)[1][1]\n\n40.6 ms  8.01 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n\n\n\ntest = pd.DataFrame({'test':['d', 'z', 'r', 'e', 'y']})\n\n\ntest\n\n\n\n\n\n  \n    \n      \n      test\n    \n  \n  \n    \n      0\n      d\n    \n    \n      1\n      z\n    \n    \n      2\n      r\n    \n    \n      3\n      e\n    \n    \n      4\n      y\n    \n  \n\n\n\n\n\nhot = OneHotEncoder(sparse=False)\nhot.fit_transform(test)\n\narray([[1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       [0., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0.]])\n\n\n\nhot.categories_\n\n[array(['d', 'e', 'r', 'y', 'z'], dtype=object)]\n\n\n\n(np.random.randint(0, 2, size=10000) == np.random.randint(0, 2, size=10000)).mean()\n\n0.4947\n\n\n\nhop_length = 2048\nwin_length = 512\nn_fft = 1024\n\n\nmeta_df['Path'].sample(1).values[0]\n\n'Brass/Trumpet_C+sordina_wah/flatterzunge_open/TpC+SW-flatt_open-G#3-mf-N-N.wav'\n\n\n\n%%timeit\npath = meta_df['Path'].sample(1).values[0]\ntest = _get_spec('Winds/Flute/ordinario/Fl-ord-D6-ff-N-T20d.wav', test_verbose=False)\n\n962 s  37.6 s per loop (mean  std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%run spectrogram_class.py\n\nAttributeError: 'spectrogram' object has no attribute 'spec'\n\n\n\nmeta_df.head(2)\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      0\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      f\n      3.0\n      0.0\n      S\n      Sordina\n      NaN\n      False\n      2\n    \n    \n      1\n      Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      ord\n      ordinario\n      A#0\n      22.0\n      p\n      1.0\n      0.0\n      S\n      Sordina\n      NaN\n      True\n      0\n    \n  \n\n\n\n\n\nmeta_test = meta_df[['Instrument (in full)']]\n\n\none_hot = OneHotEncoder(sparse= False)\n\none_hot.fit_transform(meta_test)\n\narray([[0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.]])\n\n\n\nmeta_df['Path'][2:6].values\n\narray(['Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#1-f-N-T20u.wav',\n       'Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#1-p-N-T22u.wav',\n       'Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#2-f-N-T29u.wav',\n       'Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#2-p-N-T31u.wav'],\n      dtype=object)\n\n\n\none_hot.categories_\n\n[array(['Accordion', 'Alto Saxophone', 'Bass Tuba', 'Bassoon', 'Cello',\n        'Clarinet in Bb', 'Contrabass', 'Flute', 'French Horn', 'Guitar',\n        'Harp', 'Oboe', 'Trombone', 'Trumpet in C', 'Viola', 'Violin'],\n       dtype=object)]\n\n\n\nmeta_freq = 1/meta_df.groupby('Instrument (in full)')['Instrument (in full)'].transform('count')\n\n\nmeta_freq\n\n0        0.002000\n1        0.002000\n2        0.002000\n3        0.002000\n4        0.002000\n           ...   \n13260    0.002653\n13261    0.002653\n13262    0.002653\n13263    0.002653\n13264    0.002653\nName: Instrument (in full), Length: 13265, dtype: float64\n\n\n\n%%time\nmeta_df.sample(32, )\n            # replace = True, \n            # weights=meta_freq)[['Instrument (in full)']].value_counts(normalize=True)\n\nCPU times: user 1.75 ms, sys: 0 ns, total: 1.75 ms\nWall time: 1.54 ms\n\n\n\n\n\n\n  \n    \n      \n      Path\n      Family (abbr.)\n      Family (in full)\n      Instrument (abbr.)\n      Instrument (in full)\n      Technique (abbr.)\n      Technique (in full)\n      Pitch\n      Pitch ID (if applicable)\n      Dynamics\n      Dynamics ID (if applicable)\n      Instance ID\n      Mute (abbr.)\n      Mute (in full)\n      String ID (if applicable)\n      Needed digital retuning\n      Fold\n    \n  \n  \n    \n      978\n      Brass/Horn/sforzato/Hn-sfz-F#2-f-N-N.wav\n      Brass\n      Brass\n      Hn\n      French Horn\n      sfz\n      sforzato\n      F#2\n      42.0\n      f\n      3.0\n      0.0\n      N\n      None\n      NaN\n      False\n      0\n    \n    \n      748\n      Brass/Horn/flatterzunge_stopped/Hn-flatt_stopp...\n      Brass\n      Brass\n      Hn\n      French Horn\n      flatt_stopped\n      flatterzunge_stopped\n      B3\n      59.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      4\n    \n    \n      7327\n      Strings/Viola/sul_tasto_tremolo/Va-tasto_trem-...\n      Strings\n      Violin Family\n      Va\n      Viola\n      tasto_trem\n      sul_tasto_tremolo\n      E5\n      76.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      True\n      0\n    \n    \n      2757\n      Keyboards/Accordion/ordinario/Acc-ord-D#4-mf-a...\n      Keyboards\n      Keyboards\n      Acc\n      Accordion\n      ord\n      ordinario\n      D#4\n      63.0\n      mf\n      2.0\n      4.0\n      N\n      None\n      NaN\n      False\n      4\n    \n    \n      3972\n      PluckedStrings/Harp/ordinario/Hp-ord-G#4-mf-N-...\n      PluckedStrings\n      Plucked Strings\n      Hp\n      Harp\n      ord\n      ordinario\n      G#4\n      68.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      4\n    \n    \n      7147\n      Strings/Viola/sul_ponticello/Va-pont-F#5-mf-3c...\n      Strings\n      Violin Family\n      Va\n      Viola\n      pont\n      sul_ponticello\n      F#5\n      78.0\n      mf\n      2.0\n      2.0\n      N\n      None\n      3.0\n      False\n      0\n    \n    \n      10638\n      Strings/Violoncello/pizzicato_secco/Vc-pizz_se...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      pizz_sec\n      pizzicato_secco\n      A3\n      57.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      1.0\n      False\n      0\n    \n    \n      7542\n      Strings/Viola/tremolo/Va-trem-D6-mf-1c-T12u.wav\n      Strings\n      Violin Family\n      Va\n      Viola\n      trem\n      tremolo\n      D6\n      86.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      True\n      3\n    \n    \n      7457\n      Strings/Viola/tremolo/Va-trem-C#5-pp-2c-N.wav\n      Strings\n      Violin Family\n      Va\n      Viola\n      trem\n      tremolo\n      C#5\n      73.0\n      pp\n      0.0\n      1.0\n      N\n      None\n      2.0\n      False\n      3\n    \n    \n      8171\n      Strings/Violin/artificial_harmonic/Vn-art_harm...\n      Strings\n      Violin Family\n      Vn\n      Violin\n      art_harm\n      artificial_harmonic\n      B7\n      107.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      False\n      3\n    \n    \n      12890\n      Winds/Sax_Alto/aeolian/ASax-aeol-A3-p-N-R100u.wav\n      Winds\n      Woodwinds\n      ASax\n      Alto Saxophone\n      aeol\n      aeolian\n      A3\n      57.0\n      p\n      1.0\n      0.0\n      N\n      None\n      NaN\n      True\n      2\n    \n    \n      482\n      Brass/Bass_Tuba/slap_pitched/BTb-slap-F#1-f-N-...\n      Brass\n      Brass\n      BTb\n      Bass Tuba\n      slap\n      slap_pitched\n      F#1\n      30.0\n      f\n      3.0\n      0.0\n      N\n      None\n      NaN\n      False\n      0\n    \n    \n      9260\n      Strings/Violin/sul_ponticello/Vn-pont-A3-mf-4c...\n      Strings\n      Violin Family\n      Vn\n      Violin\n      pont\n      sul_ponticello\n      A3\n      57.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      0\n    \n    \n      2346\n      Brass/Trumpet_C/slap_pitched/TpC-slap-G#4-p-N-...\n      Brass\n      Brass\n      TpC\n      Trumpet in C\n      slap\n      slap_pitched\n      G#4\n      68.0\n      p\n      1.0\n      0.0\n      N\n      None\n      NaN\n      False\n      1\n    \n    \n      3475\n      PluckedStrings/Guitar/ordinario_high_register/...\n      PluckedStrings\n      Plucked Strings\n      Gtr\n      Guitar\n      ord_hi_reg\n      ordinario_high_register\n      D6\n      86.0\n      mf\n      2.0\n      1.0\n      N\n      None\n      2.0\n      False\n      2\n    \n    \n      3089\n      Keyboards/Accordion/ordinario/Acc-ord-G1-pp-N-...\n      Keyboards\n      Keyboards\n      Acc\n      Accordion\n      ord\n      ordinario\n      G1\n      31.0\n      pp\n      0.0\n      0.0\n      N\n      None\n      NaN\n      False\n      0\n    \n    \n      8444\n      Strings/Violin/ordinario/Vn-ord-B4-mf-4c-N.wav\n      Strings\n      Violin Family\n      Vn\n      Violin\n      ord\n      ordinario\n      B4\n      71.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      1\n    \n    \n      5873\n      Strings/Viola+sordina/ordinario/Va+S-ord-G3-mf...\n      Strings\n      Violin Family\n      Va\n      Viola\n      ord\n      ordinario\n      G3\n      55.0\n      mf\n      2.0\n      3.0\n      S\n      Sordina\n      4.0\n      True\n      1\n    \n    \n      6683\n      Strings/Viola/pizzicato_l_vib/Va-pizz_lv-C#4-m...\n      Strings\n      Violin Family\n      Va\n      Viola\n      pizz_lv\n      pizzicato_l_vib\n      C#4\n      61.0\n      mf\n      2.0\n      2.0\n      N\n      None\n      3.0\n      False\n      2\n    \n    \n      9961\n      Strings/Violoncello+sordina_piombo/tremolo/Vc+...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      trem\n      tremolo\n      G#4\n      68.0\n      mf\n      2.0\n      0.0\n      SP\n      Piombo\n      1.0\n      False\n      0\n    \n    \n      1317\n      Brass/Trombone+sordina_wah/flatterzunge_open/T...\n      Brass\n      Brass\n      Tbn\n      Trombone\n      flatt_open\n      flatterzunge_open\n      B3\n      59.0\n      mf\n      2.0\n      0.0\n      SW\n      Wah\n      NaN\n      False\n      1\n    \n    \n      10597\n      Strings/Violoncello/pizzicato_l_vib/Vc-pizz_lv...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      pizz_lv\n      pizzicato_l_vib\n      F2\n      41.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      3\n    \n    \n      715\n      Brass/Horn/flatterzunge/Hn-flatt-F3-mf-N-N.wav\n      Brass\n      Brass\n      Hn\n      French Horn\n      flatt\n      flatterzunge\n      F3\n      53.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      2\n    \n    \n      6528\n      Strings/Viola/ordinario/Va-ord-G#3-pp-4c-T18u.wav\n      Strings\n      Violin Family\n      Va\n      Viola\n      ord\n      ordinario\n      G#3\n      56.0\n      pp\n      0.0\n      3.0\n      N\n      None\n      4.0\n      True\n      2\n    \n    \n      5542\n      Strings/Contrabass/tremolo/Cb-trem-B3-pp-1c-T1...\n      Strings\n      Violin Family\n      Cb\n      Contrabass\n      trem\n      tremolo\n      B3\n      59.0\n      pp\n      0.0\n      0.0\n      N\n      None\n      1.0\n      True\n      1\n    \n    \n      11158\n      Strings/Violoncello/tremolo/Vc-trem-D4-pp-2c-N...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      trem\n      tremolo\n      D4\n      62.0\n      pp\n      0.0\n      1.0\n      N\n      None\n      2.0\n      False\n      4\n    \n    \n      4033\n      PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_b...\n      PluckedStrings\n      Plucked Strings\n      Hp\n      Harp\n      pizz_bartok\n      pizzicato_bartok\n      D#2\n      39.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      NaN\n      True\n      0\n    \n    \n      10073\n      Strings/Violoncello/col_legno_battuto/Vc-legno...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      legno_batt\n      col_legno_battuto\n      C#2\n      37.0\n      mf\n      2.0\n      3.0\n      N\n      None\n      4.0\n      False\n      2\n    \n    \n      12233\n      Winds/Flute/flatterzunge/Fl-flatt-D#6-ff-N-N.wav\n      Winds\n      Woodwinds\n      Fl\n      Flute\n      flatt\n      flatterzunge\n      D#6\n      87.0\n      ff\n      4.0\n      0.0\n      N\n      None\n      NaN\n      False\n      1\n    \n    \n      11602\n      Winds/Bassoon/vibrato/Bn-vib-G#3-mf-N-N.wav\n      Winds\n      Woodwinds\n      Bn\n      Bassoon\n      vib\n      vibrato\n      G#3\n      56.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      NaN\n      False\n      2\n    \n    \n      10654\n      Strings/Violoncello/pizzicato_secco/Vc-pizz_se...\n      Strings\n      Violin Family\n      Vc\n      Cello\n      pizz_sec\n      pizzicato_secco\n      B4\n      71.0\n      mf\n      2.0\n      0.0\n      N\n      None\n      1.0\n      False\n      0\n    \n    \n      3546\n      PluckedStrings/Guitar/sul_ponticello/Gtr-pont-...\n      PluckedStrings\n      Plucked Strings\n      Gtr\n      Guitar\n      pont\n      sul_ponticello\n      C4\n      60.0\n      mf\n      2.0\n      1.0\n      N\n      None\n      2.0\n      False\n      3\n    \n  \n\n\n\n\n\nmeta_df.sample(1)['Path'].values[0]\n\n'Strings/Viola/ordinario/Va-ord-A4-ff-3c-R100d.wav'\n\n\n\ntest_spec = spectrogram(meta_df.sample(1)['Path'].values[0])\n\n\ntest_spec.plot_spec()\n\n\n\n\n\ntest, _ = librosa.load('../data/_OrchideaSOL2020_release/OrchideaSOL2020/PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav', \n                    sr = None)\n\n\ntest.shape\n\n(826215,)\n\n\n\ndef mask_spec(arr, inplace = False):\n    loop = random.randint(1, 2)\n    tmp = arr.copy()\n    for i in range(loop):\n        start = random.randint(0, arr.shape[1])\n        duration = random.randint(25, 60)\n        if inplace == True:\n            arr[:, start:start + duration] = 0\n        else:\n            tmp[:, start:start+duration] = 0\n    freq_loop = random.randint(1, 3)\n    for freq in range(freq_loop):\n        start = random.randint(0, arr.shape[0])\n        duration = random.randint(25, 60)\n        if inplace == True:\n            arr[start:start + duration, :] = 0\n        else:\n            tmp[start:start + duration, :] = 0\n\n    return None if inplace == True else tmp\n\n\n# librosa.display.specshow(librosa.amplitude_to_db(mask_spec(spec_sample)), y_axis='log', x_axis = 's')\n\n\nimport random\nprint(random.randint(0, 9))\n\n9\n\n\n\nmeta_df['_ins'] = meta_df['Instrument (in full)']\n\n\nprint(random.randint.__doc__)\n\nReturn random integer in range [a, b], including both end points.\n        \n\n\n\ntest, _ = next(gen(meta_df, test_verbose = True))\n\nHIT\nSUCCESS\n\n\n\nprint(test)\n\n[[[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n ...\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]]\n\n\n\nnp.load('/home/shiya/Documents/Music_transcription_fastai/data/_OrchideaSOL2020_release/OrchideaSOL2020/Strings/Violin/ordinario/Vn-ord-A3-ff-4c-N.npy', \n            allow_pickle=True)\n\nFileNotFoundError: [Errno 2] No such file or directory: '/home/shiya/Documents/Music_transcription_fastai/data/_OrchideaSOL2020_release/OrchideaSOL2020/Strings/Violin/ordinario/Vn-ord-A3-ff-4c-N.npy'\n\n\n\n0 in test\n\nTrue\n\n\n\nprint(test.shape)\n\n(256, 500, 1)\n\n\n\nlibrosa.display.specshow(librosa.amplitude_to_db(np.reshape(test, newshape = test.shape[:2])), x_axis = 's', \n                                                y_axis = 'mel', sr=44100, hop_length=2048, \n                                                n_fft=2048)\n\n<matplotlib.collections.QuadMesh at 0x7f7700c99760>\n\n\n\n\n\n\nsample = meta_df.sample(1)\n\n\nsample['Path'].values\n\narray(['Strings/Violoncello+sordina_piombo/ordinario/Vc+SP-ord-D3-mf-2c-N.wav'],\n      dtype=object)\n\n\n\nspec = spectrogram(sample['Path'].values[0])\n\nAttributeError: 'spectrogram' object has no attribute 'spec'\n\n\n\nsample['Path']\n\n8462    Strings/Violin/ordinario/Vn-ord-C#4-pp-4c-N.wav\nName: Path, dtype: object\n\n\n\nnp.save('testnig.npy', spec.spec)\n\n\n!ls\n\n/bin/bash: /home/shiya/anaconda3/envs/music/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n__init__.py          spectrogram_class.py\nmodel.png            spectrogram.py\nmusic_transcription_2conv.ipynb  test.ipynb\nmusic_transcription_class.ipynb  testnig.npy\nmusic_transcription.ipynb    wav_converter_class.py\n__pycache__          wav_converter.py\nspec_input_generator.py\n\n\n\nload_test = np.load('testnig.npy', allow_pickle = True)\n\n\nload_test\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nload_test.shape\n\n()\n\n\n\nsample_path = meta_df.sample(1)['Path'].values[0]\nsample_path\n\n'Strings/Violin/ordinario/Vn-ord-A4-ff-2c-N.wav'\n\n\n\ntest_sample = spectrogram(sample_path, \n                preprocess = False, trunc_off = True)\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntest_sample.add_noise()\n\n\ntest_sample.generate_spec()\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntest_sample.mask_spec()\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntest_sample.shift_spec()\n\n\ntest_sample.plot_spec()\n\n\n\n\n\ntesttest.plot_spec()\n\nNameError: name 'testtest' is not defined"
  },
  {
    "objectID": "posts/sales_backup/cycle-sales-kaggle.html",
    "href": "posts/sales_backup/cycle-sales-kaggle.html",
    "title": "Bike sales analysis",
    "section": "",
    "text": "This is a notebook preceeding after SQL script and Tableau analysis. The main focus on this notebook is to perform statistical analysis, to determine if a relation between predoctors exists statistically.\nClustering analysis is also performed to determine if customers with certain demographic information can be grouped. \n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels as sm\nimport scipy\n\n\n\n\nCode\n!pip install optuna\n\n\nLets have a look at the dataframe we will be working with:\n\n\nCode\nsales_df = pd.read_csv('./sales_cleaned.csv', index_col='index')\nsales_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Date\n      Year\n      Month\n      Customer Age\n      Customer Gender\n      Country\n      State\n      Product Category\n      Sub Category\n      Quantity\n      Unit Cost\n      Unit Price\n      Cost\n      Revenue\n      net_profit\n      Day\n      Month_numeric\n    \n    \n      index\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      02/19/16\n      2016\n      February\n      29\n      F\n      United States\n      Washington\n      Accessories\n      Tires and Tubes\n      1.0\n      80.00\n      109.000000\n      80.0\n      109.0\n      29\n      19\n      2\n    \n    \n      1\n      02/20/16\n      2016\n      February\n      29\n      F\n      United States\n      Washington\n      Clothing\n      Gloves\n      2.0\n      24.50\n      28.500000\n      49.0\n      57.0\n      8\n      20\n      2\n    \n    \n      2\n      02/27/16\n      2016\n      February\n      29\n      F\n      United States\n      Washington\n      Accessories\n      Tires and Tubes\n      3.0\n      3.67\n      5.000000\n      11.0\n      15.0\n      4\n      27\n      2\n    \n    \n      3\n      03/12/16\n      2016\n      March\n      29\n      F\n      United States\n      Washington\n      Accessories\n      Tires and Tubes\n      2.0\n      87.50\n      116.500000\n      175.0\n      233.0\n      58\n      12\n      3\n    \n    \n      4\n      03/12/16\n      2016\n      March\n      29\n      F\n      United States\n      Washington\n      Accessories\n      Tires and Tubes\n      3.0\n      35.00\n      41.666667\n      105.0\n      125.0\n      20\n      12\n      3\n    \n  \n\n\n\n\n\n\nCode\nprint(f\"The dataframe has {sales_df.shape[0]} rows and {sales_df.shape[1]} features\")\n\n\nThe dataframe has 34866 rows and 17 features\n\n\nFrom the dataframe, we can see that the interesting columns are categorical features: * Countries/States * Product categories/sub-categories * Customers gender\nand the numberic features: * Cost * Revenue * Profit * Customers age\nALthough datetime features are also included in the dataframe, we will be ignoring these since the time-series analysis was analysed in our Tableau and SQL analysis.\n\n\nCode\nsales_df.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 34866 entries, 0 to 34865\nData columns (total 17 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Date              34866 non-null  object \n 1   Year              34866 non-null  int64  \n 2   Month             34866 non-null  object \n 3   Customer Age      34866 non-null  int64  \n 4   Customer Gender   34866 non-null  object \n 5   Country           34866 non-null  object \n 6   State             34866 non-null  object \n 7   Product Category  34866 non-null  object \n 8   Sub Category      34866 non-null  object \n 9   Quantity          34866 non-null  float64\n 10  Unit Cost         34866 non-null  float64\n 11  Unit Price        34866 non-null  float64\n 12  Cost              34866 non-null  float64\n 13  Revenue           34866 non-null  float64\n 14  net_profit        34866 non-null  int64  \n 15  Day               34866 non-null  int64  \n 16  Month_numeric     34866 non-null  int64  \ndtypes: float64(5), int64(5), object(7)\nmemory usage: 4.8+ MB\n\n\n\n\nCode\nsales_df.describe()\n\n\n\n\n\n\n  \n    \n      \n      Year\n      Customer Age\n      Quantity\n      Unit Cost\n      Unit Price\n      Cost\n      Revenue\n      net_profit\n      Day\n      Month_numeric\n    \n  \n  \n    \n      count\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n      34866.000000\n    \n    \n      mean\n      2015.569237\n      36.382895\n      2.002524\n      349.880567\n      389.232485\n      576.004532\n      640.870074\n      64.865542\n      15.667671\n      6.317845\n    \n    \n      std\n      0.495190\n      11.112902\n      0.813936\n      490.015846\n      525.319091\n      690.500395\n      736.650597\n      152.879908\n      8.770677\n      3.465317\n    \n    \n      min\n      2015.000000\n      17.000000\n      1.000000\n      0.670000\n      0.666667\n      2.000000\n      2.000000\n      -937.000000\n      1.000000\n      1.000000\n    \n    \n      25%\n      2015.000000\n      28.000000\n      1.000000\n      45.000000\n      53.666667\n      85.000000\n      102.000000\n      5.000000\n      8.000000\n      3.000000\n    \n    \n      50%\n      2016.000000\n      35.000000\n      2.000000\n      150.000000\n      179.000000\n      261.000000\n      319.000000\n      27.000000\n      16.000000\n      6.000000\n    \n    \n      75%\n      2016.000000\n      44.000000\n      3.000000\n      455.000000\n      521.000000\n      769.000000\n      902.000000\n      96.000000\n      23.000000\n      9.000000\n    \n    \n      max\n      2016.000000\n      87.000000\n      3.000000\n      3240.000000\n      5082.000000\n      3600.000000\n      5082.000000\n      1842.000000\n      31.000000\n      12.000000\n    \n  \n\n\n\n\nIt appears that most of the distribution are a long tailed distribution, skewed to the lower end.\n\n\nCode\nsns.pairplot(sales_df)\n\n\n<seaborn.axisgrid.PairGrid at 0x7ffbb5595790>\n\n\n\n\n\n\n\nCode\nsales_df.duplicated().mean()\n\n\n2.868123673492801e-05\n\n\n\n\nCode\nsales_df[sales_df.duplicated(keep=False)].sort_values(['Date', 'Revenue'])\n\n\n\n\n\n\n  \n    \n      \n      Date\n      Year\n      Month\n      Customer Age\n      Customer Gender\n      Country\n      State\n      Product Category\n      Sub Category\n      Quantity\n      Unit Cost\n      Unit Price\n      Cost\n      Revenue\n      net_profit\n      Day\n      Month_numeric\n    \n    \n      index\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      868\n      08/12/15\n      2015\n      August\n      43\n      F\n      Germany\n      Bayern\n      Accessories\n      Bottles and Cages\n      2.0\n      10.0\n      14.0\n      20.0\n      28.0\n      8\n      12\n      8\n    \n    \n      869\n      08/12/15\n      2015\n      August\n      43\n      F\n      Germany\n      Bayern\n      Accessories\n      Bottles and Cages\n      2.0\n      10.0\n      14.0\n      20.0\n      28.0\n      8\n      12\n      8\n    \n  \n\n\n\n\nOut of 34866 entries, there was one entry that appears to be duplicated. It is quite possible that one person with the same demographic information purchased the exact same item on the same day. We will leave the duplicated entry as it is only one of them."
  },
  {
    "objectID": "posts/sales_backup/cycle-sales-kaggle.html#t-test-on-revenues-generated-for-different-gender.",
    "href": "posts/sales_backup/cycle-sales-kaggle.html#t-test-on-revenues-generated-for-different-gender.",
    "title": "Bike sales analysis",
    "section": "T-test on revenues generated for different gender.",
    "text": "T-test on revenues generated for different gender.\nTo determine if different gender had different spending habits, we will be running a unpaired t-test to investigate if the two distribution is the same.\nThe hypothesis of a unpaired t-test is as follows: \\[H_0: \\bar{X} - \\bar{Y} = 0\\] \\[H_1: \\bar{X} - \\bar{Y} \\neq 0\\]\nwhere X and Y are the two distribution compared.\n\n\nCode\nttest_stat, ttest_p = scipy.stats.ttest_ind(sales_df[sales_df['Customer Gender'] == 'M'].Revenue, \n                   sales_df[sales_df['Customer Gender'] == 'F'].Revenue )\n\nprint(f'The p-vale for unpaired t-test for both gender is {ttest_p:.4f}')\n\n\nThe p-vale for unpaired t-test for both gender is 0.9855\n\n\nIndeed, we can see that the p-value is around 0.98, which is highly unlikely that the mean of revenue spent for each gender were different.\n\nHowever, from the distribution above, we can see that it is highly not normal. Although we do have 34688 entries, and the central limit theorem works in our favour, and the assumption of similar means from any sample drawn from the distribution is the same.\n\nJust to be safe, we will be running a Wilcoxon test (alternative non-parametric t-test)."
  },
  {
    "objectID": "posts/sales_backup/cycle-sales-kaggle.html#mann-whitney-u-test-for-revenues-generated-by-eahc-gender.",
    "href": "posts/sales_backup/cycle-sales-kaggle.html#mann-whitney-u-test-for-revenues-generated-by-eahc-gender.",
    "title": "Bike sales analysis",
    "section": "Mann-whitney U test for revenues generated by eahc gender.",
    "text": "Mann-whitney U test for revenues generated by eahc gender.\n\n\nCode\nwhitney_stat, whitney_p = scipy.stats.mannwhitneyu(sales_df[sales_df['Customer Gender'] == 'M'].Revenue, \n                   sales_df[sales_df['Customer Gender'] == 'F'].Revenue )\n\nprint(f'The p-vale for unpaired t-test for both gender is {whitney_p:.4f}')\n\n\nThe p-vale for unpaired t-test for both gender is 0.2144\n\n\n\nConclusion\nIndeed, from the p-value above, we can see that it is still higher than the threshold of 5%.\n\n\nThere is no statistical significant that each gender had different spending habit!"
  },
  {
    "objectID": "posts/sales_backup/cycle-sales-kaggle.html#conclusion-1",
    "href": "posts/sales_backup/cycle-sales-kaggle.html#conclusion-1",
    "title": "Bike sales analysis",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nThere is no statistical significant proof that gender is related to product bought.\n\n\n\n\nCode\ndef chi2_test(df, col1, col2, show_table = True):\n    assert col1 in df.columns and col2 in df.columns, 'Column names provide can not be found in dataframe!'\n    _cont = df.groupby([col1, col2])[col2].count().unstack()\n    _result = scipy.stats.chi2_contingency(_cont)\n    if show_table:\n        print('------------------------------')\n        print('Observed contigency table')\n        display(_cont)\n        print('------------------------------')\n        print('Expected frequency table')\n        display(pd.DataFrame(_result[-1], index = _cont.index, \n                    columns = _cont.columns))\n    print(f'The p-value of chi2 independence test is {_result[1]:.4f}')\n\n\n\n\nCode\nchi2_test(sales_df, 'Country', 'Product Category')\n\n\n------------------------------\nObserved contigency table\n\n\n\n\n\n\n  \n    \n      Product Category\n      Accessories\n      Bikes\n      Clothing\n    \n    \n      Country\n      \n      \n      \n    \n  \n  \n    \n      France\n      3293\n      1152\n      723\n    \n    \n      Germany\n      3200\n      1291\n      710\n    \n    \n      United Kingdom\n      3986\n      1497\n      938\n    \n    \n      United States\n      12055\n      3153\n      2868\n    \n  \n\n\n\n\n------------------------------\nExpected frequency table\n\n\n\n\n\n\n  \n    \n      Product Category\n      Accessories\n      Bikes\n      Clothing\n    \n    \n      Country\n      \n      \n      \n    \n  \n  \n    \n      France\n      3340.093845\n      1051.357311\n      776.548844\n    \n    \n      Germany\n      3361.421844\n      1058.070699\n      781.507457\n    \n    \n      United Kingdom\n      4149.911490\n      1306.262634\n      964.825876\n    \n    \n      United States\n      11682.572822\n      3677.309356\n      2716.117823\n    \n  \n\n\n\n\nThe p-value of chi2 independence test is 0.0000\n\n\n\n\nFrom the above result, we can see that there was deviation in sales, especially between United States with other countries. Infact, the corresponding p-value is 0, indicating there is a relation between the distribution of product sold and countries."
  },
  {
    "objectID": "posts/sales_backup/cycle-sales-kaggle.html#k-prototype-clustering",
    "href": "posts/sales_backup/cycle-sales-kaggle.html#k-prototype-clustering",
    "title": "Bike sales analysis",
    "section": "K-prototype clustering",
    "text": "K-prototype clustering\nIn the previous sub-section, we have explored the option of clustering the dataframe by one-hot-encoding. However, this method is not all sensible. The distance metric was arbitrary, although the Mahalanobis distance was used, there is no guarantee that the distance calculated is a good representation of the datapoints clustering (how is the distance between California and England comparable with the distance of $200 difference in revenue?).\nAnother method we will be using is the K-Prototypes clustering, which is a combination of K-means and K-modes algorithms.\n\n\nCode\nfrom kmodes.kprototypes import KPrototypes\n\n\n\n\nCode\nclustering_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Customer Age\n      Customer Gender\n      Product Category\n      Cost\n      Revenue\n      net_profit\n      Country_with_California\n    \n    \n      index\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      29\n      F\n      Accessories\n      80.0\n      109.0\n      29\n      United States\n    \n    \n      1\n      29\n      F\n      Clothing\n      49.0\n      57.0\n      8\n      United States\n    \n    \n      2\n      29\n      F\n      Accessories\n      11.0\n      15.0\n      4\n      United States\n    \n    \n      3\n      29\n      F\n      Accessories\n      175.0\n      233.0\n      58\n      United States\n    \n    \n      4\n      29\n      F\n      Accessories\n      105.0\n      125.0\n      20\n      United States\n    \n  \n\n\n\n\n\n\nCode\nnumeric_scaler = ColumnTransformer([('std_scaler', StandardScaler(), \n                        make_column_selector(dtype_include='number'))], \n                        remainder = 'passthrough', \n                        verbose_feature_names_out=False)\n\nclustering_scaled_df = pd.DataFrame(numeric_scaler.fit_transform(clustering_df),  \n                                    columns = numeric_scaler.get_feature_names_out()).apply(lambda x: pd.to_numeric(x, errors='ignore'))\n\nclustering_scaled_df\n\n\n\n\n\n\n  \n    \n      \n      Customer Age\n      Cost\n      Revenue\n      net_profit\n      Customer Gender\n      Product Category\n      Country_with_California\n    \n  \n  \n    \n      0\n      -0.664363\n      -0.718337\n      -0.722022\n      -0.234603\n      F\n      Accessories\n      United States\n    \n    \n      1\n      -0.664363\n      -0.763232\n      -0.792612\n      -0.371968\n      F\n      Clothing\n      United States\n    \n    \n      2\n      -0.664363\n      -0.818265\n      -0.849628\n      -0.398132\n      F\n      Accessories\n      United States\n    \n    \n      3\n      -0.664363\n      -0.580753\n      -0.553690\n      -0.044909\n      F\n      Accessories\n      United States\n    \n    \n      4\n      -0.664363\n      -0.682130\n      -0.700301\n      -0.293473\n      F\n      Accessories\n      United States\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      34861\n      0.145518\n      2.133260\n      1.148634\n      -4.100438\n      M\n      Bikes\n      France\n    \n    \n      34862\n      0.145518\n      2.525734\n      1.805672\n      -2.707167\n      M\n      Bikes\n      France\n    \n    \n      34863\n      0.145518\n      2.133260\n      1.278956\n      -3.472485\n      M\n      Bikes\n      France\n    \n    \n      34864\n      0.145518\n      2.133260\n      1.413350\n      -2.824908\n      M\n      Bikes\n      France\n    \n    \n      34865\n      0.145518\n      2.525734\n      1.258593\n      -5.343261\n      M\n      Bikes\n      France\n    \n  \n\n34866 rows  7 columns\n\n\n\n\n\nCode\nclustering_scaled_df.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 34866 entries, 0 to 34865\nData columns (total 7 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Customer Age             34866 non-null  float64\n 1   Cost                     34866 non-null  float64\n 2   Revenue                  34866 non-null  float64\n 3   net_profit               34866 non-null  float64\n 4   Customer Gender          34866 non-null  object \n 5   Product Category         34866 non-null  object \n 6   Country_with_California  34866 non-null  object \ndtypes: float64(4), object(3)\nmemory usage: 1.9+ MB\n\n\n\n\nCode\ncat_position = [clustering_scaled_df.columns.get_loc(col) for col in \n                    clustering_scaled_df.select_dtypes('object').columns]\ncat_position\n\n\n[4, 5, 6]\n\n\n\n\nCode\n\nclustering_scaled_df.select_dtypes('object').columns\n\n\nIndex(['Customer Gender', 'Product Category', 'Country_with_California'], dtype='object')\n\n\n\n\nCode\nkproto = KPrototypes()\n\nkproto.fit_predict(clustering_scaled_df, categorical = cat_position )\n\n\nKeyboardInterrupt: \n\n\n\n\nCode\nkproto.cost_\n\n\nNameError: name 'kproto' is not defined\n\n\n\n\nCode\nkproto_results ={'cluster_size': [], 'cost':[]} \n\nclustering_scaled_df_sample = clustering_scaled_df.sample(frac=0.5)\n\nfor cluster_size in range(1, 8):\n    print(f'Fitting Kprototypes for cluster size :{cluster_size}')\n    _kproto = KPrototypes(n_clusters=cluster_size)\n    _labels = _kproto.fit_predict(clustering_scaled_df_sample, categorical = cat_position)\n    kproto_results['cluster_size'].append(cluster_size)\n    kproto_results['cost'].append(_kproto.cost_)\n    print(f'Cost: {_kproto.cost_}')\n    print('----------------------------------------')\n\n\nFitting Kprototypes for cluster size :1\nCost: 81947.90456327623\n----------------------------------------\nFitting Kprototypes for cluster size :2\nCost: 54558.85795930906\n----------------------------------------\nFitting Kprototypes for cluster size :3\nCost: 44219.11078464947\n----------------------------------------\nFitting Kprototypes for cluster size :4\nCost: 35877.56991156543\n----------------------------------------\nFitting Kprototypes for cluster size :5\nCost: 31335.38134042823\n----------------------------------------\nFitting Kprototypes for cluster size :6\nCost: 28857.25619255692\n----------------------------------------\nFitting Kprototypes for cluster size :7\nCost: 26973.43389824587\n----------------------------------------\n\n\n\n\nCode\n# import pickle\n# import os\n\n# with open('kproto_result.pkl', 'wb') as file:\n#     # assert not os.path.getsize(file) > 0, f\"{file} already exists! Prone to overwriting.\"\n#     pickle.dump(kproto_results, file=file)\n\n\n\n\n\nCode\nimport pickle\n\nwith open('./kproto_result.pkl', 'rb') as file:\n    kproto_results = pickle.load(file)\n\n\n\n\nCode\nplt.plot(kproto_results['cluster_size'], kproto_results['cost'])\nplt.title('Elbow graph for Kprototypes clustering')\nplt.xlabel('Cluster size')\nplt.ylabel('Cost value')\n\n\nText(0, 0.5, 'Cost value')\n\n\n\n\n\n\n\nCode\nkproto_best = KPrototypes(5)\nkproto_best_label = kproto_best.fit_predict(clustering_scaled_df, categorical = cat_position)\n\nclustering_scaled_df['kproto_labels'] = kproto_best_label\n\n\n\n\nCode\nclustering_scaled_df\n\n\n\n\n\n\n  \n    \n      \n      Customer Age\n      Cost\n      Revenue\n      net_profit\n      Customer Gender\n      Product Category\n      Country_with_California\n      kproto_labels\n    \n  \n  \n    \n      0\n      -0.664363\n      -0.718337\n      -0.722022\n      -0.234603\n      F\n      Accessories\n      United States\n      2\n    \n    \n      1\n      -0.664363\n      -0.763232\n      -0.792612\n      -0.371968\n      F\n      Clothing\n      United States\n      2\n    \n    \n      2\n      -0.664363\n      -0.818265\n      -0.849628\n      -0.398132\n      F\n      Accessories\n      United States\n      2\n    \n    \n      3\n      -0.664363\n      -0.580753\n      -0.553690\n      -0.044909\n      F\n      Accessories\n      United States\n      2\n    \n    \n      4\n      -0.664363\n      -0.682130\n      -0.700301\n      -0.293473\n      F\n      Accessories\n      United States\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      34861\n      0.145518\n      2.133260\n      1.148634\n      -4.100438\n      M\n      Bikes\n      France\n      4\n    \n    \n      34862\n      0.145518\n      2.525734\n      1.805672\n      -2.707167\n      M\n      Bikes\n      France\n      4\n    \n    \n      34863\n      0.145518\n      2.133260\n      1.278956\n      -3.472485\n      M\n      Bikes\n      France\n      4\n    \n    \n      34864\n      0.145518\n      2.133260\n      1.413350\n      -2.824908\n      M\n      Bikes\n      France\n      4\n    \n    \n      34865\n      0.145518\n      2.525734\n      1.258593\n      -5.343261\n      M\n      Bikes\n      France\n      4\n    \n  \n\n34866 rows  8 columns\n\n\n\n\n\nCode\nclustering_ohe_df = ct.fit_transform(clustering_df)\n\nclustering_ohe_df = pd.DataFrame(clustering_ohe_df, columns = ct.get_feature_names_out())\n\n\n\n\nCode\nclustering_ohe_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Customer Gender_F\n      Customer Gender_M\n      Country_with_California_California\n      Country_with_California_France\n      Country_with_California_Germany\n      Country_with_California_United Kingdom\n      Country_with_California_United States\n      Product Category_Accessories\n      Product Category_Bikes\n      Product Category_Clothing\n      Customer Age\n      Cost\n      Revenue\n      net_profit\n    \n  \n  \n    \n      0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      29.0\n      80.0\n      109.0\n      29.0\n    \n    \n      1\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      29.0\n      49.0\n      57.0\n      8.0\n    \n    \n      2\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      29.0\n      11.0\n      15.0\n      4.0\n    \n    \n      3\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      29.0\n      175.0\n      233.0\n      58.0\n    \n    \n      4\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      29.0\n      105.0\n      125.0\n      20.0\n    \n  \n\n\n\n\n\n\nCode\nfrom sklearn.decomposition import PCA\n\nclustering_pca = PCA(2)\nclustering_ohe_pca_df = clustering_pca.fit_transform(clustering_ohe_df)\nclustering_ohe_pca_df = pd.DataFrame(clustering_ohe_pca_df, \n                                    columns = clustering_pca.get_feature_names_out())\n\n\n\n\nCode\nclustering_ohe_pca_df.head()\n\n\n\n\n\n\n  \n    \n      \n      pca0\n      pca1\n    \n  \n  \n    \n      0\n      0.716264\n      -0.516476\n    \n    \n      1\n      0.708054\n      0.470574\n    \n    \n      2\n      0.716230\n      -0.523926\n    \n    \n      3\n      0.716310\n      -0.506415\n    \n    \n      4\n      0.716275\n      -0.514432\n    \n  \n\n\n\n\n\n\nCode\nclustering_ohe_pca_df['cluster'] = kproto_best_label\n\n\n\n\nCode\nsns.scatterplot(clustering_ohe_pca_df, x='pca0', y='pca1', hue = 'cluster')\n\n\n<AxesSubplot: xlabel='pca0', ylabel='pca1'>\n\n\n\n\n\n\n\nCode\nclustering_ohe_df['cluster'] = kproto_best_label\n\n\n\n\nCode\nclustering_ohe_df.groupby('cluster').mean()\n\n\n\n\n\n\n  \n    \n      \n      Customer Gender_F\n      Customer Gender_M\n      Country_with_California_California\n      Country_with_California_France\n      Country_with_California_Germany\n      Country_with_California_United Kingdom\n      Country_with_California_United States\n      Product Category_Accessories\n      Product Category_Bikes\n      Product Category_Clothing\n      Customer Age\n      Cost\n      Revenue\n      net_profit\n    \n    \n      cluster\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.484785\n      0.515215\n      0.143757\n      0.193075\n      0.443337\n      0.125393\n      0.094439\n      0.092340\n      0.708814\n      0.198846\n      37.589192\n      2001.837880\n      2461.071878\n      459.233998\n    \n    \n      1\n      0.551656\n      0.448344\n      0.345826\n      0.121455\n      0.101923\n      0.172715\n      0.258081\n      0.790502\n      0.067916\n      0.141582\n      48.370712\n      244.929804\n      278.615209\n      33.685406\n    \n    \n      2\n      0.428810\n      0.571190\n      0.306502\n      0.158408\n      0.124190\n      0.186215\n      0.224685\n      0.772876\n      0.086138\n      0.140985\n      28.539619\n      210.933305\n      233.566451\n      22.633145\n    \n    \n      3\n      0.535751\n      0.464249\n      0.211211\n      0.145262\n      0.258591\n      0.193336\n      0.191600\n      0.570982\n      0.209997\n      0.219021\n      34.156369\n      838.286185\n      1022.031933\n      183.745748\n    \n    \n      4\n      0.482447\n      0.517553\n      0.345639\n      0.168295\n      0.020268\n      0.236699\n      0.229099\n      0.001810\n      0.944987\n      0.053203\n      37.166124\n      2149.917481\n      2027.964893\n      -121.952588\n    \n  \n\n\n\n\n\n\nCode\nclustering_scaled_df\n\n\n\n\n\n\n  \n    \n      \n      Customer Age\n      Cost\n      Revenue\n      net_profit\n      Customer Gender\n      Product Category\n      Country_with_California\n      kproto_labels\n    \n  \n  \n    \n      0\n      -0.664363\n      -0.718337\n      -0.722022\n      -0.234603\n      F\n      Accessories\n      United States\n      2\n    \n    \n      1\n      -0.664363\n      -0.763232\n      -0.792612\n      -0.371968\n      F\n      Clothing\n      United States\n      2\n    \n    \n      2\n      -0.664363\n      -0.818265\n      -0.849628\n      -0.398132\n      F\n      Accessories\n      United States\n      2\n    \n    \n      3\n      -0.664363\n      -0.580753\n      -0.553690\n      -0.044909\n      F\n      Accessories\n      United States\n      2\n    \n    \n      4\n      -0.664363\n      -0.682130\n      -0.700301\n      -0.293473\n      F\n      Accessories\n      United States\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      34861\n      0.145518\n      2.133260\n      1.148634\n      -4.100438\n      M\n      Bikes\n      France\n      4\n    \n    \n      34862\n      0.145518\n      2.525734\n      1.805672\n      -2.707167\n      M\n      Bikes\n      France\n      4\n    \n    \n      34863\n      0.145518\n      2.133260\n      1.278956\n      -3.472485\n      M\n      Bikes\n      France\n      4\n    \n    \n      34864\n      0.145518\n      2.133260\n      1.413350\n      -2.824908\n      M\n      Bikes\n      France\n      4\n    \n    \n      34865\n      0.145518\n      2.525734\n      1.258593\n      -5.343261\n      M\n      Bikes\n      France\n      4\n    \n  \n\n34866 rows  8 columns\n\n\n\n\n\nCode\nplt.figure(figsize = (8, 15))\nclustering_ohe_df.groupby('cluster').mean().T.plot(kind = 'barh', figsize = (8, 15))\n\n\n<AxesSubplot: >\n\n\n<Figure size 800x1500 with 0 Axes>"
  }
]